{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwp8U_vSIz0s"
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7yQjE32oE645",
    "outputId": "153288b4-fc03-453f-a414-db2566811b0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.layers import Embedding, Flatten, dot\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kb5mJxBI5JW"
   },
   "source": [
    "# SET UP GOOGLE COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "dmP5Oh3THCsr",
    "outputId": "930d14ff-82b1-4714-f2ba-38902594a50a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# this will prompt for authorization\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BIglkQnyIElN",
    "outputId": "366ab70b-2742-4839-cb7f-82f338dc9dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Colab Notebooks'\n"
     ]
    }
   ],
   "source": [
    "# after executing the cell above, Drive files available in \"/content/drive/My Drive\"\n",
    "! ls \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5MR_5CJI_OF"
   },
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aYO5HID2JLgy",
    "outputId": "2cf12cb4-5661-48c4-8111-fa5eff15c593"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMGYe_XKE649"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "current_path = os.getcwd()\n",
    "file = '/drive/My Drive/Colab Notebooks'+'/datasets/mnist_data/mnist.pkl.gz'\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yryy_zjCE65A"
   },
   "outputs": [],
   "source": [
    "X_train_keras = X_train.reshape(50000,28,28,1)\n",
    "X_validation_keras = X_validation.reshape(10000,28,28,1)\n",
    "X_test_keras = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "y_train_keras = to_categorical(y_train)\n",
    "y_validation_keras = to_categorical(y_validation)\n",
    "y_test_keras = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xO4vT2ONE65C"
   },
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n",
    "                   len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wXfZmaTE65F"
   },
   "outputs": [],
   "source": [
    "def view_digit(X, y, example):\n",
    "    label = y.loc[example]\n",
    "    image = X.loc[example,:].values.reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (example, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "hgQDEtnME65H",
    "outputId": "6eef0db9-47ce-42fb-dda3-c9bc820c86d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEHCAYAAACHl1tOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXSQACrpIABR0kQAo6CIB+H9hqt0pGUrp/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the first digit\n",
    "view_digit(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "edhjAFk-qg_H",
    "outputId": "1799f3bf-7fb3-4f5a-fd0a-0722ab0db2aa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEHCAYAAACHl1tOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_digit(X_train, y_train, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1B4tAnYKnSL"
   },
   "source": [
    "# CONFIRM USE OF GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AtUErICRE65K",
    "outputId": "6915a69c-0335-48d4-bc9a-e69294341701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Confirm use of GPU\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else: print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqB-TfRKKvvq"
   },
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we first build a regular CNN to classify MNIST images. We use early stopping and save the best model to disk. \n",
    "- Feel free to skip if interested in the GAN problem only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5ydSEt2E65Q"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3505
    },
    "colab_type": "code",
    "id": "BqcAIbOCE65S",
    "outputId": "4c5eb21f-b9b7-45b7-c4c5-cdab9b1f3a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.1901 - acc: 0.9399 - val_loss: 0.0551 - val_acc: 0.9836\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0739 - acc: 0.9782 - val_loss: 0.0340 - val_acc: 0.9904\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0548 - acc: 0.9840 - val_loss: 0.0367 - val_acc: 0.9901\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0445 - acc: 0.9867 - val_loss: 0.0365 - val_acc: 0.9913\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0393 - acc: 0.9881 - val_loss: 0.0314 - val_acc: 0.9924\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0351 - acc: 0.9889 - val_loss: 0.0365 - val_acc: 0.9923\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0326 - acc: 0.9898 - val_loss: 0.0262 - val_acc: 0.9924\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0323 - acc: 0.9900 - val_loss: 0.0374 - val_acc: 0.9914\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0340 - val_acc: 0.9924\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0265 - acc: 0.9923 - val_loss: 0.0344 - val_acc: 0.9915\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0255 - acc: 0.9922 - val_loss: 0.0332 - val_acc: 0.9922\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0231 - acc: 0.9931 - val_loss: 0.0356 - val_acc: 0.9928\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.0307 - val_acc: 0.9927\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0374 - val_acc: 0.9923\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0231 - acc: 0.9930 - val_loss: 0.0393 - val_acc: 0.9919\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0208 - acc: 0.9937 - val_loss: 0.0341 - val_acc: 0.9928\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0337 - val_acc: 0.9937\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0315 - val_acc: 0.9929\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0213 - acc: 0.9939 - val_loss: 0.0353 - val_acc: 0.9930\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0315 - val_acc: 0.9935\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0323 - val_acc: 0.9937\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0182 - acc: 0.9946 - val_loss: 0.0405 - val_acc: 0.9931\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.0198 - acc: 0.9942 - val_loss: 0.0321 - val_acc: 0.9931\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 21s 410us/step - loss: 0.0175 - acc: 0.9951 - val_loss: 0.0330 - val_acc: 0.9935\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 20s 405us/step - loss: 0.0226 - acc: 0.9942 - val_loss: 0.0325 - val_acc: 0.9930\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 20s 402us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0528 - val_acc: 0.9910\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 20s 402us/step - loss: 0.0193 - acc: 0.9950 - val_loss: 0.0357 - val_acc: 0.9932\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 20s 402us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0345 - val_acc: 0.9931\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0412 - val_acc: 0.9929\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 20s 398us/step - loss: 0.0202 - acc: 0.9948 - val_loss: 0.0316 - val_acc: 0.9939\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0192 - acc: 0.9945 - val_loss: 0.0338 - val_acc: 0.9939\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0198 - acc: 0.9946 - val_loss: 0.0471 - val_acc: 0.9922\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0217 - acc: 0.9944 - val_loss: 0.0414 - val_acc: 0.9930\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0513 - val_acc: 0.9924\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0194 - acc: 0.9952 - val_loss: 0.0379 - val_acc: 0.9924\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0464 - val_acc: 0.9931\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0462 - val_acc: 0.9931\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.0394 - val_acc: 0.9937\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 0.0205 - acc: 0.9946 - val_loss: 0.0411 - val_acc: 0.9929\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.0328 - val_acc: 0.9947\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0397 - val_acc: 0.9935\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0447 - val_acc: 0.9932\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.0435 - val_acc: 0.9943\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.0185 - acc: 0.9954 - val_loss: 0.0540 - val_acc: 0.9923\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0219 - acc: 0.9949 - val_loss: 0.0430 - val_acc: 0.9930\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0179 - acc: 0.9951 - val_loss: 0.0470 - val_acc: 0.9930\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0193 - acc: 0.9955 - val_loss: 0.0432 - val_acc: 0.9944\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0467 - val_acc: 0.9932\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0197 - acc: 0.9948 - val_loss: 0.0549 - val_acc: 0.9926\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0207 - acc: 0.9954 - val_loss: 0.0542 - val_acc: 0.9932\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0205 - acc: 0.9951 - val_loss: 0.0475 - val_acc: 0.9943\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 0.0227 - acc: 0.9949 - val_loss: 0.0499 - val_acc: 0.9930\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0441 - val_acc: 0.9936\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 22s 436us/step - loss: 0.0197 - acc: 0.9950 - val_loss: 0.0474 - val_acc: 0.9928\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 22s 436us/step - loss: 0.0264 - acc: 0.9944 - val_loss: 0.0442 - val_acc: 0.9944\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0433 - val_acc: 0.9942\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 22s 432us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0489 - val_acc: 0.9935\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0240 - acc: 0.9948 - val_loss: 0.0464 - val_acc: 0.9936\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 21s 428us/step - loss: 0.0250 - acc: 0.9943 - val_loss: 0.0438 - val_acc: 0.9937\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0185 - acc: 0.9957 - val_loss: 0.0442 - val_acc: 0.9939\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0262 - acc: 0.9944 - val_loss: 0.0419 - val_acc: 0.9940\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0205 - acc: 0.9951 - val_loss: 0.0470 - val_acc: 0.9943\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0243 - acc: 0.9950 - val_loss: 0.0415 - val_acc: 0.9939\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0505 - val_acc: 0.9932\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.0257 - acc: 0.9943 - val_loss: 0.0499 - val_acc: 0.9936\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0276 - acc: 0.9944 - val_loss: 0.0508 - val_acc: 0.9923\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0443 - val_acc: 0.9936\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.0235 - acc: 0.9951 - val_loss: 0.0510 - val_acc: 0.9945\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0248 - acc: 0.9948 - val_loss: 0.0565 - val_acc: 0.9937\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0268 - acc: 0.9945 - val_loss: 0.0492 - val_acc: 0.9937\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0261 - acc: 0.9948 - val_loss: 0.0455 - val_acc: 0.9938\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0279 - acc: 0.9950 - val_loss: 0.0538 - val_acc: 0.9935\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.0254 - acc: 0.9948 - val_loss: 0.0508 - val_acc: 0.9935\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0220 - acc: 0.9954 - val_loss: 0.0620 - val_acc: 0.9932\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.0250 - acc: 0.9954 - val_loss: 0.0466 - val_acc: 0.9942\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.0244 - acc: 0.9950 - val_loss: 0.0451 - val_acc: 0.9942\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0272 - acc: 0.9948 - val_loss: 0.0538 - val_acc: 0.9929\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.0236 - acc: 0.9954 - val_loss: 0.0565 - val_acc: 0.9935\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.0255 - acc: 0.9951 - val_loss: 0.0572 - val_acc: 0.9923\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 20s 409us/step - loss: 0.0229 - acc: 0.9951 - val_loss: 0.0516 - val_acc: 0.9930\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 21s 410us/step - loss: 0.0264 - acc: 0.9954 - val_loss: 0.0556 - val_acc: 0.9942\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 20s 410us/step - loss: 0.0263 - acc: 0.9948 - val_loss: 0.0736 - val_acc: 0.9920\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0307 - acc: 0.9944 - val_loss: 0.0534 - val_acc: 0.9933\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0252 - acc: 0.9948 - val_loss: 0.0584 - val_acc: 0.9932\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0230 - acc: 0.9951 - val_loss: 0.0500 - val_acc: 0.9939\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0272 - acc: 0.9948 - val_loss: 0.0572 - val_acc: 0.9931\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0453 - val_acc: 0.9947\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0270 - acc: 0.9946 - val_loss: 0.0511 - val_acc: 0.9945\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0283 - acc: 0.9946 - val_loss: 0.0692 - val_acc: 0.9924\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 20s 405us/step - loss: 0.0286 - acc: 0.9951 - val_loss: 0.0708 - val_acc: 0.9932\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0304 - acc: 0.9948 - val_loss: 0.0563 - val_acc: 0.9941\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0285 - acc: 0.9951 - val_loss: 0.0519 - val_acc: 0.9946\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 20s 406us/step - loss: 0.0273 - acc: 0.9954 - val_loss: 0.0733 - val_acc: 0.9926\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 20s 404us/step - loss: 0.0350 - acc: 0.9943 - val_loss: 0.0598 - val_acc: 0.9930\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.0250 - acc: 0.9954 - val_loss: 0.0718 - val_acc: 0.9934\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 20s 400us/step - loss: 0.0396 - acc: 0.9940 - val_loss: 0.0568 - val_acc: 0.9939\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 0.0285 - acc: 0.9951 - val_loss: 0.0509 - val_acc: 0.9939\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 28s 552us/step - loss: 0.0301 - acc: 0.9949 - val_loss: 0.0517 - val_acc: 0.9941\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 22s 433us/step - loss: 0.0283 - acc: 0.9954 - val_loss: 0.0691 - val_acc: 0.9930\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.0288 - acc: 0.9945 - val_loss: 0.0686 - val_acc: 0.9934\n"
     ]
    }
   ],
   "source": [
    "# Train CNN\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
    "          validation_data=(X_validation_keras, y_validation_keras), \\\n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iaIwx_YeE65W",
    "outputId": "8d8e2dbe-a0fe-44cb-e95d-c67082ca2eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(cnn_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "KRmkdBr2E65Z",
    "outputId": "4c8cabf6-d88a-4c4b-f8d4-a20f92138537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.99454\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naUoKIiKSpqQgIiJp/x/tK5Pm/GsmUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracy of CNN\n",
    "print(\"CNN Final Accuracy\", cnn_history.history['acc'][-1])\n",
    "pd.Series(cnn_history.history['acc']).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFXGolCMQ_or"
   },
   "outputs": [],
   "source": [
    "# save the Keras CNN model\n",
    "model.save('cnn_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hd4bEYHoR4RI"
   },
   "outputs": [],
   "source": [
    "# can load the model without retraining\n",
    "from keras.models import load_model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model_loaded = load_model('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qrkFtE3TD3E"
   },
   "outputs": [],
   "source": [
    "def load_and_evaluate(model_name, return_model=False):\n",
    "    \"\"\"\n",
    "    Load in a trained model and evaluate with log loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    model = load_model(model_name + '.h5')\n",
    "    r = model.evaluate(X_validation_keras, y_validation_keras, \n",
    "                       batch_size=2048, verbose=1)\n",
    "\n",
    "    valid_crossentropy = r[0]\n",
    "    valid_accuracy = r[1]\n",
    "\n",
    "    print(f'Cross Entropy: {round(valid_crossentropy, 4)}')\n",
    "    print(f'Accuracy: {round(100 * valid_accuracy, 2)}%')\n",
    "\n",
    "    if return_model:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "U_B2eHBpTEGq",
    "outputId": "8a572f81-726f-41f6-9946-441f41ccb17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 162us/step\n",
      "Cross Entropy: 0.0686\n",
      "Accuracy: 99.34%\n"
     ]
    }
   ],
   "source": [
    "model_loaded = load_and_evaluate('cnn_model', return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0rAXmGc8iyM"
   },
   "outputs": [],
   "source": [
    "# train with early stopping and save best model to disk\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def make_callbacks(model_name, save=True):\n",
    "    \"\"\"\n",
    "    Make list of callbacks for training\n",
    "    \"\"\"\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    if save:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f'{model_name}.h5',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "callbacks = make_callbacks('cnn_model_early-stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "JkZyvelt9LYB",
    "outputId": "9b9a30d4-2436-4100-b9a9-52f0e7a51d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 0.1820 - acc: 0.9442 - val_loss: 0.0466 - val_acc: 0.9861\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 20s 401us/step - loss: 0.0695 - acc: 0.9794 - val_loss: 0.0342 - val_acc: 0.9908\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 0.0520 - acc: 0.9846 - val_loss: 0.0340 - val_acc: 0.9895\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0460 - acc: 0.9858 - val_loss: 0.0290 - val_acc: 0.9923\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0382 - acc: 0.9879 - val_loss: 0.0278 - val_acc: 0.9916\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0357 - acc: 0.9893 - val_loss: 0.0374 - val_acc: 0.9923\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0330 - val_acc: 0.9912\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0297 - acc: 0.9913 - val_loss: 0.0381 - val_acc: 0.9907\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0294 - acc: 0.9911 - val_loss: 0.0364 - val_acc: 0.9923\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0268 - acc: 0.9924 - val_loss: 0.0275 - val_acc: 0.9941\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0231 - acc: 0.9926 - val_loss: 0.0274 - val_acc: 0.9928\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0243 - acc: 0.9927 - val_loss: 0.0259 - val_acc: 0.9947\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0225 - acc: 0.9931 - val_loss: 0.0289 - val_acc: 0.9918\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0230 - acc: 0.9930 - val_loss: 0.0289 - val_acc: 0.9939\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0203 - acc: 0.9938 - val_loss: 0.0256 - val_acc: 0.9935\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0208 - acc: 0.9930 - val_loss: 0.0241 - val_acc: 0.9942\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0194 - acc: 0.9943 - val_loss: 0.0352 - val_acc: 0.9927\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0204 - acc: 0.9940 - val_loss: 0.0275 - val_acc: 0.9935\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0377 - val_acc: 0.9920\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0198 - acc: 0.9944 - val_loss: 0.0293 - val_acc: 0.9942\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0328 - val_acc: 0.9936\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_keras, y_train_keras, \n",
    "          validation_data=(X_validation_keras, y_validation_keras),\n",
    "          epochs=100,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "waiakvqrLZ6F",
    "outputId": "d84e8fd6-4997-41c5-a523-ec7e605ba37e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.99428\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt0XOV57/HvjEajuyzZlm1swIBj\nHgwOd3ML4d6EpBBWE3o7NIUmaZqU5LDSk2Zx1mGloTdOTk/CaUpXQkhIVujKglIIkAYaEgjkYhLA\nhLt5ABsM2NiWLFkaydJIM5rzx94axrJsz8jao7H277OWrZm9ZzSPR+P9037f/b5volAoICIiApCc\n7QJERKR2KBRERKRIoSAiIkUKBRERKVIoiIhIUWq2CzhQ3d2ZaV8+1dnZTF/frpksZ0aorsqorsqo\nrsrM1bq6utoSU22P9ZlCKlU32yVMSXVVRnVVRnVVJm51xToURERkdwoFEREpUiiIiEiRQkFERIoU\nCiIiUqRQEBGRIoWCiIgUHfSD10Qk3oazOV7d3E86laStOU17S5rmxhTJxJRjs6puvFBgcHiM/sFR\n+gez7BwcZedglv7BUUbGcqRTddSnkqTr60inkqRTSeonbteH+0r2B/fraG1viqRehYKIHHSGszme\n2dDDE+u389zGXnL58d32JxMJWptStDWnaWuun/Jre3M9reH9VDJBASgUoFAoFG9TKJBMp+jLZIPt\nBShQgAKME+wfzubpG8zSHx7odw7tfvAfGBolPz7z69bUp5Jc/7HTWDK/eUa/r0JBRGbEWC5PLl+g\nqSGaw0p2NF8Mgmc37mAsFwTBsoUtnLhyIYkEZHaNMTA0SmZ4jMyuMXYOZtncMxRJPfuTqkswryXN\n8iVtzGtJ09HWQEdLmnmtDXS0ppnX0kBTQx1juXFGc+Ph1zyjY8HXsbFg++Tbo2PjjOXyzGtvoqM1\nPfN1z/h3FJE5Lzua583tg2zaluH1rQNs2jrIlp4hxgsFFnU0cfjiVg5f3Mbhi9tYvriVea0N03ud\nsTzPbdjB4y9t59lXexgNg+CQBc2sOWYRa45ZxLKu1n1+j1x+nMEwJDK7RoPgCL8Ohl/z4wUSCUgk\nEiSARPhXMgGNDfVkR3PBfgCC7SQgQYLGdB3zWtN0tDbQ0dpQvN3SmCIRYRNWV1cb3d2ZGf++CgUR\n2afhbI43tmXYtDXDpm0ZNm0b5O0dQ5Su5JtOJTlyaRvpVB1vbMvwpHfzpHcX97e3pDl8cSvLw6A4\nfHErXR1NU7b7j47leW7jDp54aTtPv9rD6FgQBIvnN3PaMYtYs2oRyxa2lH3ATdUliwfs6Yjq4Fur\nFAoiB7HxQiFoJslk6RvMBm3Yg6MA1NUlqK9LUlcXdE6m6hKk6pLM3zrIrqERUnXJkj8J6lNJ6pIJ\nevpHgoP/1uDPtr7h3V6zMV3HykM7WL64jeVLWlm+pJ1D5jeTTAYH6UKhQF8my6ZtGd7YNsgb2zK8\nsS3D8xt7eX5jb/H7NDXUcVjXO2cUixZm+NmTb/D0Kz1kx/IALOpsKp4RHLaoNdLfvCWgUBCpQYVC\ngZHRPH2Z4EA/8XVnJui8nAiA/sFoOjEnNDekWLW8k+VL2sIQaGNR59S/4U9IJBLMb29kfnsjJ63s\nKm4fHB4LAyIIik3bMryyuZ+X3+rf7fldHY2sOeZQ1hyziMMXKwiqTaEgMg25/Dgjo3lGRnPB1+w7\nt4dHc4xk86TSKXb2DzOWDzoRi3/y4+RyQWfhxP3J+0bG8sVmk6nUJRN0tKY54pA2Olob6GxtoKMt\n+NremiYJjOUL5PLjJX+C+w2NafoHhoMa8uPk84XgdcPXntfawBFLggBYOK9xxg7KrU31HHvEfI49\nYn5xW3Y0z1vdQUiMJ5OsWBI0MSkIZo9CQWSSQqHAxrcHeGL9drb27mIkm2O4NABG88UrX2ZCfSpJ\nfdjEU59K0tJUH1ypEl6l0lFywO9obaCzrYHW5vppX4dfS23kDek6Viybx4pl82qqrjhTKIgQBMEb\n2wZ5/KVtPLF+Oz39I8V9CYKDV2O6jpbGehbOa6QxnaIx3NbYMHE7RVP4tTFdx6KuVoaHsqQmHfTr\nU3XF+6m6hH4rlpqiUJBY29wzxOMvbuPxl7azrTdY2rAxXceZxy1mzarF2GEdNKTrpvVbuX7zlYOR\nQkFiZ1vvLh5fHwTB5u5gYFM6lWTNMYs4bdUi3n3UAtL1tbkEo0jUFApSs3L5cd7YNsib2zOk6pI0\nN6RobkzRFH5tbqinsaG8g3dP/zBPrN/O4+u3s2lb8Nt7qi7BSSsXctqqxZzwrgU0pvXfQUT/C6Rm\n7BzMsmFzPxs2D/Dqln42bc3st0M3ATQ31dNYXxcGRar4takxRUN9HS9t6mPDlgEguGrn+BULWHPM\nIk5a2UVzo/4LiJTS/wiZFbn8OG9uH+TVzf3FINgxUNK5m4DDulpZsWweRyxpA2BXNseukVzx63A2\nx66RMUbzBTJDWXr6hxnO5vd4rUQCVi3v5PRjF3Py0V20NtVX7d8pcrCJNBTM7EbgDKAAXOPuT5Ts\nuwy4DsgCt7v7TWaWBL4BrAZGgU+5+0tR1ijVsb+zgNamek5810KOWtrOimXzOPKQtrKbc0o7dMfH\nCwyPhuERBschC1uY1zLzE4eJzEWRhYKZnQusdPczzWwVcCtwZrgvCdwEnAzsAB4ws3uANcA8dz/L\nzFYA/wxcElWNEp1cfpxX3+rn2Y07eHbDDraUzFRZehawYlkQAos6mmbk0sxkMkFLYz0tjTobEJmO\nKM8ULgTuAXD39WbWaWbt7j4ALAR2ugczZpnZQ8BFwCLg8fA5G8xsuZnVufuebQJSc3YOZnluww6e\n3biDF1/vLTblpFNJ3n3UAlYeOq/iswARqa4o/2cuAdaV3O8Otw2Et9vMbCXwOnA+8AjwLPA5M/t/\nwLuAowgCZFuEdco0jY8HI3+f3bCD5zbsKF7VA8H8NWetPoTjVyzADuvQJZ4iB4lq/rpWbBtw94KZ\nXUnQpNQPvAYk3P0BM3sP8HOCgFhf+rypdHY2k0pN/4DT1dU27edGqVbramhu4KmXtvHk+u085dvI\n7BoDgss7T1zZxSmrFnPqqmCO+2qO1K3V90t1VUZ1VSaKuqIMhS0EZwYTlgJvT9xx90eB9wKY2Q0E\nZwy4+3UTjzGzDcD2fb1IX9+uaRdYqyNOa62uvkyWX7+4lec29uKb+piYk7OzrYFzTljKCSsWcMzy\nzt1W3OrpGaxafbX2fk1QXZVRXZU50Lr2FihRhsKDwPXAzWZ2MrDF3Yv/AjN7ALgSGAIuBb5iZicQ\nXKX0MTO7GHjK3Wdu5jEpW3Y0z1OvdLP2+a28+HovhULQibvy0Hm8e8UCjl+xkEO7yl/oREQODpGF\ngruvNbN1ZraWYI3rq83sKqDf3X8A3EIQHAXgBnfvMbNeIGlmjwMjwBVR1Sd7Gi8U8Dd2svb5t3nS\nu8mOBh3FK5a2c9bqJVx89gqyu7KzXKWIRCnSPgV3v3bSpmdK9t0N3D3p8ePAVVHWJHt6e8cQj72w\nlcee38qOgeCgv6C9kd859TDOWr2EJfObgWBJxW6FgsicpusCY2pweIzH129j7fNb2RhOAdGYruPs\n4w/hPauXsPKwjmnP1y8iBy+FQozk8uM8u2EHa5/fyjOv9pAfL5BIwOqj5nPW6iWctLKLBl06KhJr\nCoUYyI+P84tn3+beX75Gf7io+6FdLZy1+hDOOG4xHa0Ns1yhiNQKhcIcVigUeOrlbu56dCNbe3eR\nTiW56NRDOfvdh3D44tq87lpEZpdCYY7yN/q485ENbNwyQDKR4LyTlvGh9xyhswIR2SeFwhzz5vZB\n7np0A89u2AHAqdbFh89dUbyCSERkXxQKc0RP/zD3/OI1Hnt+KwXgmMM7+Mh5K1ixdN5slyYiBxGF\nwkEus2uUHz22iYefeotcvsChXa38/vkrWH3kfI02FpGKKRQOUtnRPD958k0e+M0mhrN5FrQ38uFz\njuL04xZrfIGITJtC4SAz+fLS1qZ6/ujCozj/pGXUp5KzXZ6IHOQUCgeBQqHAm9sHWefd/ObFbWzf\nOUy6PsklZx3BxacdrsXnRWTG6GhSo8YLBTZuHmDdy9tZ59309AeL2qfqkpx34lI+dPaRurxURGac\nQqGG5PLjvPzmTl74+UbWPrulOPq4IV3HaasWcYot4t1HzddSliISGR1dZtlYLs8Lr/Wx7uXtPP1K\nD0MjOQBam+o5+/hDOOXoLo49opP6A1hdTkSkXAqFWTCczfHcxh2s826e3bijuG5BR2uaC05exgWn\nLWdxe5q6pDqORaS6FApV9vSrPXzjnucZzQULynV1NHLKScs45egujlzaTjKRqNnl/0Rk7lMoVFFP\n/zDf+uGLFIBLzzqCU6yLwxZVd4F7EZF9UShUSS4/zjfve5Fd2RxXXmyce+Ky2S5JRGQParSuknt/\n+Rqvbu5nzTGLOOeEpbNdjojIlBQKVfDCa73c/9gmujoaufLiY9RcJCI1S6EQsf6hUW75zxdJJhN8\n6rLVGn0sIjVNoRCh8UKBb/3wBQaGRrn8vBUceUj7bJckIrJPCoUIPfDrTbzweh/Hr1jA+9YcNtvl\niIjsl0IhIq++1c8Pfv4aHa1pPv67q9SPICIHBYVCBAaHx7j5vucpUOAvPnQcbc3p2S5JRKQsCoUZ\nVigU+M7969kxkOVD7zkSO7xztksSESmbQmGGPfzUZn77Sg/HHN7BpWcdMdvliIhURKEwg97YluGO\nh1+htameP7/0OJJJ9SOIyMFFoTBDRkZzfP3eF8jlC3zikmPpbNMCOCJy8FEozJDbfvwy23p3cfFp\nh3P8igWzXY6IyLQoFGbAr557m8de2MqRh7Tz4XOPmu1yRESmLdI5F8zsRuAMoABc4+5PlOy7DLgO\nyAK3u/tNZtYKfA/oBBqA6939x1HWeKDe3jHEbQ86TQ11/MVlx5GqU86KyMErsiOYmZ0LrHT3M4GP\nA18r2ZcEbgI+CJwDXGpmhwJXAe7u5wOXA/8cVX0zYSyX5+v3vMDo2DhXfWAVizqaZrskEZEDEuWv\ntRcC9wC4+3qg08wmJv9ZCOx09253HwceAi4CeoCJBvnO8H7Nuv3hV3mre5DzTlzKmmMWzXY5IiIH\nLMrmoyXAupL73eG2gfB2m5mtBF4Hzgcecfcvm9lVZvYqQSj87v5epLOzmdQBLGrf1dU2ref96tkt\n/OypzSxf0sZn/uhkGuqnX8NM1hU11VUZ1VUZ1VWZKOqq5jzOxYv23b1gZlcCtwL9wGtAwsz+BHjD\n3S82sxOAbwOn7uub9vXtmnZB010LuWfnMP98+29Jp5J84pJjGdg5/Rpmsq6oqa7KqK7KqK7KHGhd\newuUKJuPthCcGUxYCrw9ccfdH3X397r7JQTB8DrwHuDH4f5ngKVmNrO/gs+A+3/zBsPZHP/td45m\n2cKW2S5HRGTGRBkKDxJ0FmNmJwNb3L0Ya2b2gJktMrMW4FLgp8CrwOnh/uXAoLvnI6xxWrrDs5PT\nj108y5WIiMysyELB3dcC68xsLcGVR1eH/QW/Fz7kFoLg+CVwg7v3ADcDR5jZo8D3gU9FVd+B6M1k\naWlMzXg/gojIbIu0T8Hdr5206ZmSfXcDd096/CDwB1HWNBP6MlkWztPlpyIy92ikVYV2jeQYGc0z\nv11zG4nI3KNQqFBfZgSA+ZrwTkTmIIVChfoyWQDNgioic5JCoUK9xVBonOVKRERmnkKhQsUzBfUp\niMgcpFCokPoURGQuUyhUqHdAfQoiMncpFCrUl8nS3JCiMV3NaaNERKpDoVCh3kxW/QkiMmcpFCow\nnM0xnM2p6UhE5iyFQgV2Dgb9CepkFpG5SqFQAY1REJG5TqFQgd6B4HJUNR+JyFylUKjAxMA1NR+J\nyFylUKjAO6OZ1XwkInOTQqECOlMQkbluv6FgZsdUo5CDQe9AlsZ0HU0NGrgmInNTOUe3u8ysD/g2\ncIe774q4pprVlxlRJ7OIzGn7PVNw9+MI1ko+EnjEzL5pZmsir6zGZMfyDI3k1HQkInNaWX0K7v68\nu38R+CtgFXCfmf3czFZGWl0N6dMYBRGJgf02H5nZcuAq4I+BF4F/AH4MrAH+DTg9wvpqRl84RkFr\nM4vIXFZOn8IjBP0JF7j7lpLtj5vZ45FUVYN6tQyniMRAOc1HJwAvTwSCmX3KzFoB3P2zURZXS9R8\nJCJxUE4ofAdYUnK/GbgtmnJql8YoiEgclBMK8939axN33P2rQEd0JdUmrc0sInFQTig0mNmqiTtm\ndgqQjq6k2tQ7MEK6PkmzBq6JyBxWzhHuc8C9ZjYPqAO6gY9GWlUN6s1k6WxrJJFIzHYpIiKRKWfw\n2m/c/WjgWOBod19FzM4UxnJ5BofH1J8gInNeOeMU2oE/ARaG9xuAPwOWRlta7VAns4jERTl9CncA\nxxMEQRtwCfDpKIuqNepkFpG4KKdPodHdP2Vmj7j7X5vZDcC/APfu74lmdiNwBlAArnH3J0r2XQZc\nB2SB2939JjP7OLv3V5zq7q0V/HsioWU4RSQuygmFBjNrAZJmtsDdd5jZiv09yczOBVa6+5nh1Uu3\nAmeG+5LATcDJwA7gATO7x92/TTB6euL5fzCtf9UM69NoZhGJiXKaj74H/DnwLWC9mb0AbC3jeRcC\n9wC4+3qgM+yfgKB/Yqe7d7v7OPAQcNGk538R+LsyXidyE2szq09BROa6cs4Ubnb3AoCZPQQsAp4u\n43lLgHUl97vDbQPh7bZwltXXgfMJ5lgifJ01wJvuvt/w6exsJpWqK6OcqXV1te33MUPZPAArj1xI\ne0t1Lrwqp67ZoLoqo7oqo7oqE0Vd5YTCwwQHbdx9M7B5mq9VvMDf3QtmdiVBk1I/8FrpfuATwHfL\n+aZ9fdNf86erq43u7sx+H7d1xxD1qSQjQyNkd2Wn/XozXVe1qa7KqK7KqK7KHGhdewuUckLhaTP7\nW2AtMDqx0d0f3s/ztrD7nElLgbdLnv8o8F6AsPP69ZLHngfUzGR7fZksnW0NGrgmInNeOaFwYvj1\nvSXbCgRnEPvyIHA9cLOZnQxscfdirJnZA8CVwBBwKfCVcPtSYNDdR/f8ltWXy48zMDTK0gWxm+5J\nRGJov6Hg7udP5xu7+1ozW2dma4Fx4Gozuwrod/cfALcQBEcBuMHde8KnHgJsn85rRmGnrjwSkRgp\nZ0TzLwgO3Ltx93P291x3v3bSpmdK9t0N3D3Fc9YBH9jf964WjVEQkTgpp/noupLbaeACYDCacmpP\nbya4HFVnCiISB+U0Hz06adNPzOz+iOqpOcV5jzTFhYjEQDnNR0dN2nQYYNGUU3v6BiYmw1PzkYjM\nfeU0Hz1UcrtAMPjsS5FUU4M0xYWIxEk5zUdHmlkynI4CM6t397HoS6sNvZksqboErc31s12KiEjk\n9jv3kZl9hN1nRP2FmV0eXUm1pTczQkdrA0kNXBORGChnQrz/QbDIzoT3hdvmvFx+nIHBUU2EJyKx\nUU4oJNy9f+KOuw8QDEab8/oHRykA89vVySwi8VBOR/OTZnYHwSymSeBidp/9dM5SJ7OIxE05ofDf\ngSuA0wmuPvo34M4oi6oVGrgmInFTTig0A6Pu/lkAM/tUuG3Oj2ru0xQXIhIz5a68VjoFdjNwWzTl\n1BaNZhaRuCknFOa7+9cm7rj7V4FYzCM9sQynmo9EJC7KCYUGM1s1ccfMTiWYGG/O68tkqUsmaG+O\nxT9XRKSsPoXPAfea2TyCEOkBPhppVTWiN5MNBq4lNXBNROJhv2cK7v4bdz8aOJVg0NoW4L6oC5tt\n+fFx+gdH6VR/gojESDmzpJ4B/BnwhwQh8kngrojrmnUDQ2OMFwoazSwisbLXUDCzLwBXAS0EVyCd\nCtzp7rdXp7TZpTEKIhJH+zpT+AfgBeBqd/8ZgJntsSznXDWxjoLGKIhInOwrFA4DrgS+YWZ1wHeJ\nyVVH8M7azGo+EpE42WtHs7tvdfcvu7sBHwPeBSw3sx+a2QerVuEs6ZtoPlJHs4jESDnjFHD3n7v7\nVcBS4D+BL0ZZVC0ojmZW85GIxEg54xSK3D0D3Bz+mdN6M1mSiQTzWmLTYiYiUt6ZQhz1DWSZ15rW\nwDURiRWFwhTGCwV2DmbVySwisaNQmMLA0Cj58YLGKIhI7CgUpqB1FEQkrhQKU+gd0DoKIhJPCoUp\n9GmKCxGJKYXCFDRGQUTiSqEwhXf6FHSmICLxUtHgtUqZ2Y3AGUABuMbdnyjZdxlwHZAFbnf3m8Lt\nVwBfAHLAF939R1HWOJXegRESwLxWDVwTkXiJ7EzBzM4FVrr7mcDHga+V7EsCNwEfBM4BLjWzQ81s\nAfA3wNnAJcBlUdW3L72ZLO2taVJ1OpESkXiJ8kzhQuAeAHdfb2adZtbu7gPAQmCnu3cDmNlDwEXA\nMPDTcDqNDMGCPlU1MXDtsEVt1X5pEZFZF2UoLAHWldzvDrcNhLfbzGwl8DpwPvBI+LhmM7sP6AS+\n5O4P7etFOjubSaXqpl1kV9fuB/+dmSy5fIElC1v22FdNs/na+6K6KqO6KqO6KhNFXZH2KUxSnETI\n3QtmdiVwK9APvFayfwHwe8By4Gdmttzd97q4T1/frmkX1NXVRnd3Zrdtm7YG91vSdXvsq5ap6qoF\nqqsyqqsyqqsyB1rX3gIlylDYQnBmMGEp8PbEHXd/FHgvgJndQHDG0ASsdfccsMHMMkAXsD3COnfT\nq3UURCTGogyFB4HrgZvN7GRgS9hXAICZPUCwstsQcCnwFaAB+K6ZfZmg+agV6Imwxj30DuhyVBGJ\nr8hCwd3Xmtk6M1sLjANXm9lVQL+7/wC4hSA4CsAN7t4DYGb/Afw6/DafdffxqGqcigauiUicRdqn\n4O7XTtr0TMm+u4G7p3jOrC7ioykuRCTOdCH+JBrNLCJxplCYpDeTpb1FA9dEJJ505CtRKBToy2R1\nliAisaVQKDE0kmMsN65lOEUkthQKJXoH1MksIvGmUCjRq05mEYk5hUKJ4hiFdo1REJF4UiiUmBij\noD4FEYkrhUKJPk1xISIxp1AooT4FEYk7hUKJ3kyW1qZ66g9gfQYRkYOZQiEUDFwbUX+CiMSaQiG0\nK5tjdGxcTUciEmsKhdBEJ7MuRxWROFMohNTJLCKiUCjSOgoiIgqFondWXFMoiEh8KRRCxbWZ1acg\nIjGmUAgVm49adaYgIvGlUAj1ZrK0NKZoSGvgmojEl0IhFKy4pqYjEYk3hQIwnM0xMppnfruajkQk\n3hQKaIyCiMgEhQLQp2U4RUQAhQKgMwURkQkKBUoHrqmjWUTiTaFAyTKc6mgWkZhTKPBO81GHBq6J\nSMwpFAiaj5oaUjQ1pGa7FBGRWaVQIFhLQRPhiYhApL8am9mNwBlAAbjG3Z8o2XcZcB2QBW5395vM\n7DzgTuCF8GHPuftno6xxZDTHrmyOo5a2R/kyIiIHhchCwczOBVa6+5lmtgq4FTgz3JcEbgJOBnYA\nD5jZPeFTH3X3y6Oqa7I+XY4qIlIUZfPRhcA9AO6+Hug0s4lfxxcCO929293HgYeAiyKsZa80RkFE\n5B1RNh8tAdaV3O8Otw2Et9vMbCXwOnA+8Eh4+1gzuw+YD1zv7j/Z14t0djaTSk1/ZtNcIQHA8mUd\ndHW1Tfv7zLRaqqWU6qqM6qqM6qpMFHVV83KbxMQNdy+Y2ZUETUr9wGvh/leA64F/B44CfmZm73L3\n0b19076+XdMuqKurjTe27ASgngLd3Zlpf6+Z1NXVVjO1lFJdlVFdlVFdlTnQuvYWKFGGwhaCM4MJ\nS4G3J+64+6PAewHM7AbgdXffDNwRPmSDmW0FlhGERiTUpyAi8o4o+xQeBC4HMLOTgS3uXow1M3vA\nzBaZWQtwKfBTM7vCzD4f7l8CLAY2R1hjSZ+CprgQEYksFNx9LbDOzNYCXwOuNrOrzOz3wofcQhAc\nvwRucPce4D7gXDP7BXAv8Ol9NR3NhN6BLA3pOpoatOKaiEikfQrufu2kTc+U7LsbuHvS4zMEZw1V\n05cZYX5bA4lEYv8PFhGZ42I9onlkNMfQSE6jmUVEQrEOhd7+icV11J8gIgIxD4We/mFAVx6JiEyI\ndyjsDENB6yiIiACxD4VwcR2dKYiIAHEPhWLzkfoUREQg5qGwY+dER7POFEREIOah0NM/TLo+SUuj\nVlwTEYGYh8KO/mE62xo1cE1EJBTbUBjL5ekfHFUns4hIidiGgmZHFRHZk0JBoSAiUhTbUJiYMlvN\nRyIi74htKBTPFNo1RkFEZEJ8Q2FAZwoiIpPFNhR6Mxq4JiIyWWxDYedglvpUktam+tkuRUSkZsR2\nKO/pqxZz+uo6DVwTESkR21B432mH09XVRnd3ZrZLERGpGbFtPhIRkT0pFEREpEihICIiRQoFEREp\nUiiIiEiRQkFERIoUCiIiUqRQEBGRokShUJjtGkREpEboTEFERIoUCiIiUqRQEBGRIoWCiIgUKRRE\nRKRIoSAiIkUKBRERKYrFIjtmdiNwBlAArnH3J0r2XQT8I5AH7nf3v6tybf8HeC/Bz+IGd7+7ZN/r\nwJthbQBXuPvmKtR0HnAn8EK46Tl3/2zJ/ll5z8zs48BHSzad6u6tJfvHgF+V7L/Q3fNExMxWA/cC\nN7r7TWZ2GHAbUAe8DXzU3bOTnrPXz2LEdX0HqAfGgD9x960ljz+Pffy8I6zru8ApwI7wIf/k7j+a\n9JzZeL/uBLrC3fOBX7v7J0sefxXwd8CGcNNP3P0fIqhrt2MD8ARV+HzN+VAws3OBle5+ppmtAm4F\nzix5yNeA9wObgUfN7C53f7FKtZ0PrA5rWwD8Frh70sM+4O6D1ahnkkfd/fK97JuV98zdvw18G4o/\n1z+Y9JB+dz8v6jrC128B/gV4qGTz3wL/6u53mtk/Ah8Dvl7ynP19FqOq6++Bb7r7v5vZ1cBfAV+Y\n9NR9/byjqgvgf7r7f+7lObPyfrn775fsvxX41hRPvcPdPz+TtUyqa6pjw0NU4fMVh+ajC4F7ANx9\nPdBpZu0AZnYU0Ovub7r7OHB/+Phq+Tkw8QHcCbSYWV0VX79iNfCeTfgiwW9rsyULfBDYUrLtPOC+\n8PYPgYsmPWevn8WI6/pL4K4krtKhAAAFI0lEQVTwdjewYIZfsxxT1bU/s/V+AWBmBnS4++Mz/Jrl\n2OPYQJU+X3P+TAFYAqwrud8dbhsIv3aX7NsOrKhWYWHTxlB49+METTGTmzu+YWZHAL8k+K2qWvOS\nHGtm9xGcPl/v7j8Jt8/qewZgZmuAN0ubQEKNZvZ9YDlwl7t/Naoa3D0H5ILjRlFLyen8duCQSU/b\n12cxsrrcfQgg/IXjaoIzmsn29vOOrK7QZ8zsrwjer8+4e0/Jvll5v0pcQ3AWMZVzzey/CJrkPu/u\nv52pmsK69jg2AO+vxucrDmcKkyWmuS8yZnYZwQ/+M5N2fZHgVP88YDXwkSqV9ApwPXAZcCXwbTNL\n7+Wxs/GefQL47hTbPw98EngfcIWZnVrNoiYp532p2nsXBsJtwMPuPrkJp5Kf90y6DbjW3S8Anga+\ntJ/HV/P9SgNnu/vPptj9a+BL7n4xcB3wvQjr2NuxIbLPVxzOFLYQpOWEpQSdNFPtW0Zlp7cHzMze\nD/wv4GJ37y/d5+7fK3nc/cC7gf+IuqawM/uO8O4GM9tK8N68Rg28ZwQhuUdHqLt/Y+K2mT1E8H49\nWb2yGDSzJncfZur3ZV+fxah9B3jF3a+fvGM/P+/ITAqn+yhpHw/N5vt1LjBls5G7vwS8FN5+zMy6\nzKxupi9qmHxsMLOqfL7icKbwIHA5gJmdDGxx9wyAu78OtJvZEWaWAi4JH18VZjYP+CfgEnfvnbzP\nzH5c8hvbucDzVarrCjP7fHh7CbCYoFO5Ft6zpcCgu49O2m5m9n0zS4R1vYd3rqaplp/yztncR4D/\nmrR/r5/FKJnZFcCou//N3vbv7ecdcV13hX1UEAT95M/3rLxfoTXAM1PtMLMvmNkfh7dXA90RBMJU\nx4aqfL5iMXW2mf1v4BxgnKBN9SSCK1V+YGbnAF8OH3qXu//fKtb1SYJT5pdLNj9McEngD8zsGoLT\n+WGCqw8+W40+BTNrA74PdABpgqaFRdTGe3YK8Pfu/oHw/rUEV848ZmZfBi4g+DnfF8VlgpPq+Apw\nBMFlnpuBKwiatRqBTcCfufuYmd0e3h6e/Fl09ykPPDNc1yJghHfall9097+cqIugxWC3n7e731+F\nuv4FuBbYBQwSvEfba+D9+jDBZ/6X7n5HyWPvdffLzOxQgqavJMF797mZ7ozey7HhSoIroSL9fMUi\nFEREpDxxaD4SEZEyKRRERKRIoSAiIkUKBRERKVIoiIhIURwGr4kckHCaEQcem7TrR+7+TzPw/c8j\nuMz27AP9XiIHSqEgUp7uas3AKjKbFAoiB8DMcgSztZ4PtAJXufvzZnY6waCoMYK57T/j7i+a2Urg\nFoKm2xGCwWMAdWb2dYKBlVngd2dpynSJOfUpiByYOuD58Czi67wzA+n3CEa6ng98FfjXcPs3CBaT\nOYdgvvuJ6ZFXEUyydgZBkLy/OuWL7E5nCiLl6TKzRyZtm1io5sfh118Bf21mHcDiklWvHgFuD2+f\nHt7H3W+HYp/CS+6+LXzMWwRTTohUnUJBpDxT9imE8/BPnHEnCJqKJs8dkyjZVmDqM/TcFM8RqTo1\nH4kcuAvCr2cDz4ZToL8d9itAsELWr8Pba4GLAczsD8NlFUVqhs4URMozVfPRxHoDJ5nZp4FO4E/D\nbX8KfNXM8kAe+HS4/TPAN8O1kscI1tmt6sp1IvuiWVJFDoCZFYD6cFlHkYOemo9ERKRIZwoiIlKk\nMwURESlSKIiISJFCQUREihQKIiJSpFAQEZGi/w+ThFFV/mz7yAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracy of CNN\n",
    "print(\"CNN Final Accuracy\", history.history['acc'][-1])\n",
    "pd.Series(history.history['acc']).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGZ3XXt_SJv5"
   },
   "source": [
    "# DEEP CONVOLUTIONAL GAN (DCGAN) NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we build and train a Deep Convolutional GAN, in order to create synthetic fake images that look similar to the original MNIST data. \n",
    "- The GAN network consists of a generator and discriminator that play against each other. \n",
    "- The discriminator is a referee that tries to tell the difference between real and fake images, whereas the generator tries to make the discriminator's life hard. In equilibrium the discrimnator cannot tell the difference between real and fake images, resulting in an accuracy of 50% (i.e., random guessing).\n",
    "- The training process is quite involved, with several comments trying to elucidate the idea behind this non-standard training process. \n",
    "- At the end we plot some fake images generated by our GAN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCjEWofkVP7o"
   },
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hyU8ONlE65f"
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.D = None   # discriminator\n",
    "        self.G = None   # generator\n",
    "        self.AM = None  # adversarial model (discrimnator stacked on top of generator)\n",
    "        self.DM = None  # discriminator model\n",
    "        \n",
    "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8, \\\n",
    "                  window=5, input_dim=100, output_depth=1):\n",
    "        if self.G:\n",
    "            return self.G\n",
    "        self.G = Sequential()\n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        self.G.add(Dropout(dropout))\n",
    "        \n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(UpSampling2D())\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=momentum))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "        return self.G\n",
    "\n",
    "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape,\n",
    "            padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=alpha))\n",
    "        self.D.add(Dropout(dropout))\n",
    "\n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1))\n",
    "        self.D.add(Activation('sigmoid'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', \\\n",
    "                        optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM\n",
    "        \n",
    "class MNIST_DCGAN(object):\n",
    "    def __init__(self, x_train):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = x_train\n",
    "\n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator =  self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        \n",
    "        # loop over epochs\n",
    "        for i in range(train_steps):\n",
    "            # get random REAL training samples \n",
    "            images_train = self.x_train[np.random.randint(0, self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            # noise vector as input for generator\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            # generate fake images\n",
    "            images_fake = self.generator.predict(noise)\n",
    "            # labeled sample contains real and fake images\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            # build label vector (real=1, fake=0)\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            \n",
    "            # train discriminator with real and fake images\n",
    "            # keras train_on_batch runs a single gradient update on a single batch of data\n",
    "            # returns loss and accuracy in list of scalars\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "            \n",
    "            # train adversial network (generator+discriminator) with fake images\n",
    "            # NOTE: here fake images are labelled with y=1, since lose corresponds to fake image detected as fake\n",
    "            # generator does well if fake image declared as real in output of adversial network\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            # track training progress in log message\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            \n",
    "            # print log messages\n",
    "            print(log_mesg)\n",
    "            \n",
    "            # plot and save fake sample during training every interval steps\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, \n",
    "                        samples=noise_input.shape[0],\n",
    "                        noise=noise_input, step=(i+1))\n",
    "\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16,\n",
    "                    noise=None, step=0):\n",
    "        \n",
    "        # location for storing fake images \n",
    "        current_path = os.getcwd()\n",
    "        file = '/drive/My Drive/Colab Notebooks/images/synthetic_mnist'\n",
    "        filename = 'mnist.png'\n",
    "        \n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(current_path+file+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86377
    },
    "colab_type": "code",
    "id": "XwK6kWtfE65h",
    "outputId": "85e10b2a-e58b-4888-ffe2-240e28210f1f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_12 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.698849, acc: 0.367188]  [A loss: 1.342889, acc: 0.000000]\n",
      "1: [D loss: 0.677229, acc: 0.500000]  [A loss: 1.250711, acc: 0.000000]\n",
      "2: [D loss: 0.610463, acc: 0.992188]  [A loss: 1.465077, acc: 0.000000]\n",
      "3: [D loss: 0.509561, acc: 1.000000]  [A loss: 2.060647, acc: 0.000000]\n",
      "4: [D loss: 0.489140, acc: 0.998047]  [A loss: 0.701772, acc: 0.488281]\n",
      "5: [D loss: 2.159252, acc: 0.500000]  [A loss: 1.855667, acc: 0.000000]\n",
      "6: [D loss: 0.329798, acc: 0.982422]  [A loss: 1.349981, acc: 0.000000]\n",
      "7: [D loss: 0.411826, acc: 0.878906]  [A loss: 1.609497, acc: 0.000000]\n",
      "8: [D loss: 0.554482, acc: 0.517578]  [A loss: 2.349176, acc: 0.000000]\n",
      "9: [D loss: 0.486579, acc: 0.744141]  [A loss: 2.159620, acc: 0.000000]\n",
      "10: [D loss: 0.456830, acc: 0.818359]  [A loss: 2.086791, acc: 0.000000]\n",
      "11: [D loss: 0.440051, acc: 0.796875]  [A loss: 2.275716, acc: 0.000000]\n",
      "12: [D loss: 0.390693, acc: 0.951172]  [A loss: 2.100192, acc: 0.000000]\n",
      "13: [D loss: 0.410334, acc: 0.802734]  [A loss: 2.716871, acc: 0.000000]\n",
      "14: [D loss: 0.299349, acc: 1.000000]  [A loss: 1.636946, acc: 0.000000]\n",
      "15: [D loss: 0.710302, acc: 0.501953]  [A loss: 4.339769, acc: 0.000000]\n",
      "16: [D loss: 0.531630, acc: 0.662109]  [A loss: 1.743137, acc: 0.000000]\n",
      "17: [D loss: 0.303537, acc: 0.998047]  [A loss: 1.688210, acc: 0.000000]\n",
      "18: [D loss: 0.353141, acc: 0.882812]  [A loss: 2.182056, acc: 0.000000]\n",
      "19: [D loss: 0.243620, acc: 1.000000]  [A loss: 1.966935, acc: 0.000000]\n",
      "20: [D loss: 0.332586, acc: 0.904297]  [A loss: 2.735983, acc: 0.000000]\n",
      "21: [D loss: 0.209313, acc: 1.000000]  [A loss: 1.937182, acc: 0.000000]\n",
      "22: [D loss: 0.372124, acc: 0.751953]  [A loss: 3.340341, acc: 0.000000]\n",
      "23: [D loss: 0.237274, acc: 0.986328]  [A loss: 1.723042, acc: 0.000000]\n",
      "24: [D loss: 0.411077, acc: 0.679688]  [A loss: 3.171693, acc: 0.000000]\n",
      "25: [D loss: 0.216087, acc: 0.990234]  [A loss: 1.835705, acc: 0.000000]\n",
      "26: [D loss: 0.317668, acc: 0.878906]  [A loss: 2.780334, acc: 0.000000]\n",
      "27: [D loss: 0.159339, acc: 1.000000]  [A loss: 1.951554, acc: 0.000000]\n",
      "28: [D loss: 0.277439, acc: 0.943359]  [A loss: 2.907876, acc: 0.000000]\n",
      "29: [D loss: 0.153746, acc: 0.998047]  [A loss: 1.992667, acc: 0.000000]\n",
      "30: [D loss: 0.280046, acc: 0.929688]  [A loss: 3.121609, acc: 0.000000]\n",
      "31: [D loss: 0.148917, acc: 1.000000]  [A loss: 2.047045, acc: 0.000000]\n",
      "32: [D loss: 0.262291, acc: 0.945312]  [A loss: 3.123983, acc: 0.000000]\n",
      "33: [D loss: 0.140064, acc: 1.000000]  [A loss: 2.118248, acc: 0.000000]\n",
      "34: [D loss: 0.258008, acc: 0.943359]  [A loss: 3.248542, acc: 0.000000]\n",
      "35: [D loss: 0.138689, acc: 1.000000]  [A loss: 2.156303, acc: 0.000000]\n",
      "36: [D loss: 0.237803, acc: 0.953125]  [A loss: 3.212658, acc: 0.000000]\n",
      "37: [D loss: 0.125520, acc: 1.000000]  [A loss: 2.158983, acc: 0.000000]\n",
      "38: [D loss: 0.241322, acc: 0.953125]  [A loss: 3.317765, acc: 0.000000]\n",
      "39: [D loss: 0.128552, acc: 1.000000]  [A loss: 2.105864, acc: 0.000000]\n",
      "40: [D loss: 0.453440, acc: 0.648438]  [A loss: 3.748667, acc: 0.000000]\n",
      "41: [D loss: 0.400896, acc: 0.818359]  [A loss: 1.061252, acc: 0.058594]\n",
      "42: [D loss: 0.657069, acc: 0.511719]  [A loss: 2.639596, acc: 0.000000]\n",
      "43: [D loss: 0.282528, acc: 0.945312]  [A loss: 1.255085, acc: 0.015625]\n",
      "44: [D loss: 0.434868, acc: 0.677734]  [A loss: 2.115853, acc: 0.000000]\n",
      "45: [D loss: 0.277756, acc: 0.972656]  [A loss: 1.465492, acc: 0.000000]\n",
      "46: [D loss: 0.403335, acc: 0.740234]  [A loss: 2.223359, acc: 0.000000]\n",
      "47: [D loss: 0.311302, acc: 0.962891]  [A loss: 1.478439, acc: 0.000000]\n",
      "48: [D loss: 0.463058, acc: 0.681641]  [A loss: 2.531964, acc: 0.000000]\n",
      "49: [D loss: 0.358516, acc: 0.935547]  [A loss: 1.050823, acc: 0.097656]\n",
      "50: [D loss: 0.683293, acc: 0.521484]  [A loss: 2.609125, acc: 0.000000]\n",
      "51: [D loss: 0.395739, acc: 0.882812]  [A loss: 1.021658, acc: 0.085938]\n",
      "52: [D loss: 0.591523, acc: 0.533203]  [A loss: 2.024857, acc: 0.000000]\n",
      "53: [D loss: 0.344069, acc: 0.957031]  [A loss: 1.173004, acc: 0.027344]\n",
      "54: [D loss: 0.519147, acc: 0.585938]  [A loss: 2.123799, acc: 0.000000]\n",
      "55: [D loss: 0.376114, acc: 0.939453]  [A loss: 1.105678, acc: 0.050781]\n",
      "56: [D loss: 0.562643, acc: 0.558594]  [A loss: 2.233074, acc: 0.000000]\n",
      "57: [D loss: 0.364543, acc: 0.947266]  [A loss: 1.055445, acc: 0.058594]\n",
      "58: [D loss: 0.568751, acc: 0.560547]  [A loss: 2.250038, acc: 0.000000]\n",
      "59: [D loss: 0.363388, acc: 0.949219]  [A loss: 1.056798, acc: 0.074219]\n",
      "60: [D loss: 0.560664, acc: 0.558594]  [A loss: 2.134498, acc: 0.000000]\n",
      "61: [D loss: 0.352949, acc: 0.953125]  [A loss: 1.115435, acc: 0.046875]\n",
      "62: [D loss: 0.564770, acc: 0.556641]  [A loss: 2.278324, acc: 0.000000]\n",
      "63: [D loss: 0.387855, acc: 0.927734]  [A loss: 0.881131, acc: 0.203125]\n",
      "64: [D loss: 0.697619, acc: 0.513672]  [A loss: 2.227577, acc: 0.000000]\n",
      "65: [D loss: 0.386169, acc: 0.923828]  [A loss: 0.939270, acc: 0.117188]\n",
      "66: [D loss: 0.600896, acc: 0.525391]  [A loss: 2.010110, acc: 0.000000]\n",
      "67: [D loss: 0.360029, acc: 0.958984]  [A loss: 1.097077, acc: 0.031250]\n",
      "68: [D loss: 0.553606, acc: 0.541016]  [A loss: 2.112520, acc: 0.000000]\n",
      "69: [D loss: 0.372245, acc: 0.951172]  [A loss: 1.000805, acc: 0.074219]\n",
      "70: [D loss: 0.624763, acc: 0.527344]  [A loss: 2.222957, acc: 0.000000]\n",
      "71: [D loss: 0.385359, acc: 0.927734]  [A loss: 0.884249, acc: 0.179688]\n",
      "72: [D loss: 0.662449, acc: 0.503906]  [A loss: 2.168528, acc: 0.000000]\n",
      "73: [D loss: 0.384499, acc: 0.951172]  [A loss: 0.922672, acc: 0.140625]\n",
      "74: [D loss: 0.621756, acc: 0.507812]  [A loss: 2.031044, acc: 0.000000]\n",
      "75: [D loss: 0.381864, acc: 0.960938]  [A loss: 1.051198, acc: 0.054688]\n",
      "76: [D loss: 0.570136, acc: 0.544922]  [A loss: 2.089628, acc: 0.000000]\n",
      "77: [D loss: 0.379368, acc: 0.964844]  [A loss: 1.031215, acc: 0.058594]\n",
      "78: [D loss: 0.607988, acc: 0.521484]  [A loss: 2.204571, acc: 0.000000]\n",
      "79: [D loss: 0.384534, acc: 0.958984]  [A loss: 0.919497, acc: 0.144531]\n",
      "80: [D loss: 0.621143, acc: 0.513672]  [A loss: 2.160853, acc: 0.000000]\n",
      "81: [D loss: 0.376393, acc: 0.957031]  [A loss: 0.966531, acc: 0.089844]\n",
      "82: [D loss: 0.599335, acc: 0.515625]  [A loss: 2.202548, acc: 0.000000]\n",
      "83: [D loss: 0.392919, acc: 0.933594]  [A loss: 0.868692, acc: 0.167969]\n",
      "84: [D loss: 0.675674, acc: 0.501953]  [A loss: 2.199133, acc: 0.000000]\n",
      "85: [D loss: 0.405452, acc: 0.931641]  [A loss: 0.758231, acc: 0.382812]\n",
      "86: [D loss: 0.690554, acc: 0.500000]  [A loss: 2.063931, acc: 0.000000]\n",
      "87: [D loss: 0.383553, acc: 0.966797]  [A loss: 0.962950, acc: 0.050781]\n",
      "88: [D loss: 0.607402, acc: 0.503906]  [A loss: 2.090138, acc: 0.000000]\n",
      "89: [D loss: 0.376874, acc: 0.970703]  [A loss: 1.070387, acc: 0.019531]\n",
      "90: [D loss: 0.560951, acc: 0.509766]  [A loss: 2.227281, acc: 0.000000]\n",
      "91: [D loss: 0.373298, acc: 0.957031]  [A loss: 0.970103, acc: 0.066406]\n",
      "92: [D loss: 0.597297, acc: 0.509766]  [A loss: 2.288836, acc: 0.000000]\n",
      "93: [D loss: 0.376746, acc: 0.943359]  [A loss: 0.847574, acc: 0.191406]\n",
      "94: [D loss: 0.659279, acc: 0.503906]  [A loss: 2.351698, acc: 0.000000]\n",
      "95: [D loss: 0.381975, acc: 0.925781]  [A loss: 0.858286, acc: 0.199219]\n",
      "96: [D loss: 0.662094, acc: 0.503906]  [A loss: 2.217268, acc: 0.000000]\n",
      "97: [D loss: 0.376191, acc: 0.958984]  [A loss: 0.937142, acc: 0.089844]\n",
      "98: [D loss: 0.599298, acc: 0.503906]  [A loss: 2.177409, acc: 0.000000]\n",
      "99: [D loss: 0.364210, acc: 0.962891]  [A loss: 1.051028, acc: 0.031250]\n",
      "100: [D loss: 0.557464, acc: 0.513672]  [A loss: 2.296309, acc: 0.000000]\n",
      "101: [D loss: 0.356167, acc: 0.960938]  [A loss: 1.001680, acc: 0.039062]\n",
      "102: [D loss: 0.615296, acc: 0.503906]  [A loss: 2.417088, acc: 0.000000]\n",
      "103: [D loss: 0.368937, acc: 0.943359]  [A loss: 0.868630, acc: 0.140625]\n",
      "104: [D loss: 0.665321, acc: 0.503906]  [A loss: 2.296225, acc: 0.000000]\n",
      "105: [D loss: 0.369537, acc: 0.949219]  [A loss: 0.933554, acc: 0.109375]\n",
      "106: [D loss: 0.640797, acc: 0.501953]  [A loss: 2.297642, acc: 0.000000]\n",
      "107: [D loss: 0.380014, acc: 0.941406]  [A loss: 0.884404, acc: 0.171875]\n",
      "108: [D loss: 0.673874, acc: 0.500000]  [A loss: 2.237269, acc: 0.000000]\n",
      "109: [D loss: 0.392078, acc: 0.953125]  [A loss: 0.965406, acc: 0.148438]\n",
      "110: [D loss: 0.655575, acc: 0.505859]  [A loss: 2.324270, acc: 0.000000]\n",
      "111: [D loss: 0.416490, acc: 0.919922]  [A loss: 0.863766, acc: 0.234375]\n",
      "112: [D loss: 0.741410, acc: 0.501953]  [A loss: 2.410132, acc: 0.000000]\n",
      "113: [D loss: 0.452044, acc: 0.908203]  [A loss: 0.795294, acc: 0.316406]\n",
      "114: [D loss: 0.747431, acc: 0.505859]  [A loss: 2.141721, acc: 0.000000]\n",
      "115: [D loss: 0.442511, acc: 0.904297]  [A loss: 0.981480, acc: 0.097656]\n",
      "116: [D loss: 0.674598, acc: 0.513672]  [A loss: 2.235623, acc: 0.000000]\n",
      "117: [D loss: 0.468582, acc: 0.878906]  [A loss: 0.897271, acc: 0.203125]\n",
      "118: [D loss: 0.711769, acc: 0.509766]  [A loss: 2.229468, acc: 0.000000]\n",
      "119: [D loss: 0.490669, acc: 0.869141]  [A loss: 0.722624, acc: 0.449219]\n",
      "120: [D loss: 0.801219, acc: 0.501953]  [A loss: 2.178841, acc: 0.000000]\n",
      "121: [D loss: 0.490996, acc: 0.841797]  [A loss: 0.812831, acc: 0.347656]\n",
      "122: [D loss: 0.765339, acc: 0.503906]  [A loss: 2.193747, acc: 0.000000]\n",
      "123: [D loss: 0.489455, acc: 0.863281]  [A loss: 0.818630, acc: 0.285156]\n",
      "124: [D loss: 0.768439, acc: 0.503906]  [A loss: 2.263123, acc: 0.000000]\n",
      "125: [D loss: 0.505237, acc: 0.859375]  [A loss: 0.820259, acc: 0.324219]\n",
      "126: [D loss: 0.793175, acc: 0.501953]  [A loss: 2.246431, acc: 0.000000]\n",
      "127: [D loss: 0.513342, acc: 0.837891]  [A loss: 0.829693, acc: 0.281250]\n",
      "128: [D loss: 0.734504, acc: 0.505859]  [A loss: 2.166041, acc: 0.000000]\n",
      "129: [D loss: 0.514841, acc: 0.818359]  [A loss: 0.792748, acc: 0.363281]\n",
      "130: [D loss: 0.762004, acc: 0.501953]  [A loss: 2.197145, acc: 0.000000]\n",
      "131: [D loss: 0.519926, acc: 0.843750]  [A loss: 0.849999, acc: 0.273438]\n",
      "132: [D loss: 0.748568, acc: 0.505859]  [A loss: 2.259992, acc: 0.000000]\n",
      "133: [D loss: 0.517302, acc: 0.847656]  [A loss: 0.733657, acc: 0.476562]\n",
      "134: [D loss: 0.802445, acc: 0.500000]  [A loss: 2.176605, acc: 0.000000]\n",
      "135: [D loss: 0.554338, acc: 0.781250]  [A loss: 0.625623, acc: 0.667969]\n",
      "136: [D loss: 0.833068, acc: 0.496094]  [A loss: 2.050092, acc: 0.000000]\n",
      "137: [D loss: 0.554821, acc: 0.779297]  [A loss: 0.765099, acc: 0.371094]\n",
      "138: [D loss: 0.736159, acc: 0.501953]  [A loss: 1.974154, acc: 0.000000]\n",
      "139: [D loss: 0.552673, acc: 0.783203]  [A loss: 0.849623, acc: 0.257812]\n",
      "140: [D loss: 0.710481, acc: 0.501953]  [A loss: 2.135220, acc: 0.000000]\n",
      "141: [D loss: 0.551838, acc: 0.792969]  [A loss: 0.771679, acc: 0.375000]\n",
      "142: [D loss: 0.743614, acc: 0.500000]  [A loss: 2.169031, acc: 0.000000]\n",
      "143: [D loss: 0.554163, acc: 0.789062]  [A loss: 0.662510, acc: 0.593750]\n",
      "144: [D loss: 0.785110, acc: 0.500000]  [A loss: 2.004662, acc: 0.000000]\n",
      "145: [D loss: 0.557790, acc: 0.806641]  [A loss: 0.724752, acc: 0.445312]\n",
      "146: [D loss: 0.738540, acc: 0.496094]  [A loss: 1.989800, acc: 0.000000]\n",
      "147: [D loss: 0.562132, acc: 0.781250]  [A loss: 0.735757, acc: 0.457031]\n",
      "148: [D loss: 0.750039, acc: 0.501953]  [A loss: 2.062009, acc: 0.000000]\n",
      "149: [D loss: 0.570548, acc: 0.763672]  [A loss: 0.707255, acc: 0.535156]\n",
      "150: [D loss: 0.760139, acc: 0.496094]  [A loss: 2.073937, acc: 0.000000]\n",
      "151: [D loss: 0.573081, acc: 0.763672]  [A loss: 0.635975, acc: 0.660156]\n",
      "152: [D loss: 0.782442, acc: 0.498047]  [A loss: 1.995486, acc: 0.000000]\n",
      "153: [D loss: 0.582468, acc: 0.753906]  [A loss: 0.670963, acc: 0.546875]\n",
      "154: [D loss: 0.748525, acc: 0.501953]  [A loss: 1.838498, acc: 0.000000]\n",
      "155: [D loss: 0.575035, acc: 0.777344]  [A loss: 0.773627, acc: 0.382812]\n",
      "156: [D loss: 0.736968, acc: 0.498047]  [A loss: 2.049294, acc: 0.000000]\n",
      "157: [D loss: 0.574461, acc: 0.757812]  [A loss: 0.634248, acc: 0.656250]\n",
      "158: [D loss: 0.779001, acc: 0.501953]  [A loss: 1.881582, acc: 0.000000]\n",
      "159: [D loss: 0.563683, acc: 0.808594]  [A loss: 0.665745, acc: 0.601562]\n",
      "160: [D loss: 0.755427, acc: 0.501953]  [A loss: 1.828346, acc: 0.000000]\n",
      "161: [D loss: 0.586562, acc: 0.759766]  [A loss: 0.719151, acc: 0.488281]\n",
      "162: [D loss: 0.754462, acc: 0.496094]  [A loss: 1.947642, acc: 0.000000]\n",
      "163: [D loss: 0.580164, acc: 0.769531]  [A loss: 0.653871, acc: 0.625000]\n",
      "164: [D loss: 0.741693, acc: 0.501953]  [A loss: 1.993954, acc: 0.000000]\n",
      "165: [D loss: 0.588373, acc: 0.736328]  [A loss: 0.580539, acc: 0.753906]\n",
      "166: [D loss: 0.803289, acc: 0.498047]  [A loss: 1.935151, acc: 0.000000]\n",
      "167: [D loss: 0.580159, acc: 0.750000]  [A loss: 0.653682, acc: 0.628906]\n",
      "168: [D loss: 0.732566, acc: 0.498047]  [A loss: 1.749219, acc: 0.000000]\n",
      "169: [D loss: 0.579014, acc: 0.775391]  [A loss: 0.801367, acc: 0.261719]\n",
      "170: [D loss: 0.720426, acc: 0.501953]  [A loss: 2.042232, acc: 0.000000]\n",
      "171: [D loss: 0.568008, acc: 0.775391]  [A loss: 0.642626, acc: 0.621094]\n",
      "172: [D loss: 0.756239, acc: 0.500000]  [A loss: 2.016568, acc: 0.000000]\n",
      "173: [D loss: 0.587346, acc: 0.746094]  [A loss: 0.617568, acc: 0.718750]\n",
      "174: [D loss: 0.772884, acc: 0.494141]  [A loss: 1.863006, acc: 0.000000]\n",
      "175: [D loss: 0.575369, acc: 0.777344]  [A loss: 0.746134, acc: 0.410156]\n",
      "176: [D loss: 0.741182, acc: 0.501953]  [A loss: 1.973855, acc: 0.000000]\n",
      "177: [D loss: 0.591977, acc: 0.718750]  [A loss: 0.594619, acc: 0.789062]\n",
      "178: [D loss: 0.791888, acc: 0.500000]  [A loss: 1.825472, acc: 0.000000]\n",
      "179: [D loss: 0.595018, acc: 0.738281]  [A loss: 0.705936, acc: 0.464844]\n",
      "180: [D loss: 0.732665, acc: 0.501953]  [A loss: 1.719591, acc: 0.000000]\n",
      "181: [D loss: 0.573863, acc: 0.796875]  [A loss: 0.799976, acc: 0.300781]\n",
      "182: [D loss: 0.706995, acc: 0.505859]  [A loss: 1.880490, acc: 0.000000]\n",
      "183: [D loss: 0.587135, acc: 0.748047]  [A loss: 0.643115, acc: 0.652344]\n",
      "184: [D loss: 0.770690, acc: 0.500000]  [A loss: 2.028397, acc: 0.000000]\n",
      "185: [D loss: 0.587659, acc: 0.714844]  [A loss: 0.583646, acc: 0.781250]\n",
      "186: [D loss: 0.775459, acc: 0.500000]  [A loss: 1.718349, acc: 0.000000]\n",
      "187: [D loss: 0.588960, acc: 0.753906]  [A loss: 0.728162, acc: 0.402344]\n",
      "188: [D loss: 0.704611, acc: 0.507812]  [A loss: 1.670085, acc: 0.000000]\n",
      "189: [D loss: 0.586589, acc: 0.773438]  [A loss: 0.793638, acc: 0.281250]\n",
      "190: [D loss: 0.701018, acc: 0.505859]  [A loss: 1.825435, acc: 0.000000]\n",
      "191: [D loss: 0.590454, acc: 0.761719]  [A loss: 0.753554, acc: 0.378906]\n",
      "192: [D loss: 0.726997, acc: 0.500000]  [A loss: 1.886372, acc: 0.000000]\n",
      "193: [D loss: 0.580950, acc: 0.722656]  [A loss: 0.657301, acc: 0.613281]\n",
      "194: [D loss: 0.752527, acc: 0.498047]  [A loss: 1.775987, acc: 0.000000]\n",
      "195: [D loss: 0.599898, acc: 0.701172]  [A loss: 0.680988, acc: 0.570312]\n",
      "196: [D loss: 0.750497, acc: 0.501953]  [A loss: 1.722550, acc: 0.000000]\n",
      "197: [D loss: 0.595134, acc: 0.734375]  [A loss: 0.700388, acc: 0.507812]\n",
      "198: [D loss: 0.726937, acc: 0.500000]  [A loss: 1.674206, acc: 0.000000]\n",
      "199: [D loss: 0.611383, acc: 0.714844]  [A loss: 0.732486, acc: 0.445312]\n",
      "200: [D loss: 0.733408, acc: 0.509766]  [A loss: 1.672046, acc: 0.000000]\n",
      "201: [D loss: 0.603870, acc: 0.707031]  [A loss: 0.740225, acc: 0.449219]\n",
      "202: [D loss: 0.743203, acc: 0.511719]  [A loss: 1.735650, acc: 0.000000]\n",
      "203: [D loss: 0.610512, acc: 0.726562]  [A loss: 0.678928, acc: 0.546875]\n",
      "204: [D loss: 0.753779, acc: 0.507812]  [A loss: 1.719412, acc: 0.000000]\n",
      "205: [D loss: 0.635122, acc: 0.630859]  [A loss: 0.627222, acc: 0.691406]\n",
      "206: [D loss: 0.781879, acc: 0.501953]  [A loss: 1.532218, acc: 0.000000]\n",
      "207: [D loss: 0.630046, acc: 0.669922]  [A loss: 0.739950, acc: 0.429688]\n",
      "208: [D loss: 0.719847, acc: 0.505859]  [A loss: 1.480201, acc: 0.000000]\n",
      "209: [D loss: 0.621257, acc: 0.701172]  [A loss: 0.801053, acc: 0.292969]\n",
      "210: [D loss: 0.698318, acc: 0.523438]  [A loss: 1.477743, acc: 0.000000]\n",
      "211: [D loss: 0.600359, acc: 0.759766]  [A loss: 0.766804, acc: 0.332031]\n",
      "212: [D loss: 0.681700, acc: 0.509766]  [A loss: 1.541796, acc: 0.000000]\n",
      "213: [D loss: 0.611807, acc: 0.705078]  [A loss: 0.713199, acc: 0.492188]\n",
      "214: [D loss: 0.702747, acc: 0.505859]  [A loss: 1.508817, acc: 0.000000]\n",
      "215: [D loss: 0.608237, acc: 0.742188]  [A loss: 0.665154, acc: 0.593750]\n",
      "216: [D loss: 0.767503, acc: 0.501953]  [A loss: 1.703826, acc: 0.000000]\n",
      "217: [D loss: 0.621222, acc: 0.675781]  [A loss: 0.627555, acc: 0.679688]\n",
      "218: [D loss: 0.748693, acc: 0.500000]  [A loss: 1.510942, acc: 0.000000]\n",
      "219: [D loss: 0.629388, acc: 0.667969]  [A loss: 0.706842, acc: 0.488281]\n",
      "220: [D loss: 0.725592, acc: 0.505859]  [A loss: 1.471407, acc: 0.000000]\n",
      "221: [D loss: 0.607846, acc: 0.763672]  [A loss: 0.782419, acc: 0.296875]\n",
      "222: [D loss: 0.687956, acc: 0.519531]  [A loss: 1.349863, acc: 0.000000]\n",
      "223: [D loss: 0.626402, acc: 0.693359]  [A loss: 0.802855, acc: 0.273438]\n",
      "224: [D loss: 0.695030, acc: 0.515625]  [A loss: 1.649751, acc: 0.000000]\n",
      "225: [D loss: 0.627620, acc: 0.675781]  [A loss: 0.617188, acc: 0.730469]\n",
      "226: [D loss: 0.771901, acc: 0.507812]  [A loss: 1.650390, acc: 0.000000]\n",
      "227: [D loss: 0.642900, acc: 0.609375]  [A loss: 0.582995, acc: 0.796875]\n",
      "228: [D loss: 0.779166, acc: 0.500000]  [A loss: 1.421579, acc: 0.000000]\n",
      "229: [D loss: 0.627332, acc: 0.691406]  [A loss: 0.747892, acc: 0.433594]\n",
      "230: [D loss: 0.703405, acc: 0.515625]  [A loss: 1.394058, acc: 0.000000]\n",
      "231: [D loss: 0.630958, acc: 0.683594]  [A loss: 0.755214, acc: 0.398438]\n",
      "232: [D loss: 0.720224, acc: 0.509766]  [A loss: 1.388117, acc: 0.000000]\n",
      "233: [D loss: 0.633358, acc: 0.714844]  [A loss: 0.650272, acc: 0.636719]\n",
      "234: [D loss: 0.752566, acc: 0.496094]  [A loss: 1.489936, acc: 0.000000]\n",
      "235: [D loss: 0.646119, acc: 0.646484]  [A loss: 0.630575, acc: 0.703125]\n",
      "236: [D loss: 0.759319, acc: 0.501953]  [A loss: 1.365002, acc: 0.000000]\n",
      "237: [D loss: 0.635828, acc: 0.677734]  [A loss: 0.757530, acc: 0.359375]\n",
      "238: [D loss: 0.697380, acc: 0.533203]  [A loss: 1.270564, acc: 0.000000]\n",
      "239: [D loss: 0.627707, acc: 0.701172]  [A loss: 0.725715, acc: 0.441406]\n",
      "240: [D loss: 0.733747, acc: 0.492188]  [A loss: 1.489648, acc: 0.000000]\n",
      "241: [D loss: 0.651017, acc: 0.626953]  [A loss: 0.690187, acc: 0.542969]\n",
      "242: [D loss: 0.731408, acc: 0.505859]  [A loss: 1.377900, acc: 0.000000]\n",
      "243: [D loss: 0.655177, acc: 0.625000]  [A loss: 0.681044, acc: 0.570312]\n",
      "244: [D loss: 0.724639, acc: 0.500000]  [A loss: 1.443443, acc: 0.000000]\n",
      "245: [D loss: 0.645052, acc: 0.652344]  [A loss: 0.732284, acc: 0.425781]\n",
      "246: [D loss: 0.693115, acc: 0.500000]  [A loss: 1.232625, acc: 0.000000]\n",
      "247: [D loss: 0.640070, acc: 0.666016]  [A loss: 0.783303, acc: 0.292969]\n",
      "248: [D loss: 0.685586, acc: 0.525391]  [A loss: 1.330614, acc: 0.003906]\n",
      "249: [D loss: 0.652494, acc: 0.632812]  [A loss: 0.744511, acc: 0.398438]\n",
      "250: [D loss: 0.708605, acc: 0.507812]  [A loss: 1.437003, acc: 0.000000]\n",
      "251: [D loss: 0.649497, acc: 0.636719]  [A loss: 0.636644, acc: 0.699219]\n",
      "252: [D loss: 0.752256, acc: 0.496094]  [A loss: 1.490344, acc: 0.000000]\n",
      "253: [D loss: 0.668864, acc: 0.582031]  [A loss: 0.586046, acc: 0.832031]\n",
      "254: [D loss: 0.757275, acc: 0.494141]  [A loss: 1.137402, acc: 0.000000]\n",
      "255: [D loss: 0.643716, acc: 0.638672]  [A loss: 0.776135, acc: 0.296875]\n",
      "256: [D loss: 0.687437, acc: 0.523438]  [A loss: 1.105664, acc: 0.000000]\n",
      "257: [D loss: 0.646821, acc: 0.648438]  [A loss: 0.788327, acc: 0.273438]\n",
      "258: [D loss: 0.704537, acc: 0.511719]  [A loss: 1.203676, acc: 0.000000]\n",
      "259: [D loss: 0.644717, acc: 0.679688]  [A loss: 0.747854, acc: 0.363281]\n",
      "260: [D loss: 0.716080, acc: 0.501953]  [A loss: 1.228934, acc: 0.000000]\n",
      "261: [D loss: 0.654924, acc: 0.619141]  [A loss: 0.791639, acc: 0.230469]\n",
      "262: [D loss: 0.692275, acc: 0.525391]  [A loss: 1.144084, acc: 0.000000]\n",
      "263: [D loss: 0.653243, acc: 0.630859]  [A loss: 0.750723, acc: 0.339844]\n",
      "264: [D loss: 0.710238, acc: 0.501953]  [A loss: 1.329027, acc: 0.000000]\n",
      "265: [D loss: 0.651646, acc: 0.646484]  [A loss: 0.705626, acc: 0.449219]\n",
      "266: [D loss: 0.728138, acc: 0.501953]  [A loss: 1.317790, acc: 0.000000]\n",
      "267: [D loss: 0.669957, acc: 0.574219]  [A loss: 0.638458, acc: 0.683594]\n",
      "268: [D loss: 0.738137, acc: 0.501953]  [A loss: 1.275823, acc: 0.000000]\n",
      "269: [D loss: 0.657874, acc: 0.628906]  [A loss: 0.682542, acc: 0.527344]\n",
      "270: [D loss: 0.704081, acc: 0.513672]  [A loss: 1.186530, acc: 0.000000]\n",
      "271: [D loss: 0.658160, acc: 0.632812]  [A loss: 0.696878, acc: 0.503906]\n",
      "272: [D loss: 0.730425, acc: 0.498047]  [A loss: 1.252768, acc: 0.000000]\n",
      "273: [D loss: 0.653410, acc: 0.632812]  [A loss: 0.711178, acc: 0.476562]\n",
      "274: [D loss: 0.707903, acc: 0.492188]  [A loss: 1.221799, acc: 0.000000]\n",
      "275: [D loss: 0.667466, acc: 0.603516]  [A loss: 0.729129, acc: 0.406250]\n",
      "276: [D loss: 0.701381, acc: 0.503906]  [A loss: 1.203904, acc: 0.000000]\n",
      "277: [D loss: 0.663236, acc: 0.593750]  [A loss: 0.737404, acc: 0.335938]\n",
      "278: [D loss: 0.695046, acc: 0.511719]  [A loss: 1.145766, acc: 0.000000]\n",
      "279: [D loss: 0.657742, acc: 0.646484]  [A loss: 0.765153, acc: 0.308594]\n",
      "280: [D loss: 0.700836, acc: 0.513672]  [A loss: 1.245998, acc: 0.000000]\n",
      "281: [D loss: 0.657106, acc: 0.654297]  [A loss: 0.716175, acc: 0.449219]\n",
      "282: [D loss: 0.713126, acc: 0.503906]  [A loss: 1.301921, acc: 0.000000]\n",
      "283: [D loss: 0.674100, acc: 0.582031]  [A loss: 0.621950, acc: 0.753906]\n",
      "284: [D loss: 0.727909, acc: 0.505859]  [A loss: 1.206703, acc: 0.000000]\n",
      "285: [D loss: 0.662701, acc: 0.611328]  [A loss: 0.664487, acc: 0.597656]\n",
      "286: [D loss: 0.719671, acc: 0.500000]  [A loss: 1.154115, acc: 0.003906]\n",
      "287: [D loss: 0.656882, acc: 0.625000]  [A loss: 0.698416, acc: 0.472656]\n",
      "288: [D loss: 0.709972, acc: 0.513672]  [A loss: 1.139522, acc: 0.000000]\n",
      "289: [D loss: 0.661034, acc: 0.625000]  [A loss: 0.719253, acc: 0.398438]\n",
      "290: [D loss: 0.696272, acc: 0.500000]  [A loss: 1.148858, acc: 0.007812]\n",
      "291: [D loss: 0.660591, acc: 0.628906]  [A loss: 0.741653, acc: 0.343750]\n",
      "292: [D loss: 0.705294, acc: 0.503906]  [A loss: 1.108572, acc: 0.000000]\n",
      "293: [D loss: 0.663381, acc: 0.625000]  [A loss: 0.736843, acc: 0.363281]\n",
      "294: [D loss: 0.708394, acc: 0.519531]  [A loss: 1.151858, acc: 0.000000]\n",
      "295: [D loss: 0.669484, acc: 0.589844]  [A loss: 0.725785, acc: 0.421875]\n",
      "296: [D loss: 0.714248, acc: 0.501953]  [A loss: 1.169175, acc: 0.000000]\n",
      "297: [D loss: 0.657494, acc: 0.613281]  [A loss: 0.691362, acc: 0.550781]\n",
      "298: [D loss: 0.711877, acc: 0.496094]  [A loss: 1.115913, acc: 0.000000]\n",
      "299: [D loss: 0.674961, acc: 0.623047]  [A loss: 0.730098, acc: 0.355469]\n",
      "300: [D loss: 0.711097, acc: 0.517578]  [A loss: 1.107624, acc: 0.007812]\n",
      "301: [D loss: 0.668426, acc: 0.613281]  [A loss: 0.694627, acc: 0.539062]\n",
      "302: [D loss: 0.703358, acc: 0.509766]  [A loss: 1.123623, acc: 0.000000]\n",
      "303: [D loss: 0.666242, acc: 0.603516]  [A loss: 0.706491, acc: 0.453125]\n",
      "304: [D loss: 0.724267, acc: 0.511719]  [A loss: 1.134613, acc: 0.000000]\n",
      "305: [D loss: 0.661192, acc: 0.621094]  [A loss: 0.704327, acc: 0.476562]\n",
      "306: [D loss: 0.698319, acc: 0.511719]  [A loss: 1.105443, acc: 0.000000]\n",
      "307: [D loss: 0.665227, acc: 0.634766]  [A loss: 0.705716, acc: 0.492188]\n",
      "308: [D loss: 0.704155, acc: 0.507812]  [A loss: 1.108680, acc: 0.003906]\n",
      "309: [D loss: 0.676649, acc: 0.568359]  [A loss: 0.743564, acc: 0.289062]\n",
      "310: [D loss: 0.693462, acc: 0.511719]  [A loss: 1.074697, acc: 0.000000]\n",
      "311: [D loss: 0.671808, acc: 0.628906]  [A loss: 0.734854, acc: 0.398438]\n",
      "312: [D loss: 0.694083, acc: 0.515625]  [A loss: 1.048325, acc: 0.003906]\n",
      "313: [D loss: 0.662978, acc: 0.626953]  [A loss: 0.735851, acc: 0.335938]\n",
      "314: [D loss: 0.694860, acc: 0.517578]  [A loss: 1.050668, acc: 0.011719]\n",
      "315: [D loss: 0.653799, acc: 0.648438]  [A loss: 0.708166, acc: 0.480469]\n",
      "316: [D loss: 0.709078, acc: 0.513672]  [A loss: 1.179767, acc: 0.000000]\n",
      "317: [D loss: 0.662959, acc: 0.587891]  [A loss: 0.627054, acc: 0.734375]\n",
      "318: [D loss: 0.730680, acc: 0.501953]  [A loss: 1.095012, acc: 0.000000]\n",
      "319: [D loss: 0.663133, acc: 0.628906]  [A loss: 0.687741, acc: 0.539062]\n",
      "320: [D loss: 0.709674, acc: 0.496094]  [A loss: 1.020419, acc: 0.007812]\n",
      "321: [D loss: 0.672954, acc: 0.605469]  [A loss: 0.741296, acc: 0.343750]\n",
      "322: [D loss: 0.696522, acc: 0.515625]  [A loss: 1.031341, acc: 0.011719]\n",
      "323: [D loss: 0.669805, acc: 0.599609]  [A loss: 0.731751, acc: 0.382812]\n",
      "324: [D loss: 0.692222, acc: 0.519531]  [A loss: 1.031109, acc: 0.015625]\n",
      "325: [D loss: 0.663757, acc: 0.623047]  [A loss: 0.746980, acc: 0.355469]\n",
      "326: [D loss: 0.694287, acc: 0.513672]  [A loss: 1.095115, acc: 0.003906]\n",
      "327: [D loss: 0.660852, acc: 0.638672]  [A loss: 0.711161, acc: 0.476562]\n",
      "328: [D loss: 0.711430, acc: 0.507812]  [A loss: 1.137865, acc: 0.003906]\n",
      "329: [D loss: 0.670325, acc: 0.615234]  [A loss: 0.679766, acc: 0.570312]\n",
      "330: [D loss: 0.698951, acc: 0.505859]  [A loss: 1.062568, acc: 0.015625]\n",
      "331: [D loss: 0.680323, acc: 0.574219]  [A loss: 0.748031, acc: 0.343750]\n",
      "332: [D loss: 0.696977, acc: 0.517578]  [A loss: 1.087995, acc: 0.000000]\n",
      "333: [D loss: 0.663544, acc: 0.626953]  [A loss: 0.686418, acc: 0.535156]\n",
      "334: [D loss: 0.705103, acc: 0.505859]  [A loss: 1.088982, acc: 0.000000]\n",
      "335: [D loss: 0.670296, acc: 0.609375]  [A loss: 0.678057, acc: 0.574219]\n",
      "336: [D loss: 0.705796, acc: 0.503906]  [A loss: 1.056093, acc: 0.000000]\n",
      "337: [D loss: 0.663684, acc: 0.634766]  [A loss: 0.706825, acc: 0.484375]\n",
      "338: [D loss: 0.698363, acc: 0.511719]  [A loss: 1.013226, acc: 0.007812]\n",
      "339: [D loss: 0.676867, acc: 0.562500]  [A loss: 0.735974, acc: 0.355469]\n",
      "340: [D loss: 0.686936, acc: 0.523438]  [A loss: 0.971657, acc: 0.007812]\n",
      "341: [D loss: 0.664621, acc: 0.642578]  [A loss: 0.753868, acc: 0.308594]\n",
      "342: [D loss: 0.684767, acc: 0.537109]  [A loss: 0.994753, acc: 0.003906]\n",
      "343: [D loss: 0.663718, acc: 0.623047]  [A loss: 0.756884, acc: 0.335938]\n",
      "344: [D loss: 0.700083, acc: 0.507812]  [A loss: 1.046122, acc: 0.007812]\n",
      "345: [D loss: 0.673635, acc: 0.599609]  [A loss: 0.721249, acc: 0.414062]\n",
      "346: [D loss: 0.694948, acc: 0.533203]  [A loss: 1.061968, acc: 0.000000]\n",
      "347: [D loss: 0.669352, acc: 0.597656]  [A loss: 0.687133, acc: 0.562500]\n",
      "348: [D loss: 0.713096, acc: 0.503906]  [A loss: 1.082092, acc: 0.000000]\n",
      "349: [D loss: 0.672495, acc: 0.570312]  [A loss: 0.681570, acc: 0.554688]\n",
      "350: [D loss: 0.696613, acc: 0.521484]  [A loss: 1.016651, acc: 0.003906]\n",
      "351: [D loss: 0.672266, acc: 0.607422]  [A loss: 0.735896, acc: 0.410156]\n",
      "352: [D loss: 0.697939, acc: 0.507812]  [A loss: 0.981646, acc: 0.011719]\n",
      "353: [D loss: 0.664922, acc: 0.634766]  [A loss: 0.751949, acc: 0.308594]\n",
      "354: [D loss: 0.691716, acc: 0.529297]  [A loss: 1.013650, acc: 0.003906]\n",
      "355: [D loss: 0.676550, acc: 0.585938]  [A loss: 0.728063, acc: 0.421875]\n",
      "356: [D loss: 0.685581, acc: 0.525391]  [A loss: 1.053243, acc: 0.015625]\n",
      "357: [D loss: 0.671529, acc: 0.564453]  [A loss: 0.672489, acc: 0.574219]\n",
      "358: [D loss: 0.721636, acc: 0.501953]  [A loss: 1.060717, acc: 0.003906]\n",
      "359: [D loss: 0.668114, acc: 0.601562]  [A loss: 0.718345, acc: 0.449219]\n",
      "360: [D loss: 0.688677, acc: 0.533203]  [A loss: 1.031211, acc: 0.000000]\n",
      "361: [D loss: 0.665866, acc: 0.617188]  [A loss: 0.716961, acc: 0.437500]\n",
      "362: [D loss: 0.698970, acc: 0.505859]  [A loss: 1.000733, acc: 0.003906]\n",
      "363: [D loss: 0.671330, acc: 0.593750]  [A loss: 0.749434, acc: 0.328125]\n",
      "364: [D loss: 0.688933, acc: 0.525391]  [A loss: 0.967744, acc: 0.011719]\n",
      "365: [D loss: 0.663427, acc: 0.617188]  [A loss: 0.756398, acc: 0.316406]\n",
      "366: [D loss: 0.683328, acc: 0.525391]  [A loss: 1.011089, acc: 0.007812]\n",
      "367: [D loss: 0.659467, acc: 0.617188]  [A loss: 0.747355, acc: 0.382812]\n",
      "368: [D loss: 0.687132, acc: 0.539062]  [A loss: 1.063621, acc: 0.007812]\n",
      "369: [D loss: 0.662098, acc: 0.638672]  [A loss: 0.673749, acc: 0.593750]\n",
      "370: [D loss: 0.700053, acc: 0.509766]  [A loss: 1.125248, acc: 0.000000]\n",
      "371: [D loss: 0.671302, acc: 0.583984]  [A loss: 0.688078, acc: 0.531250]\n",
      "372: [D loss: 0.696151, acc: 0.509766]  [A loss: 1.065152, acc: 0.000000]\n",
      "373: [D loss: 0.663792, acc: 0.626953]  [A loss: 0.701849, acc: 0.500000]\n",
      "374: [D loss: 0.703321, acc: 0.531250]  [A loss: 0.982432, acc: 0.019531]\n",
      "375: [D loss: 0.674392, acc: 0.589844]  [A loss: 0.758927, acc: 0.281250]\n",
      "376: [D loss: 0.689707, acc: 0.505859]  [A loss: 1.005769, acc: 0.015625]\n",
      "377: [D loss: 0.670285, acc: 0.591797]  [A loss: 0.767523, acc: 0.277344]\n",
      "378: [D loss: 0.683187, acc: 0.537109]  [A loss: 0.950183, acc: 0.019531]\n",
      "379: [D loss: 0.660598, acc: 0.640625]  [A loss: 0.765328, acc: 0.324219]\n",
      "380: [D loss: 0.679521, acc: 0.544922]  [A loss: 0.995405, acc: 0.039062]\n",
      "381: [D loss: 0.669956, acc: 0.603516]  [A loss: 0.788744, acc: 0.230469]\n",
      "382: [D loss: 0.684553, acc: 0.529297]  [A loss: 1.041174, acc: 0.015625]\n",
      "383: [D loss: 0.653082, acc: 0.662109]  [A loss: 0.742019, acc: 0.406250]\n",
      "384: [D loss: 0.703879, acc: 0.507812]  [A loss: 1.118204, acc: 0.003906]\n",
      "385: [D loss: 0.664693, acc: 0.603516]  [A loss: 0.684616, acc: 0.546875]\n",
      "386: [D loss: 0.687927, acc: 0.537109]  [A loss: 1.091246, acc: 0.003906]\n",
      "387: [D loss: 0.680864, acc: 0.560547]  [A loss: 0.689888, acc: 0.511719]\n",
      "388: [D loss: 0.706349, acc: 0.513672]  [A loss: 1.078361, acc: 0.007812]\n",
      "389: [D loss: 0.675515, acc: 0.605469]  [A loss: 0.723000, acc: 0.441406]\n",
      "390: [D loss: 0.701426, acc: 0.511719]  [A loss: 1.013226, acc: 0.003906]\n",
      "391: [D loss: 0.661573, acc: 0.625000]  [A loss: 0.758741, acc: 0.328125]\n",
      "392: [D loss: 0.701937, acc: 0.492188]  [A loss: 1.029723, acc: 0.015625]\n",
      "393: [D loss: 0.672327, acc: 0.558594]  [A loss: 0.723599, acc: 0.445312]\n",
      "394: [D loss: 0.708977, acc: 0.519531]  [A loss: 0.999514, acc: 0.039062]\n",
      "395: [D loss: 0.676439, acc: 0.572266]  [A loss: 0.731943, acc: 0.382812]\n",
      "396: [D loss: 0.686015, acc: 0.529297]  [A loss: 0.898476, acc: 0.070312]\n",
      "397: [D loss: 0.673820, acc: 0.593750]  [A loss: 0.801830, acc: 0.218750]\n",
      "398: [D loss: 0.679943, acc: 0.556641]  [A loss: 0.966840, acc: 0.039062]\n",
      "399: [D loss: 0.676369, acc: 0.564453]  [A loss: 0.793437, acc: 0.257812]\n",
      "400: [D loss: 0.690290, acc: 0.529297]  [A loss: 0.959204, acc: 0.042969]\n",
      "401: [D loss: 0.676812, acc: 0.566406]  [A loss: 0.794592, acc: 0.253906]\n",
      "402: [D loss: 0.686469, acc: 0.541016]  [A loss: 0.931847, acc: 0.039062]\n",
      "403: [D loss: 0.674364, acc: 0.593750]  [A loss: 0.902662, acc: 0.062500]\n",
      "404: [D loss: 0.671703, acc: 0.572266]  [A loss: 0.946711, acc: 0.031250]\n",
      "405: [D loss: 0.674735, acc: 0.562500]  [A loss: 0.812337, acc: 0.191406]\n",
      "406: [D loss: 0.681802, acc: 0.544922]  [A loss: 1.010968, acc: 0.011719]\n",
      "407: [D loss: 0.672870, acc: 0.572266]  [A loss: 0.720631, acc: 0.453125]\n",
      "408: [D loss: 0.719566, acc: 0.519531]  [A loss: 1.228429, acc: 0.000000]\n",
      "409: [D loss: 0.690287, acc: 0.535156]  [A loss: 0.572737, acc: 0.855469]\n",
      "410: [D loss: 0.746403, acc: 0.500000]  [A loss: 1.117193, acc: 0.000000]\n",
      "411: [D loss: 0.672933, acc: 0.566406]  [A loss: 0.674410, acc: 0.605469]\n",
      "412: [D loss: 0.712384, acc: 0.521484]  [A loss: 0.957482, acc: 0.027344]\n",
      "413: [D loss: 0.668993, acc: 0.628906]  [A loss: 0.755972, acc: 0.324219]\n",
      "414: [D loss: 0.672634, acc: 0.580078]  [A loss: 0.852829, acc: 0.121094]\n",
      "415: [D loss: 0.671060, acc: 0.587891]  [A loss: 0.806168, acc: 0.222656]\n",
      "416: [D loss: 0.686813, acc: 0.556641]  [A loss: 0.883727, acc: 0.093750]\n",
      "417: [D loss: 0.673782, acc: 0.599609]  [A loss: 0.823903, acc: 0.160156]\n",
      "418: [D loss: 0.670504, acc: 0.578125]  [A loss: 0.883408, acc: 0.070312]\n",
      "419: [D loss: 0.670989, acc: 0.599609]  [A loss: 0.854726, acc: 0.148438]\n",
      "420: [D loss: 0.675200, acc: 0.583984]  [A loss: 0.879000, acc: 0.113281]\n",
      "421: [D loss: 0.678129, acc: 0.554688]  [A loss: 0.913533, acc: 0.054688]\n",
      "422: [D loss: 0.665670, acc: 0.611328]  [A loss: 0.794582, acc: 0.261719]\n",
      "423: [D loss: 0.697362, acc: 0.533203]  [A loss: 1.072080, acc: 0.003906]\n",
      "424: [D loss: 0.666647, acc: 0.580078]  [A loss: 0.749938, acc: 0.355469]\n",
      "425: [D loss: 0.706883, acc: 0.496094]  [A loss: 1.176013, acc: 0.000000]\n",
      "426: [D loss: 0.670474, acc: 0.583984]  [A loss: 0.646728, acc: 0.667969]\n",
      "427: [D loss: 0.723717, acc: 0.500000]  [A loss: 1.082636, acc: 0.003906]\n",
      "428: [D loss: 0.679762, acc: 0.580078]  [A loss: 0.728776, acc: 0.402344]\n",
      "429: [D loss: 0.692784, acc: 0.511719]  [A loss: 0.951264, acc: 0.035156]\n",
      "430: [D loss: 0.673601, acc: 0.599609]  [A loss: 0.765633, acc: 0.324219]\n",
      "431: [D loss: 0.697545, acc: 0.501953]  [A loss: 0.956824, acc: 0.035156]\n",
      "432: [D loss: 0.668178, acc: 0.583984]  [A loss: 0.831823, acc: 0.136719]\n",
      "433: [D loss: 0.676958, acc: 0.544922]  [A loss: 0.881200, acc: 0.078125]\n",
      "434: [D loss: 0.666606, acc: 0.589844]  [A loss: 0.867439, acc: 0.093750]\n",
      "435: [D loss: 0.676090, acc: 0.572266]  [A loss: 0.925845, acc: 0.058594]\n",
      "436: [D loss: 0.665010, acc: 0.640625]  [A loss: 0.804294, acc: 0.253906]\n",
      "437: [D loss: 0.679179, acc: 0.566406]  [A loss: 0.978650, acc: 0.007812]\n",
      "438: [D loss: 0.675828, acc: 0.582031]  [A loss: 0.774316, acc: 0.300781]\n",
      "439: [D loss: 0.695555, acc: 0.529297]  [A loss: 1.087644, acc: 0.000000]\n",
      "440: [D loss: 0.669902, acc: 0.601562]  [A loss: 0.671271, acc: 0.597656]\n",
      "441: [D loss: 0.725441, acc: 0.515625]  [A loss: 1.145938, acc: 0.003906]\n",
      "442: [D loss: 0.680399, acc: 0.533203]  [A loss: 0.679970, acc: 0.562500]\n",
      "443: [D loss: 0.699293, acc: 0.517578]  [A loss: 1.004567, acc: 0.019531]\n",
      "444: [D loss: 0.669702, acc: 0.589844]  [A loss: 0.750457, acc: 0.355469]\n",
      "445: [D loss: 0.697629, acc: 0.523438]  [A loss: 1.024317, acc: 0.023438]\n",
      "446: [D loss: 0.669288, acc: 0.601562]  [A loss: 0.735881, acc: 0.371094]\n",
      "447: [D loss: 0.686916, acc: 0.515625]  [A loss: 1.022473, acc: 0.027344]\n",
      "448: [D loss: 0.675758, acc: 0.605469]  [A loss: 0.723881, acc: 0.437500]\n",
      "449: [D loss: 0.693849, acc: 0.544922]  [A loss: 0.990527, acc: 0.039062]\n",
      "450: [D loss: 0.673660, acc: 0.562500]  [A loss: 0.766119, acc: 0.324219]\n",
      "451: [D loss: 0.693403, acc: 0.515625]  [A loss: 0.978187, acc: 0.035156]\n",
      "452: [D loss: 0.672651, acc: 0.617188]  [A loss: 0.733910, acc: 0.371094]\n",
      "453: [D loss: 0.697465, acc: 0.539062]  [A loss: 1.013747, acc: 0.015625]\n",
      "454: [D loss: 0.671612, acc: 0.593750]  [A loss: 0.758067, acc: 0.328125]\n",
      "455: [D loss: 0.680509, acc: 0.562500]  [A loss: 0.981189, acc: 0.035156]\n",
      "456: [D loss: 0.668623, acc: 0.611328]  [A loss: 0.785852, acc: 0.214844]\n",
      "457: [D loss: 0.678415, acc: 0.576172]  [A loss: 0.956447, acc: 0.035156]\n",
      "458: [D loss: 0.661614, acc: 0.644531]  [A loss: 0.788540, acc: 0.253906]\n",
      "459: [D loss: 0.677673, acc: 0.548828]  [A loss: 1.007821, acc: 0.003906]\n",
      "460: [D loss: 0.679223, acc: 0.580078]  [A loss: 0.798101, acc: 0.238281]\n",
      "461: [D loss: 0.694168, acc: 0.531250]  [A loss: 1.139160, acc: 0.007812]\n",
      "462: [D loss: 0.675857, acc: 0.550781]  [A loss: 0.708040, acc: 0.457031]\n",
      "463: [D loss: 0.696413, acc: 0.546875]  [A loss: 1.013448, acc: 0.019531]\n",
      "464: [D loss: 0.673049, acc: 0.593750]  [A loss: 0.768283, acc: 0.320312]\n",
      "465: [D loss: 0.705802, acc: 0.529297]  [A loss: 1.086513, acc: 0.007812]\n",
      "466: [D loss: 0.670468, acc: 0.591797]  [A loss: 0.684508, acc: 0.574219]\n",
      "467: [D loss: 0.707116, acc: 0.513672]  [A loss: 1.073910, acc: 0.007812]\n",
      "468: [D loss: 0.661403, acc: 0.621094]  [A loss: 0.707639, acc: 0.468750]\n",
      "469: [D loss: 0.687663, acc: 0.533203]  [A loss: 0.955106, acc: 0.023438]\n",
      "470: [D loss: 0.655960, acc: 0.640625]  [A loss: 0.743020, acc: 0.417969]\n",
      "471: [D loss: 0.697758, acc: 0.542969]  [A loss: 1.030281, acc: 0.027344]\n",
      "472: [D loss: 0.656914, acc: 0.630859]  [A loss: 0.739977, acc: 0.406250]\n",
      "473: [D loss: 0.682828, acc: 0.556641]  [A loss: 0.960868, acc: 0.039062]\n",
      "474: [D loss: 0.666090, acc: 0.595703]  [A loss: 0.772303, acc: 0.343750]\n",
      "475: [D loss: 0.700748, acc: 0.533203]  [A loss: 1.021300, acc: 0.027344]\n",
      "476: [D loss: 0.682155, acc: 0.566406]  [A loss: 0.753252, acc: 0.335938]\n",
      "477: [D loss: 0.689272, acc: 0.568359]  [A loss: 0.948599, acc: 0.078125]\n",
      "478: [D loss: 0.662212, acc: 0.597656]  [A loss: 0.779100, acc: 0.328125]\n",
      "479: [D loss: 0.693581, acc: 0.527344]  [A loss: 0.958522, acc: 0.042969]\n",
      "480: [D loss: 0.671887, acc: 0.583984]  [A loss: 0.779304, acc: 0.296875]\n",
      "481: [D loss: 0.678328, acc: 0.542969]  [A loss: 0.944468, acc: 0.058594]\n",
      "482: [D loss: 0.676786, acc: 0.583984]  [A loss: 0.782474, acc: 0.285156]\n",
      "483: [D loss: 0.685436, acc: 0.527344]  [A loss: 0.989342, acc: 0.042969]\n",
      "484: [D loss: 0.665793, acc: 0.587891]  [A loss: 0.808301, acc: 0.222656]\n",
      "485: [D loss: 0.685973, acc: 0.527344]  [A loss: 1.039932, acc: 0.015625]\n",
      "486: [D loss: 0.661934, acc: 0.625000]  [A loss: 0.730309, acc: 0.417969]\n",
      "487: [D loss: 0.710002, acc: 0.519531]  [A loss: 1.114993, acc: 0.019531]\n",
      "488: [D loss: 0.674180, acc: 0.576172]  [A loss: 0.634726, acc: 0.687500]\n",
      "489: [D loss: 0.754133, acc: 0.498047]  [A loss: 1.114414, acc: 0.027344]\n",
      "490: [D loss: 0.682433, acc: 0.560547]  [A loss: 0.736976, acc: 0.367188]\n",
      "491: [D loss: 0.690701, acc: 0.542969]  [A loss: 0.875133, acc: 0.121094]\n",
      "492: [D loss: 0.668023, acc: 0.615234]  [A loss: 0.784329, acc: 0.238281]\n",
      "493: [D loss: 0.685281, acc: 0.550781]  [A loss: 0.899040, acc: 0.085938]\n",
      "494: [D loss: 0.675862, acc: 0.576172]  [A loss: 0.846002, acc: 0.152344]\n",
      "495: [D loss: 0.675198, acc: 0.583984]  [A loss: 0.879468, acc: 0.085938]\n",
      "496: [D loss: 0.663640, acc: 0.595703]  [A loss: 0.860065, acc: 0.125000]\n",
      "497: [D loss: 0.682937, acc: 0.589844]  [A loss: 0.926904, acc: 0.082031]\n",
      "498: [D loss: 0.666790, acc: 0.603516]  [A loss: 0.793931, acc: 0.285156]\n",
      "499: [D loss: 0.679862, acc: 0.548828]  [A loss: 0.992957, acc: 0.042969]\n",
      "500: [D loss: 0.669390, acc: 0.558594]  [A loss: 0.707385, acc: 0.464844]\n",
      "501: [D loss: 0.696314, acc: 0.517578]  [A loss: 1.071805, acc: 0.011719]\n",
      "502: [D loss: 0.678253, acc: 0.562500]  [A loss: 0.750149, acc: 0.332031]\n",
      "503: [D loss: 0.696949, acc: 0.523438]  [A loss: 1.055635, acc: 0.015625]\n",
      "504: [D loss: 0.669035, acc: 0.574219]  [A loss: 0.763865, acc: 0.339844]\n",
      "505: [D loss: 0.685910, acc: 0.542969]  [A loss: 0.963007, acc: 0.042969]\n",
      "506: [D loss: 0.669162, acc: 0.574219]  [A loss: 0.769153, acc: 0.312500]\n",
      "507: [D loss: 0.676254, acc: 0.544922]  [A loss: 1.007222, acc: 0.039062]\n",
      "508: [D loss: 0.674401, acc: 0.583984]  [A loss: 0.779727, acc: 0.261719]\n",
      "509: [D loss: 0.679715, acc: 0.552734]  [A loss: 0.987629, acc: 0.046875]\n",
      "510: [D loss: 0.670537, acc: 0.601562]  [A loss: 0.790208, acc: 0.273438]\n",
      "511: [D loss: 0.672314, acc: 0.572266]  [A loss: 0.986197, acc: 0.042969]\n",
      "512: [D loss: 0.671874, acc: 0.582031]  [A loss: 0.753231, acc: 0.390625]\n",
      "513: [D loss: 0.693035, acc: 0.535156]  [A loss: 1.085144, acc: 0.027344]\n",
      "514: [D loss: 0.676597, acc: 0.568359]  [A loss: 0.692852, acc: 0.527344]\n",
      "515: [D loss: 0.697762, acc: 0.500000]  [A loss: 1.010244, acc: 0.015625]\n",
      "516: [D loss: 0.677460, acc: 0.574219]  [A loss: 0.785162, acc: 0.304688]\n",
      "517: [D loss: 0.704730, acc: 0.521484]  [A loss: 1.054975, acc: 0.015625]\n",
      "518: [D loss: 0.660286, acc: 0.625000]  [A loss: 0.723021, acc: 0.414062]\n",
      "519: [D loss: 0.702424, acc: 0.535156]  [A loss: 1.065661, acc: 0.050781]\n",
      "520: [D loss: 0.677594, acc: 0.568359]  [A loss: 0.743573, acc: 0.410156]\n",
      "521: [D loss: 0.696560, acc: 0.539062]  [A loss: 0.963271, acc: 0.046875]\n",
      "522: [D loss: 0.676956, acc: 0.570312]  [A loss: 0.803704, acc: 0.250000]\n",
      "523: [D loss: 0.678336, acc: 0.558594]  [A loss: 0.900781, acc: 0.097656]\n",
      "524: [D loss: 0.665798, acc: 0.611328]  [A loss: 0.781479, acc: 0.335938]\n",
      "525: [D loss: 0.702946, acc: 0.519531]  [A loss: 0.956997, acc: 0.105469]\n",
      "526: [D loss: 0.667996, acc: 0.605469]  [A loss: 0.796050, acc: 0.261719]\n",
      "527: [D loss: 0.697541, acc: 0.548828]  [A loss: 0.978229, acc: 0.039062]\n",
      "528: [D loss: 0.663345, acc: 0.623047]  [A loss: 0.729840, acc: 0.453125]\n",
      "529: [D loss: 0.705745, acc: 0.535156]  [A loss: 1.007439, acc: 0.031250]\n",
      "530: [D loss: 0.677770, acc: 0.564453]  [A loss: 0.722508, acc: 0.433594]\n",
      "531: [D loss: 0.699838, acc: 0.503906]  [A loss: 1.023729, acc: 0.031250]\n",
      "532: [D loss: 0.667291, acc: 0.591797]  [A loss: 0.733006, acc: 0.406250]\n",
      "533: [D loss: 0.688014, acc: 0.539062]  [A loss: 1.007515, acc: 0.035156]\n",
      "534: [D loss: 0.670142, acc: 0.593750]  [A loss: 0.779685, acc: 0.304688]\n",
      "535: [D loss: 0.681572, acc: 0.556641]  [A loss: 1.008554, acc: 0.058594]\n",
      "536: [D loss: 0.669530, acc: 0.593750]  [A loss: 0.760977, acc: 0.351562]\n",
      "537: [D loss: 0.694415, acc: 0.521484]  [A loss: 0.988363, acc: 0.039062]\n",
      "538: [D loss: 0.677011, acc: 0.593750]  [A loss: 0.764032, acc: 0.347656]\n",
      "539: [D loss: 0.690109, acc: 0.527344]  [A loss: 0.982447, acc: 0.039062]\n",
      "540: [D loss: 0.679629, acc: 0.566406]  [A loss: 0.766879, acc: 0.335938]\n",
      "541: [D loss: 0.681190, acc: 0.556641]  [A loss: 0.983190, acc: 0.039062]\n",
      "542: [D loss: 0.660089, acc: 0.628906]  [A loss: 0.776978, acc: 0.308594]\n",
      "543: [D loss: 0.690979, acc: 0.552734]  [A loss: 1.037224, acc: 0.031250]\n",
      "544: [D loss: 0.666820, acc: 0.601562]  [A loss: 0.779249, acc: 0.292969]\n",
      "545: [D loss: 0.682116, acc: 0.546875]  [A loss: 0.959246, acc: 0.058594]\n",
      "546: [D loss: 0.671298, acc: 0.615234]  [A loss: 0.786343, acc: 0.296875]\n",
      "547: [D loss: 0.690040, acc: 0.541016]  [A loss: 0.938133, acc: 0.062500]\n",
      "548: [D loss: 0.671620, acc: 0.593750]  [A loss: 0.832068, acc: 0.234375]\n",
      "549: [D loss: 0.689405, acc: 0.529297]  [A loss: 0.954482, acc: 0.078125]\n",
      "550: [D loss: 0.667112, acc: 0.574219]  [A loss: 0.795006, acc: 0.296875]\n",
      "551: [D loss: 0.706123, acc: 0.515625]  [A loss: 1.153308, acc: 0.023438]\n",
      "552: [D loss: 0.677733, acc: 0.570312]  [A loss: 0.670744, acc: 0.621094]\n",
      "553: [D loss: 0.712939, acc: 0.523438]  [A loss: 1.060188, acc: 0.015625]\n",
      "554: [D loss: 0.683998, acc: 0.539062]  [A loss: 0.719162, acc: 0.457031]\n",
      "555: [D loss: 0.699565, acc: 0.533203]  [A loss: 0.986475, acc: 0.054688]\n",
      "556: [D loss: 0.668266, acc: 0.611328]  [A loss: 0.769653, acc: 0.316406]\n",
      "557: [D loss: 0.699666, acc: 0.537109]  [A loss: 0.946729, acc: 0.050781]\n",
      "558: [D loss: 0.668042, acc: 0.617188]  [A loss: 0.780891, acc: 0.257812]\n",
      "559: [D loss: 0.684360, acc: 0.566406]  [A loss: 0.892737, acc: 0.140625]\n",
      "560: [D loss: 0.671082, acc: 0.582031]  [A loss: 0.794265, acc: 0.277344]\n",
      "561: [D loss: 0.676755, acc: 0.566406]  [A loss: 0.938142, acc: 0.089844]\n",
      "562: [D loss: 0.666572, acc: 0.595703]  [A loss: 0.822680, acc: 0.234375]\n",
      "563: [D loss: 0.666475, acc: 0.595703]  [A loss: 0.926242, acc: 0.101562]\n",
      "564: [D loss: 0.668157, acc: 0.599609]  [A loss: 0.898759, acc: 0.097656]\n",
      "565: [D loss: 0.677834, acc: 0.583984]  [A loss: 0.905124, acc: 0.109375]\n",
      "566: [D loss: 0.679384, acc: 0.535156]  [A loss: 0.910872, acc: 0.109375]\n",
      "567: [D loss: 0.669313, acc: 0.597656]  [A loss: 0.845981, acc: 0.167969]\n",
      "568: [D loss: 0.681222, acc: 0.550781]  [A loss: 0.923422, acc: 0.082031]\n",
      "569: [D loss: 0.670177, acc: 0.585938]  [A loss: 0.855949, acc: 0.214844]\n",
      "570: [D loss: 0.667368, acc: 0.578125]  [A loss: 1.099891, acc: 0.000000]\n",
      "571: [D loss: 0.671753, acc: 0.578125]  [A loss: 0.767549, acc: 0.343750]\n",
      "572: [D loss: 0.695599, acc: 0.541016]  [A loss: 1.120360, acc: 0.031250]\n",
      "573: [D loss: 0.676109, acc: 0.580078]  [A loss: 0.681085, acc: 0.539062]\n",
      "574: [D loss: 0.715428, acc: 0.511719]  [A loss: 1.074083, acc: 0.031250]\n",
      "575: [D loss: 0.673212, acc: 0.607422]  [A loss: 0.697975, acc: 0.519531]\n",
      "576: [D loss: 0.693714, acc: 0.541016]  [A loss: 0.973755, acc: 0.031250]\n",
      "577: [D loss: 0.670325, acc: 0.607422]  [A loss: 0.823631, acc: 0.222656]\n",
      "578: [D loss: 0.691172, acc: 0.542969]  [A loss: 0.947798, acc: 0.105469]\n",
      "579: [D loss: 0.670074, acc: 0.603516]  [A loss: 0.805937, acc: 0.250000]\n",
      "580: [D loss: 0.676372, acc: 0.576172]  [A loss: 0.901643, acc: 0.132812]\n",
      "581: [D loss: 0.667587, acc: 0.591797]  [A loss: 0.859346, acc: 0.152344]\n",
      "582: [D loss: 0.684272, acc: 0.556641]  [A loss: 0.895510, acc: 0.109375]\n",
      "583: [D loss: 0.678672, acc: 0.570312]  [A loss: 0.914977, acc: 0.101562]\n",
      "584: [D loss: 0.673514, acc: 0.599609]  [A loss: 0.938650, acc: 0.074219]\n",
      "585: [D loss: 0.679237, acc: 0.562500]  [A loss: 0.761077, acc: 0.371094]\n",
      "586: [D loss: 0.685333, acc: 0.544922]  [A loss: 0.997865, acc: 0.042969]\n",
      "587: [D loss: 0.661922, acc: 0.626953]  [A loss: 0.807283, acc: 0.238281]\n",
      "588: [D loss: 0.683068, acc: 0.541016]  [A loss: 1.010774, acc: 0.062500]\n",
      "589: [D loss: 0.665308, acc: 0.613281]  [A loss: 0.783899, acc: 0.332031]\n",
      "590: [D loss: 0.671532, acc: 0.582031]  [A loss: 0.982151, acc: 0.046875]\n",
      "591: [D loss: 0.654551, acc: 0.632812]  [A loss: 0.727817, acc: 0.441406]\n",
      "592: [D loss: 0.746728, acc: 0.501953]  [A loss: 1.123181, acc: 0.011719]\n",
      "593: [D loss: 0.680126, acc: 0.572266]  [A loss: 0.695898, acc: 0.531250]\n",
      "594: [D loss: 0.700366, acc: 0.531250]  [A loss: 0.991314, acc: 0.046875]\n",
      "595: [D loss: 0.659807, acc: 0.619141]  [A loss: 0.779471, acc: 0.292969]\n",
      "596: [D loss: 0.685961, acc: 0.527344]  [A loss: 0.964623, acc: 0.066406]\n",
      "597: [D loss: 0.661696, acc: 0.597656]  [A loss: 0.810474, acc: 0.226562]\n",
      "598: [D loss: 0.681095, acc: 0.546875]  [A loss: 0.927785, acc: 0.070312]\n",
      "599: [D loss: 0.675901, acc: 0.585938]  [A loss: 0.863154, acc: 0.171875]\n",
      "600: [D loss: 0.688357, acc: 0.550781]  [A loss: 0.955463, acc: 0.074219]\n",
      "601: [D loss: 0.668387, acc: 0.589844]  [A loss: 0.806786, acc: 0.265625]\n",
      "602: [D loss: 0.670175, acc: 0.576172]  [A loss: 1.024692, acc: 0.058594]\n",
      "603: [D loss: 0.672269, acc: 0.578125]  [A loss: 0.752494, acc: 0.378906]\n",
      "604: [D loss: 0.674805, acc: 0.550781]  [A loss: 0.993050, acc: 0.046875]\n",
      "605: [D loss: 0.660161, acc: 0.621094]  [A loss: 0.780585, acc: 0.320312]\n",
      "606: [D loss: 0.691619, acc: 0.541016]  [A loss: 0.941906, acc: 0.082031]\n",
      "607: [D loss: 0.677145, acc: 0.576172]  [A loss: 0.867881, acc: 0.156250]\n",
      "608: [D loss: 0.670255, acc: 0.599609]  [A loss: 0.920111, acc: 0.105469]\n",
      "609: [D loss: 0.661218, acc: 0.605469]  [A loss: 0.827855, acc: 0.246094]\n",
      "610: [D loss: 0.679529, acc: 0.564453]  [A loss: 0.942705, acc: 0.070312]\n",
      "611: [D loss: 0.653513, acc: 0.638672]  [A loss: 0.932828, acc: 0.132812]\n",
      "612: [D loss: 0.664446, acc: 0.613281]  [A loss: 0.881676, acc: 0.156250]\n",
      "613: [D loss: 0.674300, acc: 0.580078]  [A loss: 0.949077, acc: 0.085938]\n",
      "614: [D loss: 0.671646, acc: 0.595703]  [A loss: 0.895243, acc: 0.152344]\n",
      "615: [D loss: 0.673699, acc: 0.572266]  [A loss: 0.951707, acc: 0.089844]\n",
      "616: [D loss: 0.666895, acc: 0.572266]  [A loss: 0.872646, acc: 0.164062]\n",
      "617: [D loss: 0.695430, acc: 0.539062]  [A loss: 1.082741, acc: 0.019531]\n",
      "618: [D loss: 0.689775, acc: 0.546875]  [A loss: 0.766568, acc: 0.375000]\n",
      "619: [D loss: 0.686428, acc: 0.562500]  [A loss: 1.032887, acc: 0.046875]\n",
      "620: [D loss: 0.667788, acc: 0.587891]  [A loss: 0.760919, acc: 0.351562]\n",
      "621: [D loss: 0.712979, acc: 0.539062]  [A loss: 1.106414, acc: 0.027344]\n",
      "622: [D loss: 0.675092, acc: 0.576172]  [A loss: 0.755671, acc: 0.402344]\n",
      "623: [D loss: 0.689776, acc: 0.546875]  [A loss: 1.109462, acc: 0.035156]\n",
      "624: [D loss: 0.672294, acc: 0.544922]  [A loss: 0.719140, acc: 0.453125]\n",
      "625: [D loss: 0.699432, acc: 0.519531]  [A loss: 0.987090, acc: 0.066406]\n",
      "626: [D loss: 0.677954, acc: 0.578125]  [A loss: 0.860466, acc: 0.148438]\n",
      "627: [D loss: 0.688249, acc: 0.554688]  [A loss: 0.952687, acc: 0.089844]\n",
      "628: [D loss: 0.666792, acc: 0.583984]  [A loss: 0.874893, acc: 0.187500]\n",
      "629: [D loss: 0.671953, acc: 0.585938]  [A loss: 0.933304, acc: 0.093750]\n",
      "630: [D loss: 0.670517, acc: 0.582031]  [A loss: 0.850734, acc: 0.179688]\n",
      "631: [D loss: 0.666583, acc: 0.578125]  [A loss: 0.894396, acc: 0.152344]\n",
      "632: [D loss: 0.665906, acc: 0.580078]  [A loss: 0.843656, acc: 0.191406]\n",
      "633: [D loss: 0.679792, acc: 0.585938]  [A loss: 0.910142, acc: 0.117188]\n",
      "634: [D loss: 0.663601, acc: 0.615234]  [A loss: 0.828470, acc: 0.242188]\n",
      "635: [D loss: 0.687611, acc: 0.580078]  [A loss: 1.088973, acc: 0.019531]\n",
      "636: [D loss: 0.681761, acc: 0.583984]  [A loss: 0.722559, acc: 0.437500]\n",
      "637: [D loss: 0.703032, acc: 0.511719]  [A loss: 1.041280, acc: 0.015625]\n",
      "638: [D loss: 0.675819, acc: 0.589844]  [A loss: 0.738116, acc: 0.429688]\n",
      "639: [D loss: 0.674401, acc: 0.572266]  [A loss: 0.996066, acc: 0.070312]\n",
      "640: [D loss: 0.686074, acc: 0.539062]  [A loss: 0.853719, acc: 0.187500]\n",
      "641: [D loss: 0.686251, acc: 0.556641]  [A loss: 0.893545, acc: 0.175781]\n",
      "642: [D loss: 0.685283, acc: 0.560547]  [A loss: 0.912223, acc: 0.093750]\n",
      "643: [D loss: 0.678090, acc: 0.583984]  [A loss: 0.944203, acc: 0.101562]\n",
      "644: [D loss: 0.686777, acc: 0.535156]  [A loss: 0.887530, acc: 0.136719]\n",
      "645: [D loss: 0.679437, acc: 0.570312]  [A loss: 0.861402, acc: 0.175781]\n",
      "646: [D loss: 0.690832, acc: 0.523438]  [A loss: 1.039980, acc: 0.027344]\n",
      "647: [D loss: 0.661624, acc: 0.593750]  [A loss: 0.780074, acc: 0.339844]\n",
      "648: [D loss: 0.688350, acc: 0.515625]  [A loss: 1.053699, acc: 0.039062]\n",
      "649: [D loss: 0.674293, acc: 0.578125]  [A loss: 0.703394, acc: 0.484375]\n",
      "650: [D loss: 0.702131, acc: 0.525391]  [A loss: 1.089122, acc: 0.046875]\n",
      "651: [D loss: 0.657160, acc: 0.615234]  [A loss: 0.743128, acc: 0.421875]\n",
      "652: [D loss: 0.742679, acc: 0.513672]  [A loss: 1.010744, acc: 0.031250]\n",
      "653: [D loss: 0.681209, acc: 0.554688]  [A loss: 0.771850, acc: 0.332031]\n",
      "654: [D loss: 0.702209, acc: 0.519531]  [A loss: 0.930868, acc: 0.070312]\n",
      "655: [D loss: 0.666220, acc: 0.587891]  [A loss: 0.803946, acc: 0.257812]\n",
      "656: [D loss: 0.695327, acc: 0.535156]  [A loss: 1.030322, acc: 0.062500]\n",
      "657: [D loss: 0.665318, acc: 0.601562]  [A loss: 0.847225, acc: 0.191406]\n",
      "658: [D loss: 0.689650, acc: 0.564453]  [A loss: 0.905669, acc: 0.148438]\n",
      "659: [D loss: 0.675507, acc: 0.568359]  [A loss: 0.812471, acc: 0.230469]\n",
      "660: [D loss: 0.671144, acc: 0.558594]  [A loss: 0.873392, acc: 0.148438]\n",
      "661: [D loss: 0.681005, acc: 0.550781]  [A loss: 0.862004, acc: 0.164062]\n",
      "662: [D loss: 0.682202, acc: 0.568359]  [A loss: 0.904834, acc: 0.113281]\n",
      "663: [D loss: 0.682345, acc: 0.554688]  [A loss: 0.894225, acc: 0.148438]\n",
      "664: [D loss: 0.683833, acc: 0.546875]  [A loss: 0.880572, acc: 0.164062]\n",
      "665: [D loss: 0.687107, acc: 0.552734]  [A loss: 0.969294, acc: 0.058594]\n",
      "666: [D loss: 0.668614, acc: 0.589844]  [A loss: 0.837752, acc: 0.210938]\n",
      "667: [D loss: 0.689205, acc: 0.572266]  [A loss: 1.045507, acc: 0.054688]\n",
      "668: [D loss: 0.664218, acc: 0.603516]  [A loss: 0.780152, acc: 0.332031]\n",
      "669: [D loss: 0.712025, acc: 0.527344]  [A loss: 1.147748, acc: 0.023438]\n",
      "670: [D loss: 0.680436, acc: 0.574219]  [A loss: 0.680188, acc: 0.535156]\n",
      "671: [D loss: 0.689512, acc: 0.523438]  [A loss: 0.980162, acc: 0.066406]\n",
      "672: [D loss: 0.658080, acc: 0.632812]  [A loss: 0.819847, acc: 0.218750]\n",
      "673: [D loss: 0.683099, acc: 0.541016]  [A loss: 0.912847, acc: 0.109375]\n",
      "674: [D loss: 0.682328, acc: 0.550781]  [A loss: 0.856480, acc: 0.183594]\n",
      "675: [D loss: 0.673245, acc: 0.583984]  [A loss: 0.906874, acc: 0.136719]\n",
      "676: [D loss: 0.673171, acc: 0.578125]  [A loss: 0.884557, acc: 0.097656]\n",
      "677: [D loss: 0.669537, acc: 0.583984]  [A loss: 0.883933, acc: 0.121094]\n",
      "678: [D loss: 0.670872, acc: 0.580078]  [A loss: 0.858893, acc: 0.179688]\n",
      "679: [D loss: 0.679166, acc: 0.564453]  [A loss: 0.934844, acc: 0.082031]\n",
      "680: [D loss: 0.660544, acc: 0.603516]  [A loss: 0.839044, acc: 0.257812]\n",
      "681: [D loss: 0.681193, acc: 0.556641]  [A loss: 0.941129, acc: 0.128906]\n",
      "682: [D loss: 0.663425, acc: 0.611328]  [A loss: 0.843969, acc: 0.230469]\n",
      "683: [D loss: 0.674605, acc: 0.591797]  [A loss: 0.991061, acc: 0.097656]\n",
      "684: [D loss: 0.668815, acc: 0.580078]  [A loss: 0.772257, acc: 0.343750]\n",
      "685: [D loss: 0.701840, acc: 0.537109]  [A loss: 1.009300, acc: 0.089844]\n",
      "686: [D loss: 0.693988, acc: 0.554688]  [A loss: 0.946323, acc: 0.089844]\n",
      "687: [D loss: 0.675179, acc: 0.597656]  [A loss: 0.883339, acc: 0.187500]\n",
      "688: [D loss: 0.679417, acc: 0.552734]  [A loss: 0.949360, acc: 0.074219]\n",
      "689: [D loss: 0.656041, acc: 0.621094]  [A loss: 0.819327, acc: 0.269531]\n",
      "690: [D loss: 0.684432, acc: 0.544922]  [A loss: 1.064127, acc: 0.039062]\n",
      "691: [D loss: 0.670241, acc: 0.578125]  [A loss: 0.744209, acc: 0.402344]\n",
      "692: [D loss: 0.688387, acc: 0.539062]  [A loss: 1.041843, acc: 0.058594]\n",
      "693: [D loss: 0.665444, acc: 0.595703]  [A loss: 0.762945, acc: 0.386719]\n",
      "694: [D loss: 0.688049, acc: 0.546875]  [A loss: 0.967545, acc: 0.074219]\n",
      "695: [D loss: 0.676352, acc: 0.611328]  [A loss: 0.884618, acc: 0.156250]\n",
      "696: [D loss: 0.674659, acc: 0.582031]  [A loss: 0.898065, acc: 0.160156]\n",
      "697: [D loss: 0.672123, acc: 0.578125]  [A loss: 0.922235, acc: 0.128906]\n",
      "698: [D loss: 0.688230, acc: 0.564453]  [A loss: 0.876770, acc: 0.140625]\n",
      "699: [D loss: 0.684253, acc: 0.542969]  [A loss: 1.009203, acc: 0.070312]\n",
      "700: [D loss: 0.659029, acc: 0.617188]  [A loss: 0.752533, acc: 0.429688]\n",
      "701: [D loss: 0.712430, acc: 0.531250]  [A loss: 1.104651, acc: 0.031250]\n",
      "702: [D loss: 0.674459, acc: 0.587891]  [A loss: 0.730491, acc: 0.464844]\n",
      "703: [D loss: 0.715637, acc: 0.539062]  [A loss: 1.152842, acc: 0.031250]\n",
      "704: [D loss: 0.686729, acc: 0.539062]  [A loss: 0.679580, acc: 0.539062]\n",
      "705: [D loss: 0.726803, acc: 0.519531]  [A loss: 1.054673, acc: 0.039062]\n",
      "706: [D loss: 0.672292, acc: 0.576172]  [A loss: 0.751155, acc: 0.390625]\n",
      "707: [D loss: 0.710169, acc: 0.519531]  [A loss: 0.962804, acc: 0.105469]\n",
      "708: [D loss: 0.674021, acc: 0.570312]  [A loss: 0.800166, acc: 0.261719]\n",
      "709: [D loss: 0.686156, acc: 0.544922]  [A loss: 0.913444, acc: 0.125000]\n",
      "710: [D loss: 0.678814, acc: 0.558594]  [A loss: 0.820590, acc: 0.226562]\n",
      "711: [D loss: 0.668433, acc: 0.605469]  [A loss: 0.862234, acc: 0.175781]\n",
      "712: [D loss: 0.690662, acc: 0.541016]  [A loss: 0.854302, acc: 0.195312]\n",
      "713: [D loss: 0.692086, acc: 0.533203]  [A loss: 0.880396, acc: 0.136719]\n",
      "714: [D loss: 0.676272, acc: 0.556641]  [A loss: 0.876003, acc: 0.156250]\n",
      "715: [D loss: 0.670062, acc: 0.578125]  [A loss: 0.896036, acc: 0.105469]\n",
      "716: [D loss: 0.676882, acc: 0.572266]  [A loss: 0.874362, acc: 0.152344]\n",
      "717: [D loss: 0.664699, acc: 0.605469]  [A loss: 0.860888, acc: 0.199219]\n",
      "718: [D loss: 0.676679, acc: 0.548828]  [A loss: 0.929677, acc: 0.105469]\n",
      "719: [D loss: 0.679692, acc: 0.558594]  [A loss: 0.884255, acc: 0.152344]\n",
      "720: [D loss: 0.694802, acc: 0.548828]  [A loss: 0.920973, acc: 0.105469]\n",
      "721: [D loss: 0.668053, acc: 0.580078]  [A loss: 0.898928, acc: 0.156250]\n",
      "722: [D loss: 0.677352, acc: 0.564453]  [A loss: 0.961231, acc: 0.078125]\n",
      "723: [D loss: 0.667158, acc: 0.607422]  [A loss: 0.777918, acc: 0.335938]\n",
      "724: [D loss: 0.701463, acc: 0.533203]  [A loss: 1.074814, acc: 0.062500]\n",
      "725: [D loss: 0.664718, acc: 0.593750]  [A loss: 0.740864, acc: 0.394531]\n",
      "726: [D loss: 0.716232, acc: 0.517578]  [A loss: 1.055181, acc: 0.019531]\n",
      "727: [D loss: 0.671829, acc: 0.570312]  [A loss: 0.744106, acc: 0.394531]\n",
      "728: [D loss: 0.689381, acc: 0.542969]  [A loss: 0.957556, acc: 0.066406]\n",
      "729: [D loss: 0.663243, acc: 0.603516]  [A loss: 0.804061, acc: 0.285156]\n",
      "730: [D loss: 0.698467, acc: 0.542969]  [A loss: 1.013690, acc: 0.066406]\n",
      "731: [D loss: 0.683229, acc: 0.560547]  [A loss: 0.862869, acc: 0.175781]\n",
      "732: [D loss: 0.680333, acc: 0.576172]  [A loss: 0.893400, acc: 0.132812]\n",
      "733: [D loss: 0.667146, acc: 0.601562]  [A loss: 0.918469, acc: 0.121094]\n",
      "734: [D loss: 0.684584, acc: 0.550781]  [A loss: 0.920164, acc: 0.144531]\n",
      "735: [D loss: 0.678055, acc: 0.578125]  [A loss: 0.914411, acc: 0.089844]\n",
      "736: [D loss: 0.686756, acc: 0.552734]  [A loss: 0.903364, acc: 0.136719]\n",
      "737: [D loss: 0.680189, acc: 0.568359]  [A loss: 0.846708, acc: 0.179688]\n",
      "738: [D loss: 0.670610, acc: 0.595703]  [A loss: 0.971705, acc: 0.093750]\n",
      "739: [D loss: 0.679312, acc: 0.570312]  [A loss: 0.845300, acc: 0.199219]\n",
      "740: [D loss: 0.681577, acc: 0.548828]  [A loss: 1.028284, acc: 0.046875]\n",
      "741: [D loss: 0.684014, acc: 0.560547]  [A loss: 0.766098, acc: 0.367188]\n",
      "742: [D loss: 0.679537, acc: 0.564453]  [A loss: 0.994460, acc: 0.039062]\n",
      "743: [D loss: 0.684076, acc: 0.562500]  [A loss: 0.757393, acc: 0.375000]\n",
      "744: [D loss: 0.686062, acc: 0.548828]  [A loss: 1.036893, acc: 0.054688]\n",
      "745: [D loss: 0.681174, acc: 0.558594]  [A loss: 0.762498, acc: 0.363281]\n",
      "746: [D loss: 0.691694, acc: 0.529297]  [A loss: 1.056587, acc: 0.054688]\n",
      "747: [D loss: 0.681662, acc: 0.574219]  [A loss: 0.781668, acc: 0.324219]\n",
      "748: [D loss: 0.686414, acc: 0.560547]  [A loss: 0.964459, acc: 0.058594]\n",
      "749: [D loss: 0.674348, acc: 0.583984]  [A loss: 0.778396, acc: 0.320312]\n",
      "750: [D loss: 0.679977, acc: 0.585938]  [A loss: 1.015326, acc: 0.031250]\n",
      "751: [D loss: 0.698882, acc: 0.527344]  [A loss: 0.783773, acc: 0.289062]\n",
      "752: [D loss: 0.702787, acc: 0.537109]  [A loss: 0.991297, acc: 0.042969]\n",
      "753: [D loss: 0.655116, acc: 0.634766]  [A loss: 0.822522, acc: 0.265625]\n",
      "754: [D loss: 0.727637, acc: 0.507812]  [A loss: 1.038601, acc: 0.042969]\n",
      "755: [D loss: 0.680735, acc: 0.554688]  [A loss: 0.796332, acc: 0.281250]\n",
      "756: [D loss: 0.705924, acc: 0.509766]  [A loss: 1.054847, acc: 0.046875]\n",
      "757: [D loss: 0.674003, acc: 0.550781]  [A loss: 0.718671, acc: 0.437500]\n",
      "758: [D loss: 0.698744, acc: 0.544922]  [A loss: 0.967102, acc: 0.046875]\n",
      "759: [D loss: 0.681023, acc: 0.548828]  [A loss: 0.792890, acc: 0.289062]\n",
      "760: [D loss: 0.687180, acc: 0.537109]  [A loss: 0.955517, acc: 0.066406]\n",
      "761: [D loss: 0.674686, acc: 0.574219]  [A loss: 0.796780, acc: 0.316406]\n",
      "762: [D loss: 0.702025, acc: 0.533203]  [A loss: 0.959746, acc: 0.082031]\n",
      "763: [D loss: 0.674934, acc: 0.589844]  [A loss: 0.767682, acc: 0.328125]\n",
      "764: [D loss: 0.688139, acc: 0.552734]  [A loss: 1.009068, acc: 0.050781]\n",
      "765: [D loss: 0.665063, acc: 0.589844]  [A loss: 0.785380, acc: 0.324219]\n",
      "766: [D loss: 0.694966, acc: 0.542969]  [A loss: 0.991886, acc: 0.042969]\n",
      "767: [D loss: 0.681609, acc: 0.562500]  [A loss: 0.771429, acc: 0.343750]\n",
      "768: [D loss: 0.667507, acc: 0.566406]  [A loss: 0.955285, acc: 0.074219]\n",
      "769: [D loss: 0.675328, acc: 0.574219]  [A loss: 0.851553, acc: 0.222656]\n",
      "770: [D loss: 0.674319, acc: 0.558594]  [A loss: 0.958682, acc: 0.082031]\n",
      "771: [D loss: 0.668070, acc: 0.595703]  [A loss: 0.845945, acc: 0.179688]\n",
      "772: [D loss: 0.670347, acc: 0.562500]  [A loss: 0.893979, acc: 0.144531]\n",
      "773: [D loss: 0.674087, acc: 0.583984]  [A loss: 0.864864, acc: 0.160156]\n",
      "774: [D loss: 0.677583, acc: 0.626953]  [A loss: 0.930017, acc: 0.117188]\n",
      "775: [D loss: 0.664463, acc: 0.599609]  [A loss: 0.832389, acc: 0.203125]\n",
      "776: [D loss: 0.684359, acc: 0.539062]  [A loss: 1.089643, acc: 0.054688]\n",
      "777: [D loss: 0.675337, acc: 0.566406]  [A loss: 0.715206, acc: 0.468750]\n",
      "778: [D loss: 0.701643, acc: 0.537109]  [A loss: 1.064027, acc: 0.023438]\n",
      "779: [D loss: 0.668782, acc: 0.593750]  [A loss: 0.768737, acc: 0.359375]\n",
      "780: [D loss: 0.698822, acc: 0.533203]  [A loss: 1.038783, acc: 0.078125]\n",
      "781: [D loss: 0.682158, acc: 0.566406]  [A loss: 0.776193, acc: 0.328125]\n",
      "782: [D loss: 0.695224, acc: 0.542969]  [A loss: 0.891718, acc: 0.140625]\n",
      "783: [D loss: 0.682321, acc: 0.558594]  [A loss: 0.820108, acc: 0.214844]\n",
      "784: [D loss: 0.680281, acc: 0.578125]  [A loss: 0.977057, acc: 0.074219]\n",
      "785: [D loss: 0.670655, acc: 0.583984]  [A loss: 0.776035, acc: 0.367188]\n",
      "786: [D loss: 0.705364, acc: 0.515625]  [A loss: 1.016379, acc: 0.042969]\n",
      "787: [D loss: 0.672322, acc: 0.609375]  [A loss: 0.801319, acc: 0.273438]\n",
      "788: [D loss: 0.685448, acc: 0.554688]  [A loss: 0.989250, acc: 0.082031]\n",
      "789: [D loss: 0.659783, acc: 0.630859]  [A loss: 0.783147, acc: 0.339844]\n",
      "790: [D loss: 0.708088, acc: 0.539062]  [A loss: 1.023205, acc: 0.042969]\n",
      "791: [D loss: 0.675613, acc: 0.576172]  [A loss: 0.778820, acc: 0.339844]\n",
      "792: [D loss: 0.696805, acc: 0.523438]  [A loss: 1.038758, acc: 0.046875]\n",
      "793: [D loss: 0.686033, acc: 0.519531]  [A loss: 0.744378, acc: 0.410156]\n",
      "794: [D loss: 0.709501, acc: 0.517578]  [A loss: 1.017790, acc: 0.046875]\n",
      "795: [D loss: 0.672213, acc: 0.583984]  [A loss: 0.724752, acc: 0.496094]\n",
      "796: [D loss: 0.697600, acc: 0.529297]  [A loss: 1.016219, acc: 0.039062]\n",
      "797: [D loss: 0.665929, acc: 0.587891]  [A loss: 0.776406, acc: 0.328125]\n",
      "798: [D loss: 0.728211, acc: 0.515625]  [A loss: 1.017479, acc: 0.062500]\n",
      "799: [D loss: 0.686422, acc: 0.552734]  [A loss: 0.705219, acc: 0.488281]\n",
      "800: [D loss: 0.696196, acc: 0.525391]  [A loss: 1.009438, acc: 0.023438]\n",
      "801: [D loss: 0.673015, acc: 0.589844]  [A loss: 0.762196, acc: 0.367188]\n",
      "802: [D loss: 0.700106, acc: 0.546875]  [A loss: 0.954029, acc: 0.062500]\n",
      "803: [D loss: 0.675671, acc: 0.589844]  [A loss: 0.780962, acc: 0.292969]\n",
      "804: [D loss: 0.666970, acc: 0.595703]  [A loss: 0.927134, acc: 0.152344]\n",
      "805: [D loss: 0.681525, acc: 0.570312]  [A loss: 0.822480, acc: 0.265625]\n",
      "806: [D loss: 0.693445, acc: 0.548828]  [A loss: 0.960060, acc: 0.070312]\n",
      "807: [D loss: 0.676525, acc: 0.560547]  [A loss: 0.836633, acc: 0.191406]\n",
      "808: [D loss: 0.674443, acc: 0.562500]  [A loss: 0.954154, acc: 0.082031]\n",
      "809: [D loss: 0.687804, acc: 0.578125]  [A loss: 0.766422, acc: 0.390625]\n",
      "810: [D loss: 0.684234, acc: 0.566406]  [A loss: 1.005217, acc: 0.042969]\n",
      "811: [D loss: 0.667604, acc: 0.583984]  [A loss: 0.820440, acc: 0.222656]\n",
      "812: [D loss: 0.686904, acc: 0.541016]  [A loss: 1.004662, acc: 0.046875]\n",
      "813: [D loss: 0.668597, acc: 0.591797]  [A loss: 0.788182, acc: 0.289062]\n",
      "814: [D loss: 0.692494, acc: 0.535156]  [A loss: 0.908645, acc: 0.125000]\n",
      "815: [D loss: 0.671094, acc: 0.570312]  [A loss: 0.879888, acc: 0.136719]\n",
      "816: [D loss: 0.685717, acc: 0.562500]  [A loss: 0.915090, acc: 0.093750]\n",
      "817: [D loss: 0.672331, acc: 0.589844]  [A loss: 0.897013, acc: 0.160156]\n",
      "818: [D loss: 0.682886, acc: 0.576172]  [A loss: 0.897425, acc: 0.136719]\n",
      "819: [D loss: 0.673270, acc: 0.578125]  [A loss: 0.961125, acc: 0.082031]\n",
      "820: [D loss: 0.677251, acc: 0.576172]  [A loss: 0.833028, acc: 0.234375]\n",
      "821: [D loss: 0.699463, acc: 0.509766]  [A loss: 1.033815, acc: 0.046875]\n",
      "822: [D loss: 0.663307, acc: 0.621094]  [A loss: 0.737319, acc: 0.429688]\n",
      "823: [D loss: 0.726371, acc: 0.519531]  [A loss: 1.093010, acc: 0.015625]\n",
      "824: [D loss: 0.671578, acc: 0.587891]  [A loss: 0.783359, acc: 0.320312]\n",
      "825: [D loss: 0.705536, acc: 0.527344]  [A loss: 1.140820, acc: 0.003906]\n",
      "826: [D loss: 0.662523, acc: 0.591797]  [A loss: 0.716037, acc: 0.453125]\n",
      "827: [D loss: 0.701479, acc: 0.513672]  [A loss: 1.039387, acc: 0.070312]\n",
      "828: [D loss: 0.676251, acc: 0.583984]  [A loss: 0.703507, acc: 0.511719]\n",
      "829: [D loss: 0.696666, acc: 0.537109]  [A loss: 1.010636, acc: 0.031250]\n",
      "830: [D loss: 0.687473, acc: 0.525391]  [A loss: 0.817538, acc: 0.234375]\n",
      "831: [D loss: 0.684067, acc: 0.541016]  [A loss: 0.898730, acc: 0.117188]\n",
      "832: [D loss: 0.675084, acc: 0.556641]  [A loss: 0.867090, acc: 0.152344]\n",
      "833: [D loss: 0.664449, acc: 0.621094]  [A loss: 0.853727, acc: 0.207031]\n",
      "834: [D loss: 0.671058, acc: 0.601562]  [A loss: 0.954466, acc: 0.066406]\n",
      "835: [D loss: 0.683083, acc: 0.548828]  [A loss: 0.946532, acc: 0.097656]\n",
      "836: [D loss: 0.677827, acc: 0.552734]  [A loss: 0.823101, acc: 0.253906]\n",
      "837: [D loss: 0.681543, acc: 0.546875]  [A loss: 0.919730, acc: 0.117188]\n",
      "838: [D loss: 0.664820, acc: 0.601562]  [A loss: 0.860223, acc: 0.199219]\n",
      "839: [D loss: 0.705195, acc: 0.525391]  [A loss: 0.959439, acc: 0.074219]\n",
      "840: [D loss: 0.699117, acc: 0.517578]  [A loss: 0.920686, acc: 0.136719]\n",
      "841: [D loss: 0.683445, acc: 0.578125]  [A loss: 0.916032, acc: 0.128906]\n",
      "842: [D loss: 0.683861, acc: 0.554688]  [A loss: 0.870004, acc: 0.148438]\n",
      "843: [D loss: 0.671560, acc: 0.566406]  [A loss: 1.062643, acc: 0.035156]\n",
      "844: [D loss: 0.688418, acc: 0.527344]  [A loss: 0.784048, acc: 0.304688]\n",
      "845: [D loss: 0.695048, acc: 0.554688]  [A loss: 1.084273, acc: 0.027344]\n",
      "846: [D loss: 0.666488, acc: 0.593750]  [A loss: 0.713431, acc: 0.468750]\n",
      "847: [D loss: 0.746410, acc: 0.501953]  [A loss: 1.184521, acc: 0.015625]\n",
      "848: [D loss: 0.682602, acc: 0.554688]  [A loss: 0.667532, acc: 0.613281]\n",
      "849: [D loss: 0.716325, acc: 0.515625]  [A loss: 1.082450, acc: 0.035156]\n",
      "850: [D loss: 0.687339, acc: 0.562500]  [A loss: 0.769161, acc: 0.394531]\n",
      "851: [D loss: 0.705056, acc: 0.529297]  [A loss: 0.928601, acc: 0.101562]\n",
      "852: [D loss: 0.678235, acc: 0.564453]  [A loss: 0.833004, acc: 0.242188]\n",
      "853: [D loss: 0.687408, acc: 0.562500]  [A loss: 0.930324, acc: 0.078125]\n",
      "854: [D loss: 0.674583, acc: 0.583984]  [A loss: 0.844779, acc: 0.175781]\n",
      "855: [D loss: 0.692040, acc: 0.546875]  [A loss: 0.916727, acc: 0.085938]\n",
      "856: [D loss: 0.668110, acc: 0.599609]  [A loss: 0.836074, acc: 0.230469]\n",
      "857: [D loss: 0.680365, acc: 0.572266]  [A loss: 0.897465, acc: 0.121094]\n",
      "858: [D loss: 0.676259, acc: 0.574219]  [A loss: 0.809867, acc: 0.265625]\n",
      "859: [D loss: 0.677158, acc: 0.566406]  [A loss: 0.955456, acc: 0.062500]\n",
      "860: [D loss: 0.668138, acc: 0.587891]  [A loss: 0.796587, acc: 0.292969]\n",
      "861: [D loss: 0.700133, acc: 0.533203]  [A loss: 1.066628, acc: 0.031250]\n",
      "862: [D loss: 0.681125, acc: 0.558594]  [A loss: 0.693585, acc: 0.523438]\n",
      "863: [D loss: 0.709147, acc: 0.511719]  [A loss: 1.057930, acc: 0.035156]\n",
      "864: [D loss: 0.671308, acc: 0.593750]  [A loss: 0.748860, acc: 0.417969]\n",
      "865: [D loss: 0.728762, acc: 0.533203]  [A loss: 1.017040, acc: 0.039062]\n",
      "866: [D loss: 0.671108, acc: 0.574219]  [A loss: 0.825660, acc: 0.226562]\n",
      "867: [D loss: 0.676704, acc: 0.562500]  [A loss: 0.889974, acc: 0.179688]\n",
      "868: [D loss: 0.678367, acc: 0.566406]  [A loss: 0.888207, acc: 0.136719]\n",
      "869: [D loss: 0.687621, acc: 0.541016]  [A loss: 0.934739, acc: 0.117188]\n",
      "870: [D loss: 0.679497, acc: 0.587891]  [A loss: 0.854308, acc: 0.183594]\n",
      "871: [D loss: 0.675688, acc: 0.582031]  [A loss: 0.933763, acc: 0.089844]\n",
      "872: [D loss: 0.670074, acc: 0.576172]  [A loss: 0.817880, acc: 0.281250]\n",
      "873: [D loss: 0.690083, acc: 0.519531]  [A loss: 0.968801, acc: 0.093750]\n",
      "874: [D loss: 0.679320, acc: 0.580078]  [A loss: 0.786443, acc: 0.304688]\n",
      "875: [D loss: 0.680856, acc: 0.568359]  [A loss: 1.026223, acc: 0.054688]\n",
      "876: [D loss: 0.670690, acc: 0.576172]  [A loss: 0.733451, acc: 0.445312]\n",
      "877: [D loss: 0.703835, acc: 0.535156]  [A loss: 1.121139, acc: 0.031250]\n",
      "878: [D loss: 0.674676, acc: 0.583984]  [A loss: 0.671447, acc: 0.609375]\n",
      "879: [D loss: 0.719154, acc: 0.523438]  [A loss: 1.109675, acc: 0.046875]\n",
      "880: [D loss: 0.676257, acc: 0.570312]  [A loss: 0.789396, acc: 0.292969]\n",
      "881: [D loss: 0.686315, acc: 0.541016]  [A loss: 0.910138, acc: 0.140625]\n",
      "882: [D loss: 0.690757, acc: 0.572266]  [A loss: 0.911476, acc: 0.097656]\n",
      "883: [D loss: 0.677299, acc: 0.541016]  [A loss: 0.823803, acc: 0.273438]\n",
      "884: [D loss: 0.680028, acc: 0.550781]  [A loss: 0.918092, acc: 0.121094]\n",
      "885: [D loss: 0.689156, acc: 0.552734]  [A loss: 0.878549, acc: 0.167969]\n",
      "886: [D loss: 0.669872, acc: 0.599609]  [A loss: 0.972243, acc: 0.062500]\n",
      "887: [D loss: 0.673436, acc: 0.587891]  [A loss: 0.841099, acc: 0.191406]\n",
      "888: [D loss: 0.680986, acc: 0.552734]  [A loss: 0.973631, acc: 0.070312]\n",
      "889: [D loss: 0.679279, acc: 0.560547]  [A loss: 0.797075, acc: 0.242188]\n",
      "890: [D loss: 0.684448, acc: 0.564453]  [A loss: 1.009364, acc: 0.062500]\n",
      "891: [D loss: 0.669039, acc: 0.589844]  [A loss: 0.785372, acc: 0.300781]\n",
      "892: [D loss: 0.690512, acc: 0.560547]  [A loss: 1.115580, acc: 0.023438]\n",
      "893: [D loss: 0.677242, acc: 0.603516]  [A loss: 0.654282, acc: 0.562500]\n",
      "894: [D loss: 0.729438, acc: 0.521484]  [A loss: 1.107388, acc: 0.015625]\n",
      "895: [D loss: 0.679886, acc: 0.556641]  [A loss: 0.643376, acc: 0.632812]\n",
      "896: [D loss: 0.725492, acc: 0.509766]  [A loss: 0.999457, acc: 0.066406]\n",
      "897: [D loss: 0.688407, acc: 0.542969]  [A loss: 0.801952, acc: 0.308594]\n",
      "898: [D loss: 0.690405, acc: 0.552734]  [A loss: 0.918418, acc: 0.070312]\n",
      "899: [D loss: 0.674311, acc: 0.587891]  [A loss: 0.814529, acc: 0.292969]\n",
      "900: [D loss: 0.678963, acc: 0.566406]  [A loss: 0.871053, acc: 0.148438]\n",
      "901: [D loss: 0.684395, acc: 0.548828]  [A loss: 0.843165, acc: 0.187500]\n",
      "902: [D loss: 0.676804, acc: 0.580078]  [A loss: 0.886452, acc: 0.117188]\n",
      "903: [D loss: 0.690657, acc: 0.552734]  [A loss: 0.871104, acc: 0.210938]\n",
      "904: [D loss: 0.690203, acc: 0.550781]  [A loss: 0.869226, acc: 0.195312]\n",
      "905: [D loss: 0.672821, acc: 0.591797]  [A loss: 0.894781, acc: 0.136719]\n",
      "906: [D loss: 0.680627, acc: 0.570312]  [A loss: 0.869072, acc: 0.160156]\n",
      "907: [D loss: 0.680094, acc: 0.582031]  [A loss: 0.967314, acc: 0.058594]\n",
      "908: [D loss: 0.686956, acc: 0.576172]  [A loss: 0.840946, acc: 0.214844]\n",
      "909: [D loss: 0.678440, acc: 0.562500]  [A loss: 0.931541, acc: 0.117188]\n",
      "910: [D loss: 0.672875, acc: 0.582031]  [A loss: 0.768566, acc: 0.335938]\n",
      "911: [D loss: 0.697907, acc: 0.535156]  [A loss: 1.118463, acc: 0.027344]\n",
      "912: [D loss: 0.684314, acc: 0.535156]  [A loss: 0.707236, acc: 0.496094]\n",
      "913: [D loss: 0.696009, acc: 0.533203]  [A loss: 1.067239, acc: 0.019531]\n",
      "914: [D loss: 0.666282, acc: 0.615234]  [A loss: 0.708705, acc: 0.503906]\n",
      "915: [D loss: 0.726311, acc: 0.515625]  [A loss: 1.119501, acc: 0.035156]\n",
      "916: [D loss: 0.684757, acc: 0.583984]  [A loss: 0.690350, acc: 0.554688]\n",
      "917: [D loss: 0.708199, acc: 0.523438]  [A loss: 0.952976, acc: 0.085938]\n",
      "918: [D loss: 0.675335, acc: 0.550781]  [A loss: 0.814713, acc: 0.222656]\n",
      "919: [D loss: 0.697675, acc: 0.525391]  [A loss: 0.885441, acc: 0.144531]\n",
      "920: [D loss: 0.688816, acc: 0.542969]  [A loss: 0.850473, acc: 0.175781]\n",
      "921: [D loss: 0.663756, acc: 0.613281]  [A loss: 0.833715, acc: 0.246094]\n",
      "922: [D loss: 0.693827, acc: 0.550781]  [A loss: 0.950692, acc: 0.082031]\n",
      "923: [D loss: 0.679056, acc: 0.544922]  [A loss: 0.750741, acc: 0.371094]\n",
      "924: [D loss: 0.692469, acc: 0.548828]  [A loss: 1.017575, acc: 0.054688]\n",
      "925: [D loss: 0.672583, acc: 0.578125]  [A loss: 0.778957, acc: 0.328125]\n",
      "926: [D loss: 0.729766, acc: 0.507812]  [A loss: 1.062682, acc: 0.023438]\n",
      "927: [D loss: 0.690417, acc: 0.527344]  [A loss: 0.718755, acc: 0.464844]\n",
      "928: [D loss: 0.699904, acc: 0.533203]  [A loss: 0.954375, acc: 0.082031]\n",
      "929: [D loss: 0.672343, acc: 0.574219]  [A loss: 0.822845, acc: 0.257812]\n",
      "930: [D loss: 0.721989, acc: 0.511719]  [A loss: 1.023831, acc: 0.039062]\n",
      "931: [D loss: 0.672931, acc: 0.580078]  [A loss: 0.756801, acc: 0.359375]\n",
      "932: [D loss: 0.681854, acc: 0.564453]  [A loss: 0.961475, acc: 0.113281]\n",
      "933: [D loss: 0.667762, acc: 0.621094]  [A loss: 0.768082, acc: 0.351562]\n",
      "934: [D loss: 0.706705, acc: 0.519531]  [A loss: 0.951046, acc: 0.105469]\n",
      "935: [D loss: 0.675994, acc: 0.578125]  [A loss: 0.755088, acc: 0.398438]\n",
      "936: [D loss: 0.704541, acc: 0.552734]  [A loss: 1.009386, acc: 0.050781]\n",
      "937: [D loss: 0.678630, acc: 0.580078]  [A loss: 0.765830, acc: 0.343750]\n",
      "938: [D loss: 0.702153, acc: 0.521484]  [A loss: 0.993607, acc: 0.066406]\n",
      "939: [D loss: 0.678964, acc: 0.582031]  [A loss: 0.768828, acc: 0.332031]\n",
      "940: [D loss: 0.681116, acc: 0.566406]  [A loss: 0.967238, acc: 0.085938]\n",
      "941: [D loss: 0.680958, acc: 0.566406]  [A loss: 0.761806, acc: 0.347656]\n",
      "942: [D loss: 0.694872, acc: 0.570312]  [A loss: 0.984394, acc: 0.085938]\n",
      "943: [D loss: 0.674275, acc: 0.572266]  [A loss: 0.837703, acc: 0.167969]\n",
      "944: [D loss: 0.694332, acc: 0.574219]  [A loss: 0.866028, acc: 0.152344]\n",
      "945: [D loss: 0.697636, acc: 0.539062]  [A loss: 0.999765, acc: 0.054688]\n",
      "946: [D loss: 0.672085, acc: 0.593750]  [A loss: 0.733637, acc: 0.429688]\n",
      "947: [D loss: 0.706950, acc: 0.517578]  [A loss: 1.023291, acc: 0.031250]\n",
      "948: [D loss: 0.682514, acc: 0.568359]  [A loss: 0.688964, acc: 0.558594]\n",
      "949: [D loss: 0.717915, acc: 0.539062]  [A loss: 1.059762, acc: 0.082031]\n",
      "950: [D loss: 0.684921, acc: 0.544922]  [A loss: 0.752511, acc: 0.378906]\n",
      "951: [D loss: 0.686924, acc: 0.552734]  [A loss: 0.950387, acc: 0.089844]\n",
      "952: [D loss: 0.671946, acc: 0.611328]  [A loss: 0.783390, acc: 0.335938]\n",
      "953: [D loss: 0.681930, acc: 0.537109]  [A loss: 0.902640, acc: 0.128906]\n",
      "954: [D loss: 0.687908, acc: 0.542969]  [A loss: 0.782218, acc: 0.328125]\n",
      "955: [D loss: 0.691418, acc: 0.562500]  [A loss: 0.929304, acc: 0.093750]\n",
      "956: [D loss: 0.692550, acc: 0.517578]  [A loss: 0.838319, acc: 0.218750]\n",
      "957: [D loss: 0.683899, acc: 0.574219]  [A loss: 0.873523, acc: 0.175781]\n",
      "958: [D loss: 0.676626, acc: 0.552734]  [A loss: 0.857295, acc: 0.167969]\n",
      "959: [D loss: 0.676207, acc: 0.564453]  [A loss: 0.868408, acc: 0.136719]\n",
      "960: [D loss: 0.680659, acc: 0.576172]  [A loss: 0.850179, acc: 0.164062]\n",
      "961: [D loss: 0.675451, acc: 0.566406]  [A loss: 0.883212, acc: 0.152344]\n",
      "962: [D loss: 0.691782, acc: 0.548828]  [A loss: 0.919848, acc: 0.109375]\n",
      "963: [D loss: 0.682994, acc: 0.542969]  [A loss: 0.861576, acc: 0.156250]\n",
      "964: [D loss: 0.687848, acc: 0.533203]  [A loss: 0.930478, acc: 0.074219]\n",
      "965: [D loss: 0.672963, acc: 0.574219]  [A loss: 0.837723, acc: 0.222656]\n",
      "966: [D loss: 0.706136, acc: 0.503906]  [A loss: 1.001246, acc: 0.046875]\n",
      "967: [D loss: 0.685567, acc: 0.556641]  [A loss: 0.891440, acc: 0.128906]\n",
      "968: [D loss: 0.692297, acc: 0.546875]  [A loss: 0.950191, acc: 0.093750]\n",
      "969: [D loss: 0.669170, acc: 0.580078]  [A loss: 0.746608, acc: 0.402344]\n",
      "970: [D loss: 0.689250, acc: 0.562500]  [A loss: 1.070448, acc: 0.015625]\n",
      "971: [D loss: 0.673028, acc: 0.595703]  [A loss: 0.781728, acc: 0.300781]\n",
      "972: [D loss: 0.701196, acc: 0.515625]  [A loss: 1.138001, acc: 0.023438]\n",
      "973: [D loss: 0.696298, acc: 0.511719]  [A loss: 0.684153, acc: 0.527344]\n",
      "974: [D loss: 0.721024, acc: 0.513672]  [A loss: 0.974093, acc: 0.046875]\n",
      "975: [D loss: 0.675507, acc: 0.580078]  [A loss: 0.749836, acc: 0.363281]\n",
      "976: [D loss: 0.708690, acc: 0.531250]  [A loss: 0.991319, acc: 0.074219]\n",
      "977: [D loss: 0.668933, acc: 0.609375]  [A loss: 0.699154, acc: 0.562500]\n",
      "978: [D loss: 0.703389, acc: 0.544922]  [A loss: 0.972520, acc: 0.078125]\n",
      "979: [D loss: 0.683710, acc: 0.550781]  [A loss: 0.777597, acc: 0.292969]\n",
      "980: [D loss: 0.702342, acc: 0.519531]  [A loss: 0.988664, acc: 0.046875]\n",
      "981: [D loss: 0.688294, acc: 0.537109]  [A loss: 0.832290, acc: 0.226562]\n",
      "982: [D loss: 0.682045, acc: 0.548828]  [A loss: 0.855586, acc: 0.175781]\n",
      "983: [D loss: 0.668979, acc: 0.597656]  [A loss: 0.912818, acc: 0.132812]\n",
      "984: [D loss: 0.685897, acc: 0.556641]  [A loss: 0.820753, acc: 0.226562]\n",
      "985: [D loss: 0.684584, acc: 0.560547]  [A loss: 0.946914, acc: 0.105469]\n",
      "986: [D loss: 0.677070, acc: 0.556641]  [A loss: 0.855461, acc: 0.179688]\n",
      "987: [D loss: 0.682031, acc: 0.568359]  [A loss: 0.890803, acc: 0.144531]\n",
      "988: [D loss: 0.689905, acc: 0.525391]  [A loss: 0.896369, acc: 0.160156]\n",
      "989: [D loss: 0.682907, acc: 0.548828]  [A loss: 0.878937, acc: 0.160156]\n",
      "990: [D loss: 0.687819, acc: 0.527344]  [A loss: 0.871006, acc: 0.183594]\n",
      "991: [D loss: 0.674472, acc: 0.601562]  [A loss: 0.914720, acc: 0.132812]\n",
      "992: [D loss: 0.681103, acc: 0.558594]  [A loss: 0.890428, acc: 0.160156]\n",
      "993: [D loss: 0.690028, acc: 0.523438]  [A loss: 1.018254, acc: 0.062500]\n",
      "994: [D loss: 0.677306, acc: 0.580078]  [A loss: 0.798106, acc: 0.257812]\n",
      "995: [D loss: 0.683975, acc: 0.566406]  [A loss: 1.097631, acc: 0.042969]\n",
      "996: [D loss: 0.680376, acc: 0.560547]  [A loss: 0.698754, acc: 0.523438]\n",
      "997: [D loss: 0.720995, acc: 0.527344]  [A loss: 1.197828, acc: 0.007812]\n",
      "998: [D loss: 0.678339, acc: 0.550781]  [A loss: 0.611149, acc: 0.722656]\n",
      "999: [D loss: 0.716955, acc: 0.527344]  [A loss: 0.990868, acc: 0.085938]\n",
      "1000: [D loss: 0.666930, acc: 0.593750]  [A loss: 0.770164, acc: 0.355469]\n",
      "1001: [D loss: 0.705080, acc: 0.552734]  [A loss: 0.971860, acc: 0.074219]\n",
      "1002: [D loss: 0.681557, acc: 0.593750]  [A loss: 0.797088, acc: 0.289062]\n",
      "1003: [D loss: 0.691202, acc: 0.541016]  [A loss: 0.973351, acc: 0.046875]\n",
      "1004: [D loss: 0.688218, acc: 0.533203]  [A loss: 0.766371, acc: 0.328125]\n",
      "1005: [D loss: 0.678157, acc: 0.582031]  [A loss: 0.893092, acc: 0.136719]\n",
      "1006: [D loss: 0.683269, acc: 0.550781]  [A loss: 0.872693, acc: 0.199219]\n",
      "1007: [D loss: 0.677997, acc: 0.589844]  [A loss: 0.850781, acc: 0.203125]\n",
      "1008: [D loss: 0.689708, acc: 0.533203]  [A loss: 0.824202, acc: 0.246094]\n",
      "1009: [D loss: 0.680288, acc: 0.580078]  [A loss: 0.873208, acc: 0.175781]\n",
      "1010: [D loss: 0.673648, acc: 0.582031]  [A loss: 0.899901, acc: 0.144531]\n",
      "1011: [D loss: 0.675986, acc: 0.589844]  [A loss: 0.865874, acc: 0.242188]\n",
      "1012: [D loss: 0.667422, acc: 0.607422]  [A loss: 0.908000, acc: 0.136719]\n",
      "1013: [D loss: 0.682506, acc: 0.523438]  [A loss: 0.830909, acc: 0.222656]\n",
      "1014: [D loss: 0.684589, acc: 0.548828]  [A loss: 1.029179, acc: 0.066406]\n",
      "1015: [D loss: 0.677732, acc: 0.570312]  [A loss: 0.753048, acc: 0.417969]\n",
      "1016: [D loss: 0.699545, acc: 0.548828]  [A loss: 1.111272, acc: 0.031250]\n",
      "1017: [D loss: 0.671695, acc: 0.582031]  [A loss: 0.647871, acc: 0.644531]\n",
      "1018: [D loss: 0.725921, acc: 0.519531]  [A loss: 1.053732, acc: 0.035156]\n",
      "1019: [D loss: 0.676006, acc: 0.587891]  [A loss: 0.675609, acc: 0.546875]\n",
      "1020: [D loss: 0.714389, acc: 0.505859]  [A loss: 0.991987, acc: 0.058594]\n",
      "1021: [D loss: 0.689209, acc: 0.527344]  [A loss: 0.730608, acc: 0.414062]\n",
      "1022: [D loss: 0.713020, acc: 0.521484]  [A loss: 0.990698, acc: 0.062500]\n",
      "1023: [D loss: 0.669906, acc: 0.574219]  [A loss: 0.743456, acc: 0.433594]\n",
      "1024: [D loss: 0.705692, acc: 0.544922]  [A loss: 0.989780, acc: 0.031250]\n",
      "1025: [D loss: 0.671378, acc: 0.583984]  [A loss: 0.737394, acc: 0.425781]\n",
      "1026: [D loss: 0.732714, acc: 0.523438]  [A loss: 0.920532, acc: 0.144531]\n",
      "1027: [D loss: 0.700207, acc: 0.529297]  [A loss: 0.948264, acc: 0.082031]\n",
      "1028: [D loss: 0.694936, acc: 0.525391]  [A loss: 0.809227, acc: 0.214844]\n",
      "1029: [D loss: 0.676093, acc: 0.582031]  [A loss: 0.878782, acc: 0.144531]\n",
      "1030: [D loss: 0.689578, acc: 0.562500]  [A loss: 0.878700, acc: 0.125000]\n",
      "1031: [D loss: 0.689613, acc: 0.554688]  [A loss: 0.864527, acc: 0.191406]\n",
      "1032: [D loss: 0.687869, acc: 0.558594]  [A loss: 0.852633, acc: 0.171875]\n",
      "1033: [D loss: 0.681643, acc: 0.558594]  [A loss: 0.808372, acc: 0.261719]\n",
      "1034: [D loss: 0.700558, acc: 0.509766]  [A loss: 0.965665, acc: 0.074219]\n",
      "1035: [D loss: 0.669259, acc: 0.587891]  [A loss: 0.785350, acc: 0.285156]\n",
      "1036: [D loss: 0.676452, acc: 0.585938]  [A loss: 0.930060, acc: 0.097656]\n",
      "1037: [D loss: 0.677082, acc: 0.576172]  [A loss: 0.841389, acc: 0.199219]\n",
      "1038: [D loss: 0.690567, acc: 0.542969]  [A loss: 0.952123, acc: 0.085938]\n",
      "1039: [D loss: 0.683774, acc: 0.576172]  [A loss: 0.845284, acc: 0.207031]\n",
      "1040: [D loss: 0.710234, acc: 0.519531]  [A loss: 0.993711, acc: 0.062500]\n",
      "1041: [D loss: 0.685641, acc: 0.525391]  [A loss: 0.773382, acc: 0.347656]\n",
      "1042: [D loss: 0.689141, acc: 0.560547]  [A loss: 0.948734, acc: 0.097656]\n",
      "1043: [D loss: 0.674897, acc: 0.570312]  [A loss: 0.840425, acc: 0.191406]\n",
      "1044: [D loss: 0.685174, acc: 0.539062]  [A loss: 0.901873, acc: 0.136719]\n",
      "1045: [D loss: 0.680224, acc: 0.576172]  [A loss: 0.884434, acc: 0.167969]\n",
      "1046: [D loss: 0.683625, acc: 0.537109]  [A loss: 0.852025, acc: 0.191406]\n",
      "1047: [D loss: 0.696445, acc: 0.542969]  [A loss: 0.932762, acc: 0.105469]\n",
      "1048: [D loss: 0.684484, acc: 0.582031]  [A loss: 0.848710, acc: 0.210938]\n",
      "1049: [D loss: 0.679189, acc: 0.572266]  [A loss: 0.945829, acc: 0.082031]\n",
      "1050: [D loss: 0.691867, acc: 0.541016]  [A loss: 0.831602, acc: 0.199219]\n",
      "1051: [D loss: 0.685000, acc: 0.591797]  [A loss: 0.989319, acc: 0.046875]\n",
      "1052: [D loss: 0.665879, acc: 0.595703]  [A loss: 0.784990, acc: 0.339844]\n",
      "1053: [D loss: 0.683589, acc: 0.572266]  [A loss: 1.074889, acc: 0.039062]\n",
      "1054: [D loss: 0.679992, acc: 0.560547]  [A loss: 0.705831, acc: 0.519531]\n",
      "1055: [D loss: 0.704399, acc: 0.539062]  [A loss: 1.119738, acc: 0.031250]\n",
      "1056: [D loss: 0.686262, acc: 0.558594]  [A loss: 0.740424, acc: 0.410156]\n",
      "1057: [D loss: 0.710709, acc: 0.519531]  [A loss: 0.986056, acc: 0.074219]\n",
      "1058: [D loss: 0.696329, acc: 0.533203]  [A loss: 0.789110, acc: 0.289062]\n",
      "1059: [D loss: 0.688451, acc: 0.560547]  [A loss: 0.962575, acc: 0.089844]\n",
      "1060: [D loss: 0.671307, acc: 0.589844]  [A loss: 0.806327, acc: 0.253906]\n",
      "1061: [D loss: 0.688694, acc: 0.527344]  [A loss: 1.039888, acc: 0.070312]\n",
      "1062: [D loss: 0.666934, acc: 0.603516]  [A loss: 0.665030, acc: 0.585938]\n",
      "1063: [D loss: 0.706461, acc: 0.531250]  [A loss: 1.071594, acc: 0.042969]\n",
      "1064: [D loss: 0.679030, acc: 0.560547]  [A loss: 0.743581, acc: 0.417969]\n",
      "1065: [D loss: 0.699033, acc: 0.527344]  [A loss: 0.957938, acc: 0.074219]\n",
      "1066: [D loss: 0.681167, acc: 0.583984]  [A loss: 0.734885, acc: 0.429688]\n",
      "1067: [D loss: 0.706967, acc: 0.552734]  [A loss: 0.993548, acc: 0.062500]\n",
      "1068: [D loss: 0.669457, acc: 0.578125]  [A loss: 0.765762, acc: 0.363281]\n",
      "1069: [D loss: 0.711105, acc: 0.539062]  [A loss: 0.925920, acc: 0.097656]\n",
      "1070: [D loss: 0.690423, acc: 0.533203]  [A loss: 0.851385, acc: 0.234375]\n",
      "1071: [D loss: 0.696557, acc: 0.529297]  [A loss: 0.871794, acc: 0.152344]\n",
      "1072: [D loss: 0.683770, acc: 0.529297]  [A loss: 0.822926, acc: 0.257812]\n",
      "1073: [D loss: 0.687000, acc: 0.548828]  [A loss: 0.952742, acc: 0.074219]\n",
      "1074: [D loss: 0.676340, acc: 0.574219]  [A loss: 0.808315, acc: 0.261719]\n",
      "1075: [D loss: 0.702697, acc: 0.533203]  [A loss: 1.011834, acc: 0.042969]\n",
      "1076: [D loss: 0.689358, acc: 0.548828]  [A loss: 0.767207, acc: 0.332031]\n",
      "1077: [D loss: 0.703071, acc: 0.539062]  [A loss: 0.967488, acc: 0.089844]\n",
      "1078: [D loss: 0.700229, acc: 0.533203]  [A loss: 0.871960, acc: 0.183594]\n",
      "1079: [D loss: 0.685168, acc: 0.544922]  [A loss: 0.958117, acc: 0.085938]\n",
      "1080: [D loss: 0.680480, acc: 0.585938]  [A loss: 0.823059, acc: 0.242188]\n",
      "1081: [D loss: 0.684835, acc: 0.560547]  [A loss: 0.928602, acc: 0.097656]\n",
      "1082: [D loss: 0.678096, acc: 0.560547]  [A loss: 0.799670, acc: 0.273438]\n",
      "1083: [D loss: 0.681221, acc: 0.566406]  [A loss: 0.982362, acc: 0.066406]\n",
      "1084: [D loss: 0.668944, acc: 0.591797]  [A loss: 0.828291, acc: 0.214844]\n",
      "1085: [D loss: 0.702991, acc: 0.525391]  [A loss: 0.943559, acc: 0.082031]\n",
      "1086: [D loss: 0.682431, acc: 0.544922]  [A loss: 0.836398, acc: 0.191406]\n",
      "1087: [D loss: 0.687217, acc: 0.558594]  [A loss: 0.998403, acc: 0.050781]\n",
      "1088: [D loss: 0.680265, acc: 0.566406]  [A loss: 0.749287, acc: 0.406250]\n",
      "1089: [D loss: 0.697092, acc: 0.537109]  [A loss: 1.114825, acc: 0.035156]\n",
      "1090: [D loss: 0.685334, acc: 0.550781]  [A loss: 0.677317, acc: 0.558594]\n",
      "1091: [D loss: 0.715444, acc: 0.546875]  [A loss: 1.026450, acc: 0.031250]\n",
      "1092: [D loss: 0.676349, acc: 0.578125]  [A loss: 0.759489, acc: 0.394531]\n",
      "1093: [D loss: 0.703176, acc: 0.542969]  [A loss: 1.041922, acc: 0.039062]\n",
      "1094: [D loss: 0.675404, acc: 0.583984]  [A loss: 0.714726, acc: 0.460938]\n",
      "1095: [D loss: 0.702375, acc: 0.517578]  [A loss: 0.955268, acc: 0.085938]\n",
      "1096: [D loss: 0.682520, acc: 0.578125]  [A loss: 0.792963, acc: 0.320312]\n",
      "1097: [D loss: 0.693776, acc: 0.523438]  [A loss: 0.906256, acc: 0.128906]\n",
      "1098: [D loss: 0.678678, acc: 0.574219]  [A loss: 0.815945, acc: 0.222656]\n",
      "1099: [D loss: 0.691984, acc: 0.544922]  [A loss: 0.966986, acc: 0.089844]\n",
      "1100: [D loss: 0.689011, acc: 0.574219]  [A loss: 0.785508, acc: 0.312500]\n",
      "1101: [D loss: 0.689723, acc: 0.511719]  [A loss: 0.954810, acc: 0.062500]\n",
      "1102: [D loss: 0.682543, acc: 0.589844]  [A loss: 0.774404, acc: 0.316406]\n",
      "1103: [D loss: 0.692366, acc: 0.527344]  [A loss: 0.967314, acc: 0.082031]\n",
      "1104: [D loss: 0.681072, acc: 0.560547]  [A loss: 0.806611, acc: 0.250000]\n",
      "1105: [D loss: 0.696490, acc: 0.548828]  [A loss: 0.986428, acc: 0.042969]\n",
      "1106: [D loss: 0.677299, acc: 0.562500]  [A loss: 0.735518, acc: 0.453125]\n",
      "1107: [D loss: 0.691227, acc: 0.537109]  [A loss: 0.976251, acc: 0.078125]\n",
      "1108: [D loss: 0.684299, acc: 0.535156]  [A loss: 0.753429, acc: 0.398438]\n",
      "1109: [D loss: 0.693662, acc: 0.544922]  [A loss: 0.945722, acc: 0.101562]\n",
      "1110: [D loss: 0.669062, acc: 0.587891]  [A loss: 0.742773, acc: 0.394531]\n",
      "1111: [D loss: 0.690138, acc: 0.546875]  [A loss: 0.976948, acc: 0.093750]\n",
      "1112: [D loss: 0.674675, acc: 0.560547]  [A loss: 0.778384, acc: 0.335938]\n",
      "1113: [D loss: 0.696629, acc: 0.535156]  [A loss: 0.977152, acc: 0.066406]\n",
      "1114: [D loss: 0.671651, acc: 0.578125]  [A loss: 0.747067, acc: 0.386719]\n",
      "1115: [D loss: 0.705079, acc: 0.521484]  [A loss: 0.995170, acc: 0.089844]\n",
      "1116: [D loss: 0.693565, acc: 0.546875]  [A loss: 0.800756, acc: 0.253906]\n",
      "1117: [D loss: 0.695571, acc: 0.527344]  [A loss: 0.957769, acc: 0.082031]\n",
      "1118: [D loss: 0.672740, acc: 0.591797]  [A loss: 0.825882, acc: 0.218750]\n",
      "1119: [D loss: 0.671238, acc: 0.591797]  [A loss: 0.957102, acc: 0.101562]\n",
      "1120: [D loss: 0.671927, acc: 0.591797]  [A loss: 0.826818, acc: 0.242188]\n",
      "1121: [D loss: 0.700142, acc: 0.517578]  [A loss: 0.956637, acc: 0.074219]\n",
      "1122: [D loss: 0.702817, acc: 0.521484]  [A loss: 0.816252, acc: 0.253906]\n",
      "1123: [D loss: 0.702511, acc: 0.529297]  [A loss: 0.927416, acc: 0.109375]\n",
      "1124: [D loss: 0.698986, acc: 0.525391]  [A loss: 0.806014, acc: 0.257812]\n",
      "1125: [D loss: 0.695533, acc: 0.525391]  [A loss: 1.037059, acc: 0.054688]\n",
      "1126: [D loss: 0.680806, acc: 0.539062]  [A loss: 0.665304, acc: 0.601562]\n",
      "1127: [D loss: 0.721808, acc: 0.521484]  [A loss: 1.003395, acc: 0.039062]\n",
      "1128: [D loss: 0.705062, acc: 0.509766]  [A loss: 0.821618, acc: 0.242188]\n",
      "1129: [D loss: 0.683562, acc: 0.578125]  [A loss: 0.928392, acc: 0.085938]\n",
      "1130: [D loss: 0.693241, acc: 0.535156]  [A loss: 0.843874, acc: 0.167969]\n",
      "1131: [D loss: 0.684929, acc: 0.572266]  [A loss: 0.872885, acc: 0.171875]\n",
      "1132: [D loss: 0.683056, acc: 0.574219]  [A loss: 0.832484, acc: 0.203125]\n",
      "1133: [D loss: 0.689996, acc: 0.566406]  [A loss: 0.870669, acc: 0.183594]\n",
      "1134: [D loss: 0.678378, acc: 0.560547]  [A loss: 0.835025, acc: 0.210938]\n",
      "1135: [D loss: 0.684843, acc: 0.546875]  [A loss: 0.850331, acc: 0.187500]\n",
      "1136: [D loss: 0.718363, acc: 0.496094]  [A loss: 0.905189, acc: 0.132812]\n",
      "1137: [D loss: 0.695861, acc: 0.525391]  [A loss: 0.927068, acc: 0.113281]\n",
      "1138: [D loss: 0.679122, acc: 0.568359]  [A loss: 0.827779, acc: 0.234375]\n",
      "1139: [D loss: 0.695238, acc: 0.546875]  [A loss: 0.935451, acc: 0.109375]\n",
      "1140: [D loss: 0.691484, acc: 0.550781]  [A loss: 0.935000, acc: 0.105469]\n",
      "1141: [D loss: 0.688861, acc: 0.525391]  [A loss: 0.901486, acc: 0.148438]\n",
      "1142: [D loss: 0.675526, acc: 0.597656]  [A loss: 0.835212, acc: 0.214844]\n",
      "1143: [D loss: 0.686430, acc: 0.568359]  [A loss: 0.995789, acc: 0.062500]\n",
      "1144: [D loss: 0.697403, acc: 0.529297]  [A loss: 0.779274, acc: 0.335938]\n",
      "1145: [D loss: 0.690239, acc: 0.548828]  [A loss: 0.996796, acc: 0.062500]\n",
      "1146: [D loss: 0.673578, acc: 0.582031]  [A loss: 0.755989, acc: 0.390625]\n",
      "1147: [D loss: 0.693469, acc: 0.537109]  [A loss: 1.012232, acc: 0.058594]\n",
      "1148: [D loss: 0.683740, acc: 0.558594]  [A loss: 0.729029, acc: 0.429688]\n",
      "1149: [D loss: 0.702405, acc: 0.529297]  [A loss: 1.077157, acc: 0.023438]\n",
      "1150: [D loss: 0.688497, acc: 0.541016]  [A loss: 0.644940, acc: 0.660156]\n",
      "1151: [D loss: 0.713628, acc: 0.507812]  [A loss: 1.013771, acc: 0.046875]\n",
      "1152: [D loss: 0.690410, acc: 0.519531]  [A loss: 0.811695, acc: 0.250000]\n",
      "1153: [D loss: 0.693062, acc: 0.544922]  [A loss: 0.888211, acc: 0.132812]\n",
      "1154: [D loss: 0.682942, acc: 0.541016]  [A loss: 0.795364, acc: 0.281250]\n",
      "1155: [D loss: 0.683002, acc: 0.560547]  [A loss: 0.969520, acc: 0.050781]\n",
      "1156: [D loss: 0.674340, acc: 0.593750]  [A loss: 0.820903, acc: 0.214844]\n",
      "1157: [D loss: 0.682762, acc: 0.576172]  [A loss: 0.974647, acc: 0.097656]\n",
      "1158: [D loss: 0.675899, acc: 0.583984]  [A loss: 0.805632, acc: 0.292969]\n",
      "1159: [D loss: 0.690213, acc: 0.548828]  [A loss: 0.951531, acc: 0.105469]\n",
      "1160: [D loss: 0.689992, acc: 0.533203]  [A loss: 0.751494, acc: 0.378906]\n",
      "1161: [D loss: 0.699000, acc: 0.519531]  [A loss: 0.932385, acc: 0.093750]\n",
      "1162: [D loss: 0.677855, acc: 0.562500]  [A loss: 0.777449, acc: 0.320312]\n",
      "1163: [D loss: 0.700365, acc: 0.507812]  [A loss: 1.019270, acc: 0.046875]\n",
      "1164: [D loss: 0.673101, acc: 0.585938]  [A loss: 0.772442, acc: 0.324219]\n",
      "1165: [D loss: 0.693854, acc: 0.558594]  [A loss: 0.951166, acc: 0.078125]\n",
      "1166: [D loss: 0.673874, acc: 0.578125]  [A loss: 0.775113, acc: 0.359375]\n",
      "1167: [D loss: 0.698196, acc: 0.537109]  [A loss: 0.984816, acc: 0.070312]\n",
      "1168: [D loss: 0.681936, acc: 0.541016]  [A loss: 0.737763, acc: 0.375000]\n",
      "1169: [D loss: 0.696363, acc: 0.533203]  [A loss: 0.925802, acc: 0.078125]\n",
      "1170: [D loss: 0.670009, acc: 0.578125]  [A loss: 0.841324, acc: 0.199219]\n",
      "1171: [D loss: 0.690969, acc: 0.562500]  [A loss: 0.978806, acc: 0.089844]\n",
      "1172: [D loss: 0.697701, acc: 0.523438]  [A loss: 0.788543, acc: 0.324219]\n",
      "1173: [D loss: 0.691290, acc: 0.537109]  [A loss: 1.021035, acc: 0.050781]\n",
      "1174: [D loss: 0.679836, acc: 0.556641]  [A loss: 0.751670, acc: 0.394531]\n",
      "1175: [D loss: 0.690883, acc: 0.539062]  [A loss: 1.010298, acc: 0.050781]\n",
      "1176: [D loss: 0.680366, acc: 0.558594]  [A loss: 0.661825, acc: 0.582031]\n",
      "1177: [D loss: 0.715965, acc: 0.531250]  [A loss: 0.985032, acc: 0.050781]\n",
      "1178: [D loss: 0.697697, acc: 0.533203]  [A loss: 0.763460, acc: 0.335938]\n",
      "1179: [D loss: 0.684108, acc: 0.552734]  [A loss: 0.928077, acc: 0.093750]\n",
      "1180: [D loss: 0.664474, acc: 0.607422]  [A loss: 0.796427, acc: 0.316406]\n",
      "1181: [D loss: 0.679790, acc: 0.568359]  [A loss: 0.888405, acc: 0.191406]\n",
      "1182: [D loss: 0.688701, acc: 0.552734]  [A loss: 0.851624, acc: 0.175781]\n",
      "1183: [D loss: 0.676545, acc: 0.574219]  [A loss: 0.833123, acc: 0.210938]\n",
      "1184: [D loss: 0.691256, acc: 0.529297]  [A loss: 0.868123, acc: 0.160156]\n",
      "1185: [D loss: 0.681090, acc: 0.583984]  [A loss: 0.866591, acc: 0.171875]\n",
      "1186: [D loss: 0.687397, acc: 0.546875]  [A loss: 0.950474, acc: 0.109375]\n",
      "1187: [D loss: 0.681567, acc: 0.560547]  [A loss: 0.837631, acc: 0.222656]\n",
      "1188: [D loss: 0.686037, acc: 0.554688]  [A loss: 0.878691, acc: 0.156250]\n",
      "1189: [D loss: 0.684761, acc: 0.527344]  [A loss: 0.792266, acc: 0.277344]\n",
      "1190: [D loss: 0.685924, acc: 0.580078]  [A loss: 0.953578, acc: 0.066406]\n",
      "1191: [D loss: 0.678216, acc: 0.570312]  [A loss: 0.877312, acc: 0.160156]\n",
      "1192: [D loss: 0.703880, acc: 0.523438]  [A loss: 0.977479, acc: 0.070312]\n",
      "1193: [D loss: 0.685410, acc: 0.544922]  [A loss: 0.885146, acc: 0.156250]\n",
      "1194: [D loss: 0.688774, acc: 0.533203]  [A loss: 0.939209, acc: 0.128906]\n",
      "1195: [D loss: 0.681403, acc: 0.562500]  [A loss: 0.751460, acc: 0.351562]\n",
      "1196: [D loss: 0.701819, acc: 0.533203]  [A loss: 1.061704, acc: 0.046875]\n",
      "1197: [D loss: 0.669601, acc: 0.574219]  [A loss: 0.664327, acc: 0.585938]\n",
      "1198: [D loss: 0.707687, acc: 0.529297]  [A loss: 1.100998, acc: 0.054688]\n",
      "1199: [D loss: 0.683788, acc: 0.544922]  [A loss: 0.656257, acc: 0.605469]\n",
      "1200: [D loss: 0.716719, acc: 0.519531]  [A loss: 0.995579, acc: 0.062500]\n",
      "1201: [D loss: 0.702581, acc: 0.515625]  [A loss: 0.782198, acc: 0.324219]\n",
      "1202: [D loss: 0.702648, acc: 0.533203]  [A loss: 0.921638, acc: 0.117188]\n",
      "1203: [D loss: 0.674892, acc: 0.585938]  [A loss: 0.776252, acc: 0.320312]\n",
      "1204: [D loss: 0.703072, acc: 0.509766]  [A loss: 0.915256, acc: 0.113281]\n",
      "1205: [D loss: 0.693667, acc: 0.519531]  [A loss: 0.830242, acc: 0.214844]\n",
      "1206: [D loss: 0.674975, acc: 0.580078]  [A loss: 0.805141, acc: 0.296875]\n",
      "1207: [D loss: 0.685975, acc: 0.544922]  [A loss: 0.905540, acc: 0.117188]\n",
      "1208: [D loss: 0.674166, acc: 0.562500]  [A loss: 0.862793, acc: 0.171875]\n",
      "1209: [D loss: 0.677812, acc: 0.564453]  [A loss: 0.928508, acc: 0.105469]\n",
      "1210: [D loss: 0.678868, acc: 0.542969]  [A loss: 0.832193, acc: 0.242188]\n",
      "1211: [D loss: 0.704573, acc: 0.525391]  [A loss: 1.032830, acc: 0.035156]\n",
      "1212: [D loss: 0.689566, acc: 0.576172]  [A loss: 0.736672, acc: 0.394531]\n",
      "1213: [D loss: 0.721976, acc: 0.500000]  [A loss: 0.978401, acc: 0.054688]\n",
      "1214: [D loss: 0.675914, acc: 0.574219]  [A loss: 0.811121, acc: 0.292969]\n",
      "1215: [D loss: 0.685318, acc: 0.552734]  [A loss: 0.945565, acc: 0.113281]\n",
      "1216: [D loss: 0.673614, acc: 0.582031]  [A loss: 0.854138, acc: 0.195312]\n",
      "1217: [D loss: 0.687041, acc: 0.544922]  [A loss: 0.918959, acc: 0.082031]\n",
      "1218: [D loss: 0.686331, acc: 0.541016]  [A loss: 0.833305, acc: 0.238281]\n",
      "1219: [D loss: 0.693356, acc: 0.531250]  [A loss: 1.000183, acc: 0.042969]\n",
      "1220: [D loss: 0.682925, acc: 0.537109]  [A loss: 0.751664, acc: 0.363281]\n",
      "1221: [D loss: 0.710517, acc: 0.521484]  [A loss: 0.918104, acc: 0.089844]\n",
      "1222: [D loss: 0.692026, acc: 0.525391]  [A loss: 0.917936, acc: 0.070312]\n",
      "1223: [D loss: 0.687684, acc: 0.529297]  [A loss: 0.857071, acc: 0.179688]\n",
      "1224: [D loss: 0.682389, acc: 0.574219]  [A loss: 0.853560, acc: 0.171875]\n",
      "1225: [D loss: 0.679700, acc: 0.580078]  [A loss: 0.953558, acc: 0.125000]\n",
      "1226: [D loss: 0.674891, acc: 0.576172]  [A loss: 0.778608, acc: 0.281250]\n",
      "1227: [D loss: 0.702784, acc: 0.531250]  [A loss: 1.007627, acc: 0.046875]\n",
      "1228: [D loss: 0.690464, acc: 0.531250]  [A loss: 0.691332, acc: 0.535156]\n",
      "1229: [D loss: 0.712492, acc: 0.527344]  [A loss: 1.060420, acc: 0.054688]\n",
      "1230: [D loss: 0.688090, acc: 0.548828]  [A loss: 0.671745, acc: 0.597656]\n",
      "1231: [D loss: 0.726144, acc: 0.507812]  [A loss: 1.078035, acc: 0.035156]\n",
      "1232: [D loss: 0.682144, acc: 0.556641]  [A loss: 0.655195, acc: 0.636719]\n",
      "1233: [D loss: 0.698740, acc: 0.531250]  [A loss: 0.919205, acc: 0.132812]\n",
      "1234: [D loss: 0.678874, acc: 0.564453]  [A loss: 0.824091, acc: 0.222656]\n",
      "1235: [D loss: 0.675241, acc: 0.574219]  [A loss: 0.909642, acc: 0.113281]\n",
      "1236: [D loss: 0.682950, acc: 0.591797]  [A loss: 0.788039, acc: 0.300781]\n",
      "1237: [D loss: 0.694518, acc: 0.529297]  [A loss: 0.885897, acc: 0.160156]\n",
      "1238: [D loss: 0.683124, acc: 0.542969]  [A loss: 0.857603, acc: 0.140625]\n",
      "1239: [D loss: 0.680876, acc: 0.580078]  [A loss: 0.849267, acc: 0.210938]\n",
      "1240: [D loss: 0.690291, acc: 0.552734]  [A loss: 0.879077, acc: 0.144531]\n",
      "1241: [D loss: 0.686413, acc: 0.535156]  [A loss: 0.779596, acc: 0.296875]\n",
      "1242: [D loss: 0.693232, acc: 0.531250]  [A loss: 0.976725, acc: 0.097656]\n",
      "1243: [D loss: 0.667464, acc: 0.609375]  [A loss: 0.719511, acc: 0.449219]\n",
      "1244: [D loss: 0.717335, acc: 0.535156]  [A loss: 0.979482, acc: 0.062500]\n",
      "1245: [D loss: 0.680456, acc: 0.554688]  [A loss: 0.794185, acc: 0.312500]\n",
      "1246: [D loss: 0.704265, acc: 0.503906]  [A loss: 0.975201, acc: 0.046875]\n",
      "1247: [D loss: 0.691348, acc: 0.550781]  [A loss: 0.770107, acc: 0.332031]\n",
      "1248: [D loss: 0.709114, acc: 0.511719]  [A loss: 1.067510, acc: 0.039062]\n",
      "1249: [D loss: 0.688895, acc: 0.515625]  [A loss: 0.753809, acc: 0.367188]\n",
      "1250: [D loss: 0.698645, acc: 0.544922]  [A loss: 0.934370, acc: 0.089844]\n",
      "1251: [D loss: 0.692064, acc: 0.519531]  [A loss: 0.789137, acc: 0.250000]\n",
      "1252: [D loss: 0.683253, acc: 0.546875]  [A loss: 0.924070, acc: 0.093750]\n",
      "1253: [D loss: 0.675733, acc: 0.595703]  [A loss: 0.793451, acc: 0.316406]\n",
      "1254: [D loss: 0.685048, acc: 0.562500]  [A loss: 0.928484, acc: 0.125000]\n",
      "1255: [D loss: 0.697799, acc: 0.521484]  [A loss: 0.754425, acc: 0.328125]\n",
      "1256: [D loss: 0.695208, acc: 0.531250]  [A loss: 0.891177, acc: 0.156250]\n",
      "1257: [D loss: 0.672579, acc: 0.605469]  [A loss: 0.879600, acc: 0.156250]\n",
      "1258: [D loss: 0.687154, acc: 0.548828]  [A loss: 0.844659, acc: 0.210938]\n",
      "1259: [D loss: 0.678470, acc: 0.558594]  [A loss: 0.840025, acc: 0.187500]\n",
      "1260: [D loss: 0.698669, acc: 0.539062]  [A loss: 0.935289, acc: 0.074219]\n",
      "1261: [D loss: 0.677276, acc: 0.580078]  [A loss: 0.747721, acc: 0.410156]\n",
      "1262: [D loss: 0.711780, acc: 0.507812]  [A loss: 0.948658, acc: 0.136719]\n",
      "1263: [D loss: 0.704032, acc: 0.517578]  [A loss: 0.947415, acc: 0.074219]\n",
      "1264: [D loss: 0.682556, acc: 0.544922]  [A loss: 0.818548, acc: 0.257812]\n",
      "1265: [D loss: 0.676100, acc: 0.574219]  [A loss: 0.897692, acc: 0.191406]\n",
      "1266: [D loss: 0.712112, acc: 0.496094]  [A loss: 0.910368, acc: 0.117188]\n",
      "1267: [D loss: 0.688741, acc: 0.539062]  [A loss: 0.847240, acc: 0.179688]\n",
      "1268: [D loss: 0.700471, acc: 0.529297]  [A loss: 0.934524, acc: 0.093750]\n",
      "1269: [D loss: 0.672147, acc: 0.580078]  [A loss: 0.739605, acc: 0.425781]\n",
      "1270: [D loss: 0.707540, acc: 0.535156]  [A loss: 1.044562, acc: 0.019531]\n",
      "1271: [D loss: 0.678940, acc: 0.550781]  [A loss: 0.700941, acc: 0.464844]\n",
      "1272: [D loss: 0.706146, acc: 0.517578]  [A loss: 1.040936, acc: 0.042969]\n",
      "1273: [D loss: 0.689375, acc: 0.558594]  [A loss: 0.713286, acc: 0.476562]\n",
      "1274: [D loss: 0.702438, acc: 0.519531]  [A loss: 0.927017, acc: 0.105469]\n",
      "1275: [D loss: 0.679990, acc: 0.580078]  [A loss: 0.759924, acc: 0.347656]\n",
      "1276: [D loss: 0.708751, acc: 0.505859]  [A loss: 0.947823, acc: 0.078125]\n",
      "1277: [D loss: 0.676880, acc: 0.580078]  [A loss: 0.756364, acc: 0.390625]\n",
      "1278: [D loss: 0.673027, acc: 0.580078]  [A loss: 0.891064, acc: 0.191406]\n",
      "1279: [D loss: 0.677572, acc: 0.583984]  [A loss: 0.809334, acc: 0.226562]\n",
      "1280: [D loss: 0.682410, acc: 0.550781]  [A loss: 0.915147, acc: 0.148438]\n",
      "1281: [D loss: 0.689390, acc: 0.548828]  [A loss: 0.831357, acc: 0.257812]\n",
      "1282: [D loss: 0.697297, acc: 0.523438]  [A loss: 0.885803, acc: 0.148438]\n",
      "1283: [D loss: 0.684194, acc: 0.554688]  [A loss: 0.813207, acc: 0.253906]\n",
      "1284: [D loss: 0.696996, acc: 0.525391]  [A loss: 0.938799, acc: 0.093750]\n",
      "1285: [D loss: 0.681980, acc: 0.583984]  [A loss: 0.750768, acc: 0.375000]\n",
      "1286: [D loss: 0.696067, acc: 0.542969]  [A loss: 1.023841, acc: 0.070312]\n",
      "1287: [D loss: 0.670066, acc: 0.580078]  [A loss: 0.720692, acc: 0.433594]\n",
      "1288: [D loss: 0.709988, acc: 0.515625]  [A loss: 1.039725, acc: 0.039062]\n",
      "1289: [D loss: 0.680285, acc: 0.576172]  [A loss: 0.779632, acc: 0.316406]\n",
      "1290: [D loss: 0.684606, acc: 0.525391]  [A loss: 0.956502, acc: 0.113281]\n",
      "1291: [D loss: 0.691804, acc: 0.523438]  [A loss: 0.781462, acc: 0.296875]\n",
      "1292: [D loss: 0.689481, acc: 0.521484]  [A loss: 1.011076, acc: 0.066406]\n",
      "1293: [D loss: 0.678831, acc: 0.570312]  [A loss: 0.718881, acc: 0.453125]\n",
      "1294: [D loss: 0.716014, acc: 0.503906]  [A loss: 1.004269, acc: 0.082031]\n",
      "1295: [D loss: 0.684277, acc: 0.556641]  [A loss: 0.785019, acc: 0.289062]\n",
      "1296: [D loss: 0.679849, acc: 0.558594]  [A loss: 0.901557, acc: 0.117188]\n",
      "1297: [D loss: 0.675861, acc: 0.564453]  [A loss: 0.766124, acc: 0.296875]\n",
      "1298: [D loss: 0.680077, acc: 0.568359]  [A loss: 0.938992, acc: 0.093750]\n",
      "1299: [D loss: 0.682543, acc: 0.566406]  [A loss: 0.771816, acc: 0.320312]\n",
      "1300: [D loss: 0.688658, acc: 0.562500]  [A loss: 0.992482, acc: 0.089844]\n",
      "1301: [D loss: 0.676300, acc: 0.593750]  [A loss: 0.774207, acc: 0.335938]\n",
      "1302: [D loss: 0.693994, acc: 0.556641]  [A loss: 0.986232, acc: 0.070312]\n",
      "1303: [D loss: 0.689089, acc: 0.554688]  [A loss: 0.828957, acc: 0.226562]\n",
      "1304: [D loss: 0.683802, acc: 0.552734]  [A loss: 0.880791, acc: 0.140625]\n",
      "1305: [D loss: 0.680012, acc: 0.560547]  [A loss: 0.877612, acc: 0.175781]\n",
      "1306: [D loss: 0.681303, acc: 0.550781]  [A loss: 0.842579, acc: 0.222656]\n",
      "1307: [D loss: 0.679858, acc: 0.560547]  [A loss: 0.911446, acc: 0.132812]\n",
      "1308: [D loss: 0.678568, acc: 0.568359]  [A loss: 0.739174, acc: 0.414062]\n",
      "1309: [D loss: 0.697475, acc: 0.552734]  [A loss: 1.024584, acc: 0.042969]\n",
      "1310: [D loss: 0.687065, acc: 0.546875]  [A loss: 0.740821, acc: 0.402344]\n",
      "1311: [D loss: 0.691858, acc: 0.537109]  [A loss: 1.001730, acc: 0.066406]\n",
      "1312: [D loss: 0.683550, acc: 0.562500]  [A loss: 0.744444, acc: 0.429688]\n",
      "1313: [D loss: 0.700489, acc: 0.515625]  [A loss: 0.967598, acc: 0.070312]\n",
      "1314: [D loss: 0.680091, acc: 0.542969]  [A loss: 0.703999, acc: 0.492188]\n",
      "1315: [D loss: 0.713550, acc: 0.531250]  [A loss: 0.955558, acc: 0.085938]\n",
      "1316: [D loss: 0.689355, acc: 0.484375]  [A loss: 0.737067, acc: 0.472656]\n",
      "1317: [D loss: 0.691775, acc: 0.554688]  [A loss: 1.001556, acc: 0.074219]\n",
      "1318: [D loss: 0.689509, acc: 0.544922]  [A loss: 0.779543, acc: 0.347656]\n",
      "1319: [D loss: 0.691488, acc: 0.550781]  [A loss: 0.860831, acc: 0.183594]\n",
      "1320: [D loss: 0.694363, acc: 0.546875]  [A loss: 0.842483, acc: 0.183594]\n",
      "1321: [D loss: 0.691507, acc: 0.560547]  [A loss: 0.907831, acc: 0.097656]\n",
      "1322: [D loss: 0.677990, acc: 0.570312]  [A loss: 0.802116, acc: 0.257812]\n",
      "1323: [D loss: 0.690070, acc: 0.539062]  [A loss: 0.903993, acc: 0.093750]\n",
      "1324: [D loss: 0.679189, acc: 0.564453]  [A loss: 0.824214, acc: 0.261719]\n",
      "1325: [D loss: 0.711294, acc: 0.542969]  [A loss: 0.941884, acc: 0.066406]\n",
      "1326: [D loss: 0.679551, acc: 0.560547]  [A loss: 0.789649, acc: 0.285156]\n",
      "1327: [D loss: 0.695835, acc: 0.529297]  [A loss: 0.937109, acc: 0.125000]\n",
      "1328: [D loss: 0.683704, acc: 0.535156]  [A loss: 0.787710, acc: 0.273438]\n",
      "1329: [D loss: 0.691139, acc: 0.539062]  [A loss: 0.947476, acc: 0.093750]\n",
      "1330: [D loss: 0.670113, acc: 0.576172]  [A loss: 0.746521, acc: 0.410156]\n",
      "1331: [D loss: 0.694562, acc: 0.531250]  [A loss: 0.892048, acc: 0.113281]\n",
      "1332: [D loss: 0.677821, acc: 0.566406]  [A loss: 0.832742, acc: 0.230469]\n",
      "1333: [D loss: 0.684943, acc: 0.523438]  [A loss: 0.925703, acc: 0.109375]\n",
      "1334: [D loss: 0.699128, acc: 0.558594]  [A loss: 0.767784, acc: 0.343750]\n",
      "1335: [D loss: 0.697805, acc: 0.544922]  [A loss: 1.042027, acc: 0.039062]\n",
      "1336: [D loss: 0.694964, acc: 0.533203]  [A loss: 0.673159, acc: 0.566406]\n",
      "1337: [D loss: 0.694080, acc: 0.544922]  [A loss: 0.954377, acc: 0.093750]\n",
      "1338: [D loss: 0.702326, acc: 0.533203]  [A loss: 0.833220, acc: 0.253906]\n",
      "1339: [D loss: 0.680462, acc: 0.552734]  [A loss: 0.891863, acc: 0.144531]\n",
      "1340: [D loss: 0.705674, acc: 0.527344]  [A loss: 0.808841, acc: 0.234375]\n",
      "1341: [D loss: 0.685213, acc: 0.541016]  [A loss: 0.855361, acc: 0.167969]\n",
      "1342: [D loss: 0.697374, acc: 0.539062]  [A loss: 0.828924, acc: 0.210938]\n",
      "1343: [D loss: 0.677671, acc: 0.568359]  [A loss: 0.802563, acc: 0.296875]\n",
      "1344: [D loss: 0.699212, acc: 0.517578]  [A loss: 0.984482, acc: 0.078125]\n",
      "1345: [D loss: 0.671523, acc: 0.566406]  [A loss: 0.738527, acc: 0.390625]\n",
      "1346: [D loss: 0.712335, acc: 0.517578]  [A loss: 1.022355, acc: 0.031250]\n",
      "1347: [D loss: 0.703553, acc: 0.492188]  [A loss: 0.752594, acc: 0.371094]\n",
      "1348: [D loss: 0.696244, acc: 0.517578]  [A loss: 0.916450, acc: 0.113281]\n",
      "1349: [D loss: 0.698749, acc: 0.525391]  [A loss: 0.840090, acc: 0.156250]\n",
      "1350: [D loss: 0.697592, acc: 0.509766]  [A loss: 0.931076, acc: 0.109375]\n",
      "1351: [D loss: 0.676923, acc: 0.576172]  [A loss: 0.774596, acc: 0.363281]\n",
      "1352: [D loss: 0.688909, acc: 0.546875]  [A loss: 0.899443, acc: 0.160156]\n",
      "1353: [D loss: 0.680054, acc: 0.560547]  [A loss: 0.791479, acc: 0.250000]\n",
      "1354: [D loss: 0.696499, acc: 0.511719]  [A loss: 0.979681, acc: 0.058594]\n",
      "1355: [D loss: 0.688845, acc: 0.558594]  [A loss: 0.734945, acc: 0.410156]\n",
      "1356: [D loss: 0.708941, acc: 0.515625]  [A loss: 1.032131, acc: 0.039062]\n",
      "1357: [D loss: 0.687223, acc: 0.525391]  [A loss: 0.679530, acc: 0.562500]\n",
      "1358: [D loss: 0.717106, acc: 0.511719]  [A loss: 0.995652, acc: 0.082031]\n",
      "1359: [D loss: 0.692601, acc: 0.521484]  [A loss: 0.710814, acc: 0.496094]\n",
      "1360: [D loss: 0.699796, acc: 0.542969]  [A loss: 0.871181, acc: 0.152344]\n",
      "1361: [D loss: 0.691297, acc: 0.541016]  [A loss: 0.799022, acc: 0.285156]\n",
      "1362: [D loss: 0.693580, acc: 0.527344]  [A loss: 0.830823, acc: 0.210938]\n",
      "1363: [D loss: 0.698414, acc: 0.537109]  [A loss: 0.823769, acc: 0.230469]\n",
      "1364: [D loss: 0.687495, acc: 0.535156]  [A loss: 0.919411, acc: 0.117188]\n",
      "1365: [D loss: 0.691879, acc: 0.542969]  [A loss: 0.830279, acc: 0.207031]\n",
      "1366: [D loss: 0.687498, acc: 0.550781]  [A loss: 0.890467, acc: 0.128906]\n",
      "1367: [D loss: 0.681903, acc: 0.546875]  [A loss: 0.817922, acc: 0.214844]\n",
      "1368: [D loss: 0.686081, acc: 0.566406]  [A loss: 0.878501, acc: 0.144531]\n",
      "1369: [D loss: 0.694105, acc: 0.541016]  [A loss: 0.849627, acc: 0.175781]\n",
      "1370: [D loss: 0.686898, acc: 0.535156]  [A loss: 0.914811, acc: 0.101562]\n",
      "1371: [D loss: 0.678563, acc: 0.562500]  [A loss: 0.814281, acc: 0.234375]\n",
      "1372: [D loss: 0.680028, acc: 0.566406]  [A loss: 0.935630, acc: 0.085938]\n",
      "1373: [D loss: 0.686069, acc: 0.562500]  [A loss: 0.765342, acc: 0.328125]\n",
      "1374: [D loss: 0.688497, acc: 0.564453]  [A loss: 0.961821, acc: 0.097656]\n",
      "1375: [D loss: 0.682090, acc: 0.564453]  [A loss: 0.748911, acc: 0.390625]\n",
      "1376: [D loss: 0.706821, acc: 0.531250]  [A loss: 0.979774, acc: 0.062500]\n",
      "1377: [D loss: 0.683222, acc: 0.564453]  [A loss: 0.704449, acc: 0.515625]\n",
      "1378: [D loss: 0.710429, acc: 0.533203]  [A loss: 1.035415, acc: 0.019531]\n",
      "1379: [D loss: 0.682711, acc: 0.564453]  [A loss: 0.726693, acc: 0.441406]\n",
      "1380: [D loss: 0.710677, acc: 0.509766]  [A loss: 0.976307, acc: 0.089844]\n",
      "1381: [D loss: 0.666982, acc: 0.597656]  [A loss: 0.726349, acc: 0.414062]\n",
      "1382: [D loss: 0.698852, acc: 0.542969]  [A loss: 0.945978, acc: 0.109375]\n",
      "1383: [D loss: 0.684549, acc: 0.556641]  [A loss: 0.829867, acc: 0.191406]\n",
      "1384: [D loss: 0.679094, acc: 0.583984]  [A loss: 0.897238, acc: 0.140625]\n",
      "1385: [D loss: 0.698565, acc: 0.531250]  [A loss: 0.877338, acc: 0.144531]\n",
      "1386: [D loss: 0.681796, acc: 0.572266]  [A loss: 0.847174, acc: 0.191406]\n",
      "1387: [D loss: 0.682654, acc: 0.562500]  [A loss: 0.895560, acc: 0.148438]\n",
      "1388: [D loss: 0.693684, acc: 0.537109]  [A loss: 0.780750, acc: 0.347656]\n",
      "1389: [D loss: 0.685568, acc: 0.552734]  [A loss: 0.900472, acc: 0.125000]\n",
      "1390: [D loss: 0.680866, acc: 0.572266]  [A loss: 0.793783, acc: 0.269531]\n",
      "1391: [D loss: 0.696047, acc: 0.517578]  [A loss: 0.918831, acc: 0.089844]\n",
      "1392: [D loss: 0.680992, acc: 0.564453]  [A loss: 0.787039, acc: 0.324219]\n",
      "1393: [D loss: 0.681904, acc: 0.570312]  [A loss: 0.970653, acc: 0.105469]\n",
      "1394: [D loss: 0.689686, acc: 0.533203]  [A loss: 0.733520, acc: 0.382812]\n",
      "1395: [D loss: 0.699651, acc: 0.542969]  [A loss: 0.944880, acc: 0.101562]\n",
      "1396: [D loss: 0.673619, acc: 0.568359]  [A loss: 0.748353, acc: 0.433594]\n",
      "1397: [D loss: 0.722226, acc: 0.498047]  [A loss: 0.925394, acc: 0.085938]\n",
      "1398: [D loss: 0.695666, acc: 0.550781]  [A loss: 0.921568, acc: 0.082031]\n",
      "1399: [D loss: 0.679862, acc: 0.539062]  [A loss: 0.728093, acc: 0.417969]\n",
      "1400: [D loss: 0.724643, acc: 0.503906]  [A loss: 1.003018, acc: 0.082031]\n",
      "1401: [D loss: 0.693018, acc: 0.554688]  [A loss: 0.757350, acc: 0.355469]\n",
      "1402: [D loss: 0.697518, acc: 0.523438]  [A loss: 0.948215, acc: 0.078125]\n",
      "1403: [D loss: 0.692904, acc: 0.525391]  [A loss: 0.740959, acc: 0.382812]\n",
      "1404: [D loss: 0.709249, acc: 0.521484]  [A loss: 0.966054, acc: 0.054688]\n",
      "1405: [D loss: 0.679677, acc: 0.566406]  [A loss: 0.774578, acc: 0.328125]\n",
      "1406: [D loss: 0.712793, acc: 0.539062]  [A loss: 0.935013, acc: 0.085938]\n",
      "1407: [D loss: 0.689109, acc: 0.527344]  [A loss: 0.751667, acc: 0.371094]\n",
      "1408: [D loss: 0.694872, acc: 0.539062]  [A loss: 0.942077, acc: 0.109375]\n",
      "1409: [D loss: 0.681400, acc: 0.582031]  [A loss: 0.762857, acc: 0.347656]\n",
      "1410: [D loss: 0.692912, acc: 0.548828]  [A loss: 0.859890, acc: 0.164062]\n",
      "1411: [D loss: 0.696841, acc: 0.521484]  [A loss: 0.802155, acc: 0.296875]\n",
      "1412: [D loss: 0.704767, acc: 0.496094]  [A loss: 0.839971, acc: 0.195312]\n",
      "1413: [D loss: 0.693762, acc: 0.541016]  [A loss: 0.805557, acc: 0.257812]\n",
      "1414: [D loss: 0.686759, acc: 0.537109]  [A loss: 0.815834, acc: 0.242188]\n",
      "1415: [D loss: 0.692595, acc: 0.566406]  [A loss: 0.827813, acc: 0.234375]\n",
      "1416: [D loss: 0.685999, acc: 0.544922]  [A loss: 0.830045, acc: 0.191406]\n",
      "1417: [D loss: 0.692582, acc: 0.539062]  [A loss: 0.841829, acc: 0.191406]\n",
      "1418: [D loss: 0.682408, acc: 0.562500]  [A loss: 0.896009, acc: 0.144531]\n",
      "1419: [D loss: 0.675055, acc: 0.570312]  [A loss: 0.858838, acc: 0.175781]\n",
      "1420: [D loss: 0.688849, acc: 0.564453]  [A loss: 0.811860, acc: 0.289062]\n",
      "1421: [D loss: 0.698308, acc: 0.533203]  [A loss: 0.970181, acc: 0.085938]\n",
      "1422: [D loss: 0.689688, acc: 0.531250]  [A loss: 0.777177, acc: 0.332031]\n",
      "1423: [D loss: 0.687256, acc: 0.560547]  [A loss: 0.874706, acc: 0.148438]\n",
      "1424: [D loss: 0.685261, acc: 0.556641]  [A loss: 0.800109, acc: 0.316406]\n",
      "1425: [D loss: 0.693267, acc: 0.564453]  [A loss: 0.954758, acc: 0.097656]\n",
      "1426: [D loss: 0.677488, acc: 0.564453]  [A loss: 0.735064, acc: 0.429688]\n",
      "1427: [D loss: 0.697191, acc: 0.552734]  [A loss: 0.963292, acc: 0.109375]\n",
      "1428: [D loss: 0.694571, acc: 0.523438]  [A loss: 0.720514, acc: 0.472656]\n",
      "1429: [D loss: 0.718861, acc: 0.521484]  [A loss: 0.961730, acc: 0.078125]\n",
      "1430: [D loss: 0.675193, acc: 0.576172]  [A loss: 0.705874, acc: 0.472656]\n",
      "1431: [D loss: 0.707350, acc: 0.541016]  [A loss: 0.903370, acc: 0.097656]\n",
      "1432: [D loss: 0.684532, acc: 0.560547]  [A loss: 0.865926, acc: 0.167969]\n",
      "1433: [D loss: 0.689622, acc: 0.539062]  [A loss: 0.776719, acc: 0.304688]\n",
      "1434: [D loss: 0.685085, acc: 0.574219]  [A loss: 0.870765, acc: 0.183594]\n",
      "1435: [D loss: 0.684775, acc: 0.541016]  [A loss: 0.803630, acc: 0.277344]\n",
      "1436: [D loss: 0.692911, acc: 0.556641]  [A loss: 0.889185, acc: 0.125000]\n",
      "1437: [D loss: 0.682278, acc: 0.562500]  [A loss: 0.839626, acc: 0.183594]\n",
      "1438: [D loss: 0.691054, acc: 0.521484]  [A loss: 0.815637, acc: 0.257812]\n",
      "1439: [D loss: 0.694890, acc: 0.541016]  [A loss: 0.934882, acc: 0.117188]\n",
      "1440: [D loss: 0.675165, acc: 0.597656]  [A loss: 0.738224, acc: 0.425781]\n",
      "1441: [D loss: 0.708277, acc: 0.501953]  [A loss: 0.964727, acc: 0.117188]\n",
      "1442: [D loss: 0.694234, acc: 0.548828]  [A loss: 0.731959, acc: 0.390625]\n",
      "1443: [D loss: 0.699736, acc: 0.529297]  [A loss: 0.917959, acc: 0.082031]\n",
      "1444: [D loss: 0.685393, acc: 0.574219]  [A loss: 0.753094, acc: 0.355469]\n",
      "1445: [D loss: 0.680178, acc: 0.574219]  [A loss: 0.904544, acc: 0.128906]\n",
      "1446: [D loss: 0.677482, acc: 0.570312]  [A loss: 0.826971, acc: 0.242188]\n",
      "1447: [D loss: 0.686166, acc: 0.568359]  [A loss: 0.837690, acc: 0.210938]\n",
      "1448: [D loss: 0.690850, acc: 0.554688]  [A loss: 0.902794, acc: 0.156250]\n",
      "1449: [D loss: 0.680770, acc: 0.583984]  [A loss: 0.780490, acc: 0.296875]\n",
      "1450: [D loss: 0.692638, acc: 0.531250]  [A loss: 0.852049, acc: 0.167969]\n",
      "1451: [D loss: 0.680970, acc: 0.548828]  [A loss: 0.828976, acc: 0.250000]\n",
      "1452: [D loss: 0.691290, acc: 0.566406]  [A loss: 0.888289, acc: 0.136719]\n",
      "1453: [D loss: 0.691273, acc: 0.533203]  [A loss: 0.856143, acc: 0.191406]\n",
      "1454: [D loss: 0.691440, acc: 0.544922]  [A loss: 0.880760, acc: 0.136719]\n",
      "1455: [D loss: 0.694636, acc: 0.560547]  [A loss: 0.884561, acc: 0.171875]\n",
      "1456: [D loss: 0.678811, acc: 0.589844]  [A loss: 0.774732, acc: 0.335938]\n",
      "1457: [D loss: 0.696586, acc: 0.529297]  [A loss: 0.980428, acc: 0.085938]\n",
      "1458: [D loss: 0.693869, acc: 0.552734]  [A loss: 0.756112, acc: 0.355469]\n",
      "1459: [D loss: 0.694718, acc: 0.515625]  [A loss: 0.907523, acc: 0.132812]\n",
      "1460: [D loss: 0.679802, acc: 0.554688]  [A loss: 0.732717, acc: 0.433594]\n",
      "1461: [D loss: 0.692398, acc: 0.531250]  [A loss: 0.952482, acc: 0.097656]\n",
      "1462: [D loss: 0.686238, acc: 0.548828]  [A loss: 0.718449, acc: 0.476562]\n",
      "1463: [D loss: 0.699068, acc: 0.529297]  [A loss: 0.884936, acc: 0.152344]\n",
      "1464: [D loss: 0.683523, acc: 0.564453]  [A loss: 0.853122, acc: 0.183594]\n",
      "1465: [D loss: 0.688026, acc: 0.541016]  [A loss: 0.862370, acc: 0.175781]\n",
      "1466: [D loss: 0.685149, acc: 0.570312]  [A loss: 0.881721, acc: 0.132812]\n",
      "1467: [D loss: 0.688534, acc: 0.578125]  [A loss: 0.910997, acc: 0.148438]\n",
      "1468: [D loss: 0.686690, acc: 0.554688]  [A loss: 0.843206, acc: 0.191406]\n",
      "1469: [D loss: 0.691267, acc: 0.535156]  [A loss: 0.904894, acc: 0.160156]\n",
      "1470: [D loss: 0.703873, acc: 0.464844]  [A loss: 0.784625, acc: 0.261719]\n",
      "1471: [D loss: 0.675282, acc: 0.583984]  [A loss: 0.932610, acc: 0.121094]\n",
      "1472: [D loss: 0.698939, acc: 0.509766]  [A loss: 0.805765, acc: 0.300781]\n",
      "1473: [D loss: 0.696187, acc: 0.521484]  [A loss: 0.903144, acc: 0.113281]\n",
      "1474: [D loss: 0.683300, acc: 0.566406]  [A loss: 0.839670, acc: 0.171875]\n",
      "1475: [D loss: 0.696164, acc: 0.537109]  [A loss: 0.863345, acc: 0.164062]\n",
      "1476: [D loss: 0.693137, acc: 0.558594]  [A loss: 0.900192, acc: 0.078125]\n",
      "1477: [D loss: 0.683550, acc: 0.558594]  [A loss: 0.806495, acc: 0.230469]\n",
      "1478: [D loss: 0.696864, acc: 0.535156]  [A loss: 0.973871, acc: 0.074219]\n",
      "1479: [D loss: 0.688199, acc: 0.544922]  [A loss: 0.691222, acc: 0.542969]\n",
      "1480: [D loss: 0.729214, acc: 0.505859]  [A loss: 1.087889, acc: 0.019531]\n",
      "1481: [D loss: 0.690187, acc: 0.546875]  [A loss: 0.702332, acc: 0.492188]\n",
      "1482: [D loss: 0.710153, acc: 0.517578]  [A loss: 0.921368, acc: 0.117188]\n",
      "1483: [D loss: 0.674798, acc: 0.560547]  [A loss: 0.741106, acc: 0.363281]\n",
      "1484: [D loss: 0.697939, acc: 0.525391]  [A loss: 0.926879, acc: 0.097656]\n",
      "1485: [D loss: 0.675879, acc: 0.562500]  [A loss: 0.752548, acc: 0.390625]\n",
      "1486: [D loss: 0.692058, acc: 0.554688]  [A loss: 0.868734, acc: 0.187500]\n",
      "1487: [D loss: 0.698740, acc: 0.529297]  [A loss: 0.846319, acc: 0.167969]\n",
      "1488: [D loss: 0.701682, acc: 0.503906]  [A loss: 0.852414, acc: 0.148438]\n",
      "1489: [D loss: 0.691752, acc: 0.544922]  [A loss: 0.834530, acc: 0.199219]\n",
      "1490: [D loss: 0.668433, acc: 0.587891]  [A loss: 0.811545, acc: 0.246094]\n",
      "1491: [D loss: 0.694318, acc: 0.544922]  [A loss: 0.891760, acc: 0.132812]\n",
      "1492: [D loss: 0.686225, acc: 0.570312]  [A loss: 0.802323, acc: 0.300781]\n",
      "1493: [D loss: 0.681294, acc: 0.562500]  [A loss: 0.865557, acc: 0.187500]\n",
      "1494: [D loss: 0.694261, acc: 0.511719]  [A loss: 0.840196, acc: 0.195312]\n",
      "1495: [D loss: 0.685194, acc: 0.564453]  [A loss: 0.799041, acc: 0.281250]\n",
      "1496: [D loss: 0.671206, acc: 0.593750]  [A loss: 0.858907, acc: 0.199219]\n",
      "1497: [D loss: 0.696391, acc: 0.550781]  [A loss: 0.882301, acc: 0.160156]\n",
      "1498: [D loss: 0.690980, acc: 0.517578]  [A loss: 0.991392, acc: 0.046875]\n",
      "1499: [D loss: 0.697745, acc: 0.519531]  [A loss: 0.706305, acc: 0.511719]\n",
      "1500: [D loss: 0.721100, acc: 0.490234]  [A loss: 1.048488, acc: 0.031250]\n",
      "1501: [D loss: 0.704410, acc: 0.501953]  [A loss: 0.688691, acc: 0.550781]\n",
      "1502: [D loss: 0.712022, acc: 0.511719]  [A loss: 0.899903, acc: 0.125000]\n",
      "1503: [D loss: 0.698922, acc: 0.523438]  [A loss: 0.755713, acc: 0.359375]\n",
      "1504: [D loss: 0.689680, acc: 0.525391]  [A loss: 0.903152, acc: 0.117188]\n",
      "1505: [D loss: 0.676136, acc: 0.578125]  [A loss: 0.770355, acc: 0.347656]\n",
      "1506: [D loss: 0.698753, acc: 0.566406]  [A loss: 0.880061, acc: 0.132812]\n",
      "1507: [D loss: 0.672202, acc: 0.601562]  [A loss: 0.771846, acc: 0.347656]\n",
      "1508: [D loss: 0.716255, acc: 0.515625]  [A loss: 0.909734, acc: 0.101562]\n",
      "1509: [D loss: 0.688822, acc: 0.517578]  [A loss: 0.806752, acc: 0.285156]\n",
      "1510: [D loss: 0.692734, acc: 0.562500]  [A loss: 0.898725, acc: 0.175781]\n",
      "1511: [D loss: 0.679291, acc: 0.554688]  [A loss: 0.759535, acc: 0.347656]\n",
      "1512: [D loss: 0.699913, acc: 0.537109]  [A loss: 0.894632, acc: 0.117188]\n",
      "1513: [D loss: 0.679019, acc: 0.562500]  [A loss: 0.815651, acc: 0.222656]\n",
      "1514: [D loss: 0.681315, acc: 0.560547]  [A loss: 0.896830, acc: 0.125000]\n",
      "1515: [D loss: 0.683373, acc: 0.582031]  [A loss: 0.828924, acc: 0.214844]\n",
      "1516: [D loss: 0.706936, acc: 0.515625]  [A loss: 0.815570, acc: 0.273438]\n",
      "1517: [D loss: 0.707262, acc: 0.486328]  [A loss: 0.915953, acc: 0.097656]\n",
      "1518: [D loss: 0.679889, acc: 0.580078]  [A loss: 0.823359, acc: 0.238281]\n",
      "1519: [D loss: 0.681186, acc: 0.560547]  [A loss: 0.828475, acc: 0.210938]\n",
      "1520: [D loss: 0.692263, acc: 0.556641]  [A loss: 0.859429, acc: 0.191406]\n",
      "1521: [D loss: 0.672887, acc: 0.603516]  [A loss: 0.832193, acc: 0.234375]\n",
      "1522: [D loss: 0.692023, acc: 0.527344]  [A loss: 0.843905, acc: 0.207031]\n",
      "1523: [D loss: 0.693809, acc: 0.539062]  [A loss: 0.860724, acc: 0.199219]\n",
      "1524: [D loss: 0.685742, acc: 0.541016]  [A loss: 0.905582, acc: 0.144531]\n",
      "1525: [D loss: 0.701753, acc: 0.511719]  [A loss: 0.867531, acc: 0.121094]\n",
      "1526: [D loss: 0.689348, acc: 0.548828]  [A loss: 0.862630, acc: 0.175781]\n",
      "1527: [D loss: 0.683474, acc: 0.556641]  [A loss: 0.858653, acc: 0.179688]\n",
      "1528: [D loss: 0.717670, acc: 0.492188]  [A loss: 0.794692, acc: 0.292969]\n",
      "1529: [D loss: 0.737533, acc: 0.478516]  [A loss: 1.100576, acc: 0.015625]\n",
      "1530: [D loss: 0.700308, acc: 0.517578]  [A loss: 0.649520, acc: 0.613281]\n",
      "1531: [D loss: 0.739932, acc: 0.500000]  [A loss: 0.980024, acc: 0.058594]\n",
      "1532: [D loss: 0.689429, acc: 0.507812]  [A loss: 0.774879, acc: 0.281250]\n",
      "1533: [D loss: 0.690432, acc: 0.542969]  [A loss: 0.878850, acc: 0.132812]\n",
      "1534: [D loss: 0.701259, acc: 0.500000]  [A loss: 0.774341, acc: 0.257812]\n",
      "1535: [D loss: 0.693431, acc: 0.535156]  [A loss: 0.866272, acc: 0.175781]\n",
      "1536: [D loss: 0.679009, acc: 0.572266]  [A loss: 0.793396, acc: 0.246094]\n",
      "1537: [D loss: 0.685816, acc: 0.542969]  [A loss: 0.855702, acc: 0.207031]\n",
      "1538: [D loss: 0.686811, acc: 0.525391]  [A loss: 0.808718, acc: 0.281250]\n",
      "1539: [D loss: 0.698981, acc: 0.527344]  [A loss: 0.844738, acc: 0.183594]\n",
      "1540: [D loss: 0.689230, acc: 0.537109]  [A loss: 0.852034, acc: 0.160156]\n",
      "1541: [D loss: 0.694607, acc: 0.521484]  [A loss: 0.906847, acc: 0.109375]\n",
      "1542: [D loss: 0.685434, acc: 0.562500]  [A loss: 0.788571, acc: 0.289062]\n",
      "1543: [D loss: 0.696288, acc: 0.533203]  [A loss: 0.909747, acc: 0.121094]\n",
      "1544: [D loss: 0.686591, acc: 0.578125]  [A loss: 0.795634, acc: 0.308594]\n",
      "1545: [D loss: 0.704580, acc: 0.472656]  [A loss: 0.862455, acc: 0.136719]\n",
      "1546: [D loss: 0.683603, acc: 0.554688]  [A loss: 0.785825, acc: 0.304688]\n",
      "1547: [D loss: 0.687242, acc: 0.542969]  [A loss: 0.879636, acc: 0.167969]\n",
      "1548: [D loss: 0.693626, acc: 0.527344]  [A loss: 0.846058, acc: 0.207031]\n",
      "1549: [D loss: 0.681848, acc: 0.572266]  [A loss: 0.855308, acc: 0.179688]\n",
      "1550: [D loss: 0.691796, acc: 0.539062]  [A loss: 0.878675, acc: 0.144531]\n",
      "1551: [D loss: 0.678293, acc: 0.589844]  [A loss: 0.861140, acc: 0.167969]\n",
      "1552: [D loss: 0.692640, acc: 0.523438]  [A loss: 0.903425, acc: 0.105469]\n",
      "1553: [D loss: 0.691790, acc: 0.519531]  [A loss: 0.826121, acc: 0.253906]\n",
      "1554: [D loss: 0.689800, acc: 0.535156]  [A loss: 0.898726, acc: 0.121094]\n",
      "1555: [D loss: 0.697640, acc: 0.523438]  [A loss: 0.803692, acc: 0.281250]\n",
      "1556: [D loss: 0.701637, acc: 0.509766]  [A loss: 1.035447, acc: 0.035156]\n",
      "1557: [D loss: 0.697145, acc: 0.523438]  [A loss: 0.681614, acc: 0.550781]\n",
      "1558: [D loss: 0.713094, acc: 0.505859]  [A loss: 1.048076, acc: 0.062500]\n",
      "1559: [D loss: 0.700823, acc: 0.533203]  [A loss: 0.744403, acc: 0.398438]\n",
      "1560: [D loss: 0.702741, acc: 0.509766]  [A loss: 0.967510, acc: 0.054688]\n",
      "1561: [D loss: 0.676375, acc: 0.570312]  [A loss: 0.709233, acc: 0.457031]\n",
      "1562: [D loss: 0.709657, acc: 0.519531]  [A loss: 0.897022, acc: 0.109375]\n",
      "1563: [D loss: 0.683675, acc: 0.558594]  [A loss: 0.800145, acc: 0.250000]\n",
      "1564: [D loss: 0.688065, acc: 0.568359]  [A loss: 0.833785, acc: 0.191406]\n",
      "1565: [D loss: 0.692499, acc: 0.533203]  [A loss: 0.852386, acc: 0.210938]\n",
      "1566: [D loss: 0.693552, acc: 0.550781]  [A loss: 0.830377, acc: 0.238281]\n",
      "1567: [D loss: 0.685771, acc: 0.548828]  [A loss: 0.881052, acc: 0.175781]\n",
      "1568: [D loss: 0.705028, acc: 0.525391]  [A loss: 0.811247, acc: 0.218750]\n",
      "1569: [D loss: 0.686379, acc: 0.539062]  [A loss: 0.850171, acc: 0.183594]\n",
      "1570: [D loss: 0.696315, acc: 0.523438]  [A loss: 0.842300, acc: 0.144531]\n",
      "1571: [D loss: 0.689493, acc: 0.550781]  [A loss: 0.918097, acc: 0.101562]\n",
      "1572: [D loss: 0.683331, acc: 0.564453]  [A loss: 0.768726, acc: 0.339844]\n",
      "1573: [D loss: 0.696458, acc: 0.552734]  [A loss: 0.928206, acc: 0.074219]\n",
      "1574: [D loss: 0.703100, acc: 0.503906]  [A loss: 0.819375, acc: 0.265625]\n",
      "1575: [D loss: 0.684919, acc: 0.544922]  [A loss: 0.890764, acc: 0.121094]\n",
      "1576: [D loss: 0.691620, acc: 0.560547]  [A loss: 0.880886, acc: 0.144531]\n",
      "1577: [D loss: 0.691942, acc: 0.539062]  [A loss: 0.817132, acc: 0.273438]\n",
      "1578: [D loss: 0.693200, acc: 0.548828]  [A loss: 0.847881, acc: 0.160156]\n",
      "1579: [D loss: 0.706915, acc: 0.505859]  [A loss: 0.867005, acc: 0.144531]\n",
      "1580: [D loss: 0.685921, acc: 0.525391]  [A loss: 0.898323, acc: 0.164062]\n",
      "1581: [D loss: 0.697844, acc: 0.533203]  [A loss: 0.810733, acc: 0.226562]\n",
      "1582: [D loss: 0.695625, acc: 0.535156]  [A loss: 0.866519, acc: 0.164062]\n",
      "1583: [D loss: 0.698190, acc: 0.544922]  [A loss: 0.855553, acc: 0.175781]\n",
      "1584: [D loss: 0.678602, acc: 0.544922]  [A loss: 0.897304, acc: 0.117188]\n",
      "1585: [D loss: 0.687754, acc: 0.537109]  [A loss: 0.847424, acc: 0.214844]\n",
      "1586: [D loss: 0.709674, acc: 0.505859]  [A loss: 0.875448, acc: 0.167969]\n",
      "1587: [D loss: 0.701086, acc: 0.531250]  [A loss: 0.873116, acc: 0.152344]\n",
      "1588: [D loss: 0.688407, acc: 0.550781]  [A loss: 0.852714, acc: 0.179688]\n",
      "1589: [D loss: 0.691125, acc: 0.539062]  [A loss: 0.935621, acc: 0.093750]\n",
      "1590: [D loss: 0.686019, acc: 0.533203]  [A loss: 0.799107, acc: 0.265625]\n",
      "1591: [D loss: 0.696505, acc: 0.529297]  [A loss: 0.924841, acc: 0.082031]\n",
      "1592: [D loss: 0.680351, acc: 0.558594]  [A loss: 0.820631, acc: 0.234375]\n",
      "1593: [D loss: 0.689465, acc: 0.535156]  [A loss: 0.921316, acc: 0.097656]\n",
      "1594: [D loss: 0.683313, acc: 0.554688]  [A loss: 0.791418, acc: 0.265625]\n",
      "1595: [D loss: 0.686265, acc: 0.527344]  [A loss: 0.924481, acc: 0.128906]\n",
      "1596: [D loss: 0.695776, acc: 0.525391]  [A loss: 0.817287, acc: 0.238281]\n",
      "1597: [D loss: 0.682916, acc: 0.556641]  [A loss: 0.948619, acc: 0.078125]\n",
      "1598: [D loss: 0.686122, acc: 0.535156]  [A loss: 0.721633, acc: 0.480469]\n",
      "1599: [D loss: 0.693515, acc: 0.548828]  [A loss: 1.013116, acc: 0.078125]\n",
      "1600: [D loss: 0.680115, acc: 0.568359]  [A loss: 0.709777, acc: 0.460938]\n",
      "1601: [D loss: 0.748912, acc: 0.519531]  [A loss: 1.031893, acc: 0.046875]\n",
      "1602: [D loss: 0.700477, acc: 0.505859]  [A loss: 0.771634, acc: 0.339844]\n",
      "1603: [D loss: 0.719830, acc: 0.486328]  [A loss: 1.042792, acc: 0.039062]\n",
      "1604: [D loss: 0.693976, acc: 0.537109]  [A loss: 0.650409, acc: 0.667969]\n",
      "1605: [D loss: 0.726857, acc: 0.523438]  [A loss: 0.917932, acc: 0.082031]\n",
      "1606: [D loss: 0.693112, acc: 0.525391]  [A loss: 0.790177, acc: 0.269531]\n",
      "1607: [D loss: 0.688846, acc: 0.529297]  [A loss: 0.868125, acc: 0.132812]\n",
      "1608: [D loss: 0.692927, acc: 0.542969]  [A loss: 0.814761, acc: 0.226562]\n",
      "1609: [D loss: 0.696085, acc: 0.507812]  [A loss: 0.849313, acc: 0.183594]\n",
      "1610: [D loss: 0.684926, acc: 0.542969]  [A loss: 0.813898, acc: 0.265625]\n",
      "1611: [D loss: 0.692719, acc: 0.517578]  [A loss: 0.837906, acc: 0.226562]\n",
      "1612: [D loss: 0.683338, acc: 0.554688]  [A loss: 0.851853, acc: 0.164062]\n",
      "1613: [D loss: 0.692859, acc: 0.527344]  [A loss: 0.908581, acc: 0.125000]\n",
      "1614: [D loss: 0.687955, acc: 0.531250]  [A loss: 0.799506, acc: 0.292969]\n",
      "1615: [D loss: 0.695740, acc: 0.546875]  [A loss: 0.897002, acc: 0.144531]\n",
      "1616: [D loss: 0.697906, acc: 0.542969]  [A loss: 0.797134, acc: 0.246094]\n",
      "1617: [D loss: 0.683760, acc: 0.574219]  [A loss: 0.969591, acc: 0.074219]\n",
      "1618: [D loss: 0.684941, acc: 0.554688]  [A loss: 0.745866, acc: 0.398438]\n",
      "1619: [D loss: 0.703209, acc: 0.527344]  [A loss: 0.956312, acc: 0.054688]\n",
      "1620: [D loss: 0.688227, acc: 0.542969]  [A loss: 0.682845, acc: 0.527344]\n",
      "1621: [D loss: 0.702094, acc: 0.531250]  [A loss: 1.005914, acc: 0.062500]\n",
      "1622: [D loss: 0.683210, acc: 0.537109]  [A loss: 0.697209, acc: 0.527344]\n",
      "1623: [D loss: 0.692697, acc: 0.556641]  [A loss: 0.951119, acc: 0.093750]\n",
      "1624: [D loss: 0.685201, acc: 0.556641]  [A loss: 0.750652, acc: 0.394531]\n",
      "1625: [D loss: 0.704211, acc: 0.521484]  [A loss: 0.946936, acc: 0.074219]\n",
      "1626: [D loss: 0.682324, acc: 0.583984]  [A loss: 0.744046, acc: 0.410156]\n",
      "1627: [D loss: 0.706419, acc: 0.535156]  [A loss: 0.943150, acc: 0.105469]\n",
      "1628: [D loss: 0.687284, acc: 0.554688]  [A loss: 0.808555, acc: 0.253906]\n",
      "1629: [D loss: 0.684180, acc: 0.574219]  [A loss: 0.785733, acc: 0.273438]\n",
      "1630: [D loss: 0.699143, acc: 0.517578]  [A loss: 0.900373, acc: 0.167969]\n",
      "1631: [D loss: 0.682517, acc: 0.580078]  [A loss: 0.737893, acc: 0.414062]\n",
      "1632: [D loss: 0.718442, acc: 0.511719]  [A loss: 0.960320, acc: 0.101562]\n",
      "1633: [D loss: 0.695046, acc: 0.531250]  [A loss: 0.748127, acc: 0.394531]\n",
      "1634: [D loss: 0.718461, acc: 0.509766]  [A loss: 0.899193, acc: 0.109375]\n",
      "1635: [D loss: 0.671521, acc: 0.583984]  [A loss: 0.789720, acc: 0.320312]\n",
      "1636: [D loss: 0.706585, acc: 0.509766]  [A loss: 0.908370, acc: 0.132812]\n",
      "1637: [D loss: 0.677159, acc: 0.562500]  [A loss: 0.767502, acc: 0.324219]\n",
      "1638: [D loss: 0.687188, acc: 0.552734]  [A loss: 0.929875, acc: 0.089844]\n",
      "1639: [D loss: 0.686864, acc: 0.560547]  [A loss: 0.748971, acc: 0.375000]\n",
      "1640: [D loss: 0.697382, acc: 0.550781]  [A loss: 0.853109, acc: 0.136719]\n",
      "1641: [D loss: 0.688254, acc: 0.548828]  [A loss: 0.858068, acc: 0.164062]\n",
      "1642: [D loss: 0.692603, acc: 0.519531]  [A loss: 0.819165, acc: 0.226562]\n",
      "1643: [D loss: 0.695604, acc: 0.529297]  [A loss: 0.876452, acc: 0.148438]\n",
      "1644: [D loss: 0.688080, acc: 0.550781]  [A loss: 0.786592, acc: 0.265625]\n",
      "1645: [D loss: 0.702064, acc: 0.544922]  [A loss: 0.872705, acc: 0.152344]\n",
      "1646: [D loss: 0.689474, acc: 0.564453]  [A loss: 0.806641, acc: 0.250000]\n",
      "1647: [D loss: 0.690114, acc: 0.541016]  [A loss: 0.871714, acc: 0.164062]\n",
      "1648: [D loss: 0.682899, acc: 0.576172]  [A loss: 0.797632, acc: 0.242188]\n",
      "1649: [D loss: 0.692900, acc: 0.550781]  [A loss: 0.886284, acc: 0.121094]\n",
      "1650: [D loss: 0.683350, acc: 0.576172]  [A loss: 0.787633, acc: 0.312500]\n",
      "1651: [D loss: 0.684613, acc: 0.552734]  [A loss: 0.923458, acc: 0.164062]\n",
      "1652: [D loss: 0.698956, acc: 0.533203]  [A loss: 0.859096, acc: 0.164062]\n",
      "1653: [D loss: 0.700100, acc: 0.542969]  [A loss: 0.850946, acc: 0.160156]\n",
      "1654: [D loss: 0.689452, acc: 0.539062]  [A loss: 0.829423, acc: 0.222656]\n",
      "1655: [D loss: 0.696219, acc: 0.515625]  [A loss: 0.831345, acc: 0.164062]\n",
      "1656: [D loss: 0.697861, acc: 0.533203]  [A loss: 0.832331, acc: 0.179688]\n",
      "1657: [D loss: 0.698186, acc: 0.544922]  [A loss: 0.924288, acc: 0.117188]\n",
      "1658: [D loss: 0.695568, acc: 0.542969]  [A loss: 0.769921, acc: 0.343750]\n",
      "1659: [D loss: 0.706694, acc: 0.542969]  [A loss: 1.009442, acc: 0.019531]\n",
      "1660: [D loss: 0.698632, acc: 0.521484]  [A loss: 0.729413, acc: 0.441406]\n",
      "1661: [D loss: 0.698679, acc: 0.527344]  [A loss: 0.989515, acc: 0.066406]\n",
      "1662: [D loss: 0.704132, acc: 0.509766]  [A loss: 0.718512, acc: 0.503906]\n",
      "1663: [D loss: 0.729057, acc: 0.503906]  [A loss: 1.011455, acc: 0.023438]\n",
      "1664: [D loss: 0.678367, acc: 0.599609]  [A loss: 0.640735, acc: 0.636719]\n",
      "1665: [D loss: 0.740339, acc: 0.484375]  [A loss: 0.930893, acc: 0.074219]\n",
      "1666: [D loss: 0.694020, acc: 0.544922]  [A loss: 0.762307, acc: 0.339844]\n",
      "1667: [D loss: 0.700390, acc: 0.515625]  [A loss: 0.920789, acc: 0.070312]\n",
      "1668: [D loss: 0.688801, acc: 0.523438]  [A loss: 0.778782, acc: 0.273438]\n",
      "1669: [D loss: 0.700044, acc: 0.521484]  [A loss: 0.873212, acc: 0.125000]\n",
      "1670: [D loss: 0.677563, acc: 0.576172]  [A loss: 0.793772, acc: 0.265625]\n",
      "1671: [D loss: 0.690259, acc: 0.552734]  [A loss: 0.851186, acc: 0.191406]\n",
      "1672: [D loss: 0.694063, acc: 0.546875]  [A loss: 0.861069, acc: 0.160156]\n",
      "1673: [D loss: 0.690696, acc: 0.521484]  [A loss: 0.863338, acc: 0.164062]\n",
      "1674: [D loss: 0.681944, acc: 0.556641]  [A loss: 0.813112, acc: 0.277344]\n",
      "1675: [D loss: 0.703919, acc: 0.525391]  [A loss: 0.971887, acc: 0.058594]\n",
      "1676: [D loss: 0.680761, acc: 0.570312]  [A loss: 0.718171, acc: 0.472656]\n",
      "1677: [D loss: 0.726832, acc: 0.488281]  [A loss: 0.930996, acc: 0.089844]\n",
      "1678: [D loss: 0.692480, acc: 0.533203]  [A loss: 0.766144, acc: 0.339844]\n",
      "1679: [D loss: 0.695017, acc: 0.546875]  [A loss: 0.957476, acc: 0.105469]\n",
      "1680: [D loss: 0.692171, acc: 0.531250]  [A loss: 0.751278, acc: 0.359375]\n",
      "1681: [D loss: 0.700535, acc: 0.527344]  [A loss: 0.958436, acc: 0.085938]\n",
      "1682: [D loss: 0.683764, acc: 0.544922]  [A loss: 0.725594, acc: 0.433594]\n",
      "1683: [D loss: 0.716487, acc: 0.496094]  [A loss: 0.964820, acc: 0.050781]\n",
      "1684: [D loss: 0.694207, acc: 0.531250]  [A loss: 0.693972, acc: 0.535156]\n",
      "1685: [D loss: 0.710891, acc: 0.500000]  [A loss: 1.007486, acc: 0.054688]\n",
      "1686: [D loss: 0.691313, acc: 0.531250]  [A loss: 0.731348, acc: 0.441406]\n",
      "1687: [D loss: 0.707839, acc: 0.527344]  [A loss: 0.904211, acc: 0.074219]\n",
      "1688: [D loss: 0.691193, acc: 0.531250]  [A loss: 0.746938, acc: 0.359375]\n",
      "1689: [D loss: 0.685861, acc: 0.564453]  [A loss: 0.859969, acc: 0.160156]\n",
      "1690: [D loss: 0.678431, acc: 0.554688]  [A loss: 0.813158, acc: 0.234375]\n",
      "1691: [D loss: 0.689412, acc: 0.548828]  [A loss: 0.882567, acc: 0.132812]\n",
      "1692: [D loss: 0.695732, acc: 0.527344]  [A loss: 0.741142, acc: 0.410156]\n",
      "1693: [D loss: 0.704548, acc: 0.509766]  [A loss: 0.901116, acc: 0.105469]\n",
      "1694: [D loss: 0.692909, acc: 0.542969]  [A loss: 0.822663, acc: 0.207031]\n",
      "1695: [D loss: 0.687803, acc: 0.548828]  [A loss: 0.897331, acc: 0.125000]\n",
      "1696: [D loss: 0.699461, acc: 0.531250]  [A loss: 0.783736, acc: 0.292969]\n",
      "1697: [D loss: 0.697009, acc: 0.523438]  [A loss: 0.851060, acc: 0.199219]\n",
      "1698: [D loss: 0.677183, acc: 0.576172]  [A loss: 0.876967, acc: 0.144531]\n",
      "1699: [D loss: 0.677164, acc: 0.583984]  [A loss: 0.819197, acc: 0.234375]\n",
      "1700: [D loss: 0.689033, acc: 0.548828]  [A loss: 0.920869, acc: 0.105469]\n",
      "1701: [D loss: 0.679793, acc: 0.583984]  [A loss: 0.780732, acc: 0.289062]\n",
      "1702: [D loss: 0.691082, acc: 0.546875]  [A loss: 0.975542, acc: 0.093750]\n",
      "1703: [D loss: 0.689984, acc: 0.519531]  [A loss: 0.715539, acc: 0.421875]\n",
      "1704: [D loss: 0.704439, acc: 0.517578]  [A loss: 0.998380, acc: 0.070312]\n",
      "1705: [D loss: 0.690443, acc: 0.539062]  [A loss: 0.681389, acc: 0.546875]\n",
      "1706: [D loss: 0.714280, acc: 0.529297]  [A loss: 0.941223, acc: 0.117188]\n",
      "1707: [D loss: 0.688050, acc: 0.542969]  [A loss: 0.686980, acc: 0.570312]\n",
      "1708: [D loss: 0.706825, acc: 0.529297]  [A loss: 0.928881, acc: 0.082031]\n",
      "1709: [D loss: 0.681266, acc: 0.558594]  [A loss: 0.779048, acc: 0.316406]\n",
      "1710: [D loss: 0.691520, acc: 0.535156]  [A loss: 0.926398, acc: 0.097656]\n",
      "1711: [D loss: 0.695835, acc: 0.542969]  [A loss: 0.737462, acc: 0.394531]\n",
      "1712: [D loss: 0.702518, acc: 0.490234]  [A loss: 0.913142, acc: 0.136719]\n",
      "1713: [D loss: 0.670673, acc: 0.583984]  [A loss: 0.768420, acc: 0.355469]\n",
      "1714: [D loss: 0.703525, acc: 0.517578]  [A loss: 0.901707, acc: 0.144531]\n",
      "1715: [D loss: 0.695471, acc: 0.531250]  [A loss: 0.791475, acc: 0.273438]\n",
      "1716: [D loss: 0.698014, acc: 0.523438]  [A loss: 0.887789, acc: 0.136719]\n",
      "1717: [D loss: 0.695489, acc: 0.523438]  [A loss: 0.791991, acc: 0.285156]\n",
      "1718: [D loss: 0.683564, acc: 0.544922]  [A loss: 0.880987, acc: 0.148438]\n",
      "1719: [D loss: 0.696360, acc: 0.541016]  [A loss: 0.753963, acc: 0.375000]\n",
      "1720: [D loss: 0.706654, acc: 0.521484]  [A loss: 0.923234, acc: 0.136719]\n",
      "1721: [D loss: 0.690023, acc: 0.556641]  [A loss: 0.821525, acc: 0.242188]\n",
      "1722: [D loss: 0.694535, acc: 0.542969]  [A loss: 0.860499, acc: 0.164062]\n",
      "1723: [D loss: 0.690229, acc: 0.525391]  [A loss: 0.780116, acc: 0.316406]\n",
      "1724: [D loss: 0.692798, acc: 0.533203]  [A loss: 0.929122, acc: 0.113281]\n",
      "1725: [D loss: 0.683121, acc: 0.570312]  [A loss: 0.766847, acc: 0.332031]\n",
      "1726: [D loss: 0.693228, acc: 0.542969]  [A loss: 0.856400, acc: 0.160156]\n",
      "1727: [D loss: 0.688498, acc: 0.556641]  [A loss: 0.830954, acc: 0.234375]\n",
      "1728: [D loss: 0.692531, acc: 0.525391]  [A loss: 0.884747, acc: 0.136719]\n",
      "1729: [D loss: 0.689962, acc: 0.539062]  [A loss: 0.891980, acc: 0.148438]\n",
      "1730: [D loss: 0.679481, acc: 0.552734]  [A loss: 0.802890, acc: 0.277344]\n",
      "1731: [D loss: 0.710090, acc: 0.511719]  [A loss: 0.834540, acc: 0.187500]\n",
      "1732: [D loss: 0.693988, acc: 0.556641]  [A loss: 1.003158, acc: 0.058594]\n",
      "1733: [D loss: 0.696600, acc: 0.544922]  [A loss: 0.710560, acc: 0.492188]\n",
      "1734: [D loss: 0.703418, acc: 0.529297]  [A loss: 1.019444, acc: 0.039062]\n",
      "1735: [D loss: 0.679870, acc: 0.542969]  [A loss: 0.683249, acc: 0.535156]\n",
      "1736: [D loss: 0.707837, acc: 0.527344]  [A loss: 0.955915, acc: 0.070312]\n",
      "1737: [D loss: 0.684913, acc: 0.560547]  [A loss: 0.725674, acc: 0.429688]\n",
      "1738: [D loss: 0.704779, acc: 0.515625]  [A loss: 0.957043, acc: 0.101562]\n",
      "1739: [D loss: 0.682087, acc: 0.548828]  [A loss: 0.767061, acc: 0.335938]\n",
      "1740: [D loss: 0.706949, acc: 0.525391]  [A loss: 0.881165, acc: 0.167969]\n",
      "1741: [D loss: 0.687378, acc: 0.546875]  [A loss: 0.825224, acc: 0.230469]\n",
      "1742: [D loss: 0.688441, acc: 0.535156]  [A loss: 0.863603, acc: 0.179688]\n",
      "1743: [D loss: 0.683101, acc: 0.582031]  [A loss: 0.770640, acc: 0.308594]\n",
      "1744: [D loss: 0.718888, acc: 0.509766]  [A loss: 0.837989, acc: 0.214844]\n",
      "1745: [D loss: 0.701066, acc: 0.535156]  [A loss: 0.917182, acc: 0.097656]\n",
      "1746: [D loss: 0.696391, acc: 0.550781]  [A loss: 0.751426, acc: 0.382812]\n",
      "1747: [D loss: 0.697301, acc: 0.511719]  [A loss: 0.896176, acc: 0.125000]\n",
      "1748: [D loss: 0.686154, acc: 0.544922]  [A loss: 0.784399, acc: 0.332031]\n",
      "1749: [D loss: 0.691669, acc: 0.544922]  [A loss: 0.883020, acc: 0.113281]\n",
      "1750: [D loss: 0.679185, acc: 0.576172]  [A loss: 0.839745, acc: 0.242188]\n",
      "1751: [D loss: 0.691993, acc: 0.533203]  [A loss: 0.890594, acc: 0.128906]\n",
      "1752: [D loss: 0.692044, acc: 0.548828]  [A loss: 0.798160, acc: 0.277344]\n",
      "1753: [D loss: 0.693701, acc: 0.539062]  [A loss: 0.876797, acc: 0.152344]\n",
      "1754: [D loss: 0.688631, acc: 0.564453]  [A loss: 0.834184, acc: 0.191406]\n",
      "1755: [D loss: 0.691627, acc: 0.554688]  [A loss: 0.843731, acc: 0.175781]\n",
      "1756: [D loss: 0.697007, acc: 0.539062]  [A loss: 0.923490, acc: 0.109375]\n",
      "1757: [D loss: 0.693938, acc: 0.519531]  [A loss: 0.733224, acc: 0.429688]\n",
      "1758: [D loss: 0.701092, acc: 0.525391]  [A loss: 0.986682, acc: 0.058594]\n",
      "1759: [D loss: 0.690723, acc: 0.554688]  [A loss: 0.622649, acc: 0.699219]\n",
      "1760: [D loss: 0.744852, acc: 0.505859]  [A loss: 1.085547, acc: 0.050781]\n",
      "1761: [D loss: 0.681494, acc: 0.556641]  [A loss: 0.658673, acc: 0.574219]\n",
      "1762: [D loss: 0.776694, acc: 0.468750]  [A loss: 0.834203, acc: 0.210938]\n",
      "1763: [D loss: 0.705272, acc: 0.507812]  [A loss: 0.937411, acc: 0.070312]\n",
      "1764: [D loss: 0.693164, acc: 0.515625]  [A loss: 0.735795, acc: 0.410156]\n",
      "1765: [D loss: 0.718805, acc: 0.509766]  [A loss: 0.863041, acc: 0.203125]\n",
      "1766: [D loss: 0.685725, acc: 0.548828]  [A loss: 0.753703, acc: 0.406250]\n",
      "1767: [D loss: 0.707319, acc: 0.515625]  [A loss: 0.916862, acc: 0.097656]\n",
      "1768: [D loss: 0.704707, acc: 0.498047]  [A loss: 0.809121, acc: 0.242188]\n",
      "1769: [D loss: 0.707111, acc: 0.480469]  [A loss: 0.851721, acc: 0.148438]\n",
      "1770: [D loss: 0.689493, acc: 0.517578]  [A loss: 0.835268, acc: 0.167969]\n",
      "1771: [D loss: 0.691587, acc: 0.564453]  [A loss: 0.827011, acc: 0.175781]\n",
      "1772: [D loss: 0.707301, acc: 0.517578]  [A loss: 0.823090, acc: 0.199219]\n",
      "1773: [D loss: 0.680883, acc: 0.583984]  [A loss: 0.777162, acc: 0.281250]\n",
      "1774: [D loss: 0.687709, acc: 0.564453]  [A loss: 0.842874, acc: 0.199219]\n",
      "1775: [D loss: 0.685168, acc: 0.568359]  [A loss: 0.787076, acc: 0.285156]\n",
      "1776: [D loss: 0.697535, acc: 0.548828]  [A loss: 0.889363, acc: 0.152344]\n",
      "1777: [D loss: 0.700337, acc: 0.533203]  [A loss: 0.795951, acc: 0.250000]\n",
      "1778: [D loss: 0.691864, acc: 0.517578]  [A loss: 0.831523, acc: 0.187500]\n",
      "1779: [D loss: 0.689828, acc: 0.539062]  [A loss: 0.857872, acc: 0.167969]\n",
      "1780: [D loss: 0.699205, acc: 0.560547]  [A loss: 0.833309, acc: 0.195312]\n",
      "1781: [D loss: 0.690893, acc: 0.542969]  [A loss: 0.861402, acc: 0.171875]\n",
      "1782: [D loss: 0.691663, acc: 0.537109]  [A loss: 0.810660, acc: 0.273438]\n",
      "1783: [D loss: 0.719324, acc: 0.500000]  [A loss: 0.910780, acc: 0.109375]\n",
      "1784: [D loss: 0.704238, acc: 0.527344]  [A loss: 0.846053, acc: 0.191406]\n",
      "1785: [D loss: 0.689139, acc: 0.539062]  [A loss: 0.833452, acc: 0.203125]\n",
      "1786: [D loss: 0.699822, acc: 0.521484]  [A loss: 0.857325, acc: 0.160156]\n",
      "1787: [D loss: 0.692629, acc: 0.562500]  [A loss: 0.886461, acc: 0.152344]\n",
      "1788: [D loss: 0.690745, acc: 0.554688]  [A loss: 0.805233, acc: 0.257812]\n",
      "1789: [D loss: 0.687554, acc: 0.560547]  [A loss: 0.948471, acc: 0.093750]\n",
      "1790: [D loss: 0.675108, acc: 0.574219]  [A loss: 0.743787, acc: 0.414062]\n",
      "1791: [D loss: 0.695473, acc: 0.535156]  [A loss: 0.883373, acc: 0.136719]\n",
      "1792: [D loss: 0.673862, acc: 0.570312]  [A loss: 0.831337, acc: 0.269531]\n",
      "1793: [D loss: 0.715763, acc: 0.472656]  [A loss: 0.927399, acc: 0.105469]\n",
      "1794: [D loss: 0.696202, acc: 0.537109]  [A loss: 0.732262, acc: 0.449219]\n",
      "1795: [D loss: 0.713804, acc: 0.546875]  [A loss: 1.009324, acc: 0.042969]\n",
      "1796: [D loss: 0.693460, acc: 0.556641]  [A loss: 0.642308, acc: 0.652344]\n",
      "1797: [D loss: 0.731084, acc: 0.490234]  [A loss: 1.027778, acc: 0.062500]\n",
      "1798: [D loss: 0.694151, acc: 0.505859]  [A loss: 0.726519, acc: 0.421875]\n",
      "1799: [D loss: 0.701829, acc: 0.533203]  [A loss: 0.846111, acc: 0.214844]\n",
      "1800: [D loss: 0.687818, acc: 0.572266]  [A loss: 0.823002, acc: 0.250000]\n",
      "1801: [D loss: 0.706968, acc: 0.496094]  [A loss: 0.818153, acc: 0.230469]\n",
      "1802: [D loss: 0.696346, acc: 0.544922]  [A loss: 0.848198, acc: 0.160156]\n",
      "1803: [D loss: 0.694574, acc: 0.537109]  [A loss: 0.866030, acc: 0.148438]\n",
      "1804: [D loss: 0.699656, acc: 0.546875]  [A loss: 0.834438, acc: 0.218750]\n",
      "1805: [D loss: 0.677469, acc: 0.554688]  [A loss: 0.850951, acc: 0.199219]\n",
      "1806: [D loss: 0.697916, acc: 0.507812]  [A loss: 0.844331, acc: 0.179688]\n",
      "1807: [D loss: 0.695833, acc: 0.539062]  [A loss: 0.805123, acc: 0.238281]\n",
      "1808: [D loss: 0.694450, acc: 0.519531]  [A loss: 0.900561, acc: 0.132812]\n",
      "1809: [D loss: 0.682468, acc: 0.558594]  [A loss: 0.776907, acc: 0.308594]\n",
      "1810: [D loss: 0.703238, acc: 0.546875]  [A loss: 0.996912, acc: 0.062500]\n",
      "1811: [D loss: 0.679350, acc: 0.576172]  [A loss: 0.648190, acc: 0.640625]\n",
      "1812: [D loss: 0.719081, acc: 0.505859]  [A loss: 1.068539, acc: 0.027344]\n",
      "1813: [D loss: 0.706969, acc: 0.505859]  [A loss: 0.726810, acc: 0.457031]\n",
      "1814: [D loss: 0.709703, acc: 0.511719]  [A loss: 0.950352, acc: 0.097656]\n",
      "1815: [D loss: 0.689656, acc: 0.562500]  [A loss: 0.722267, acc: 0.441406]\n",
      "1816: [D loss: 0.704470, acc: 0.542969]  [A loss: 0.869318, acc: 0.148438]\n",
      "1817: [D loss: 0.690924, acc: 0.558594]  [A loss: 0.801893, acc: 0.261719]\n",
      "1818: [D loss: 0.696640, acc: 0.521484]  [A loss: 0.838197, acc: 0.214844]\n",
      "1819: [D loss: 0.692351, acc: 0.521484]  [A loss: 0.827788, acc: 0.203125]\n",
      "1820: [D loss: 0.690463, acc: 0.537109]  [A loss: 0.862803, acc: 0.187500]\n",
      "1821: [D loss: 0.687134, acc: 0.552734]  [A loss: 0.827960, acc: 0.214844]\n",
      "1822: [D loss: 0.677947, acc: 0.542969]  [A loss: 0.819332, acc: 0.218750]\n",
      "1823: [D loss: 0.684484, acc: 0.558594]  [A loss: 0.854648, acc: 0.199219]\n",
      "1824: [D loss: 0.674417, acc: 0.541016]  [A loss: 0.823202, acc: 0.253906]\n",
      "1825: [D loss: 0.703362, acc: 0.521484]  [A loss: 0.890478, acc: 0.167969]\n",
      "1826: [D loss: 0.683340, acc: 0.564453]  [A loss: 0.794107, acc: 0.308594]\n",
      "1827: [D loss: 0.699972, acc: 0.521484]  [A loss: 0.964478, acc: 0.117188]\n",
      "1828: [D loss: 0.682337, acc: 0.539062]  [A loss: 0.714300, acc: 0.453125]\n",
      "1829: [D loss: 0.703794, acc: 0.535156]  [A loss: 0.935492, acc: 0.101562]\n",
      "1830: [D loss: 0.670445, acc: 0.619141]  [A loss: 0.750434, acc: 0.375000]\n",
      "1831: [D loss: 0.697562, acc: 0.552734]  [A loss: 0.968189, acc: 0.117188]\n",
      "1832: [D loss: 0.696088, acc: 0.535156]  [A loss: 0.789720, acc: 0.281250]\n",
      "1833: [D loss: 0.691337, acc: 0.572266]  [A loss: 0.922970, acc: 0.085938]\n",
      "1834: [D loss: 0.693038, acc: 0.546875]  [A loss: 0.755969, acc: 0.382812]\n",
      "1835: [D loss: 0.695134, acc: 0.533203]  [A loss: 0.917340, acc: 0.085938]\n",
      "1836: [D loss: 0.691543, acc: 0.527344]  [A loss: 0.790054, acc: 0.332031]\n",
      "1837: [D loss: 0.703777, acc: 0.535156]  [A loss: 0.926785, acc: 0.078125]\n",
      "1838: [D loss: 0.695933, acc: 0.517578]  [A loss: 0.788116, acc: 0.308594]\n",
      "1839: [D loss: 0.693191, acc: 0.558594]  [A loss: 0.909683, acc: 0.113281]\n",
      "1840: [D loss: 0.696053, acc: 0.542969]  [A loss: 0.701591, acc: 0.464844]\n",
      "1841: [D loss: 0.725825, acc: 0.507812]  [A loss: 0.976399, acc: 0.054688]\n",
      "1842: [D loss: 0.700043, acc: 0.539062]  [A loss: 0.765336, acc: 0.371094]\n",
      "1843: [D loss: 0.690554, acc: 0.541016]  [A loss: 0.918343, acc: 0.093750]\n",
      "1844: [D loss: 0.682064, acc: 0.568359]  [A loss: 0.698319, acc: 0.511719]\n",
      "1845: [D loss: 0.714685, acc: 0.531250]  [A loss: 0.987198, acc: 0.074219]\n",
      "1846: [D loss: 0.693643, acc: 0.544922]  [A loss: 0.743757, acc: 0.417969]\n",
      "1847: [D loss: 0.703680, acc: 0.511719]  [A loss: 0.906531, acc: 0.101562]\n",
      "1848: [D loss: 0.697134, acc: 0.523438]  [A loss: 0.784818, acc: 0.300781]\n",
      "1849: [D loss: 0.712720, acc: 0.513672]  [A loss: 0.950845, acc: 0.089844]\n",
      "1850: [D loss: 0.689598, acc: 0.566406]  [A loss: 0.704045, acc: 0.531250]\n",
      "1851: [D loss: 0.708247, acc: 0.523438]  [A loss: 0.933692, acc: 0.097656]\n",
      "1852: [D loss: 0.691595, acc: 0.517578]  [A loss: 0.738519, acc: 0.429688]\n",
      "1853: [D loss: 0.708816, acc: 0.507812]  [A loss: 0.924814, acc: 0.125000]\n",
      "1854: [D loss: 0.697803, acc: 0.531250]  [A loss: 0.761327, acc: 0.359375]\n",
      "1855: [D loss: 0.699299, acc: 0.521484]  [A loss: 0.872169, acc: 0.179688]\n",
      "1856: [D loss: 0.693114, acc: 0.554688]  [A loss: 0.783619, acc: 0.308594]\n",
      "1857: [D loss: 0.699508, acc: 0.533203]  [A loss: 0.869736, acc: 0.167969]\n",
      "1858: [D loss: 0.695680, acc: 0.519531]  [A loss: 0.734191, acc: 0.410156]\n",
      "1859: [D loss: 0.699769, acc: 0.529297]  [A loss: 0.942549, acc: 0.148438]\n",
      "1860: [D loss: 0.699243, acc: 0.535156]  [A loss: 0.746428, acc: 0.406250]\n",
      "1861: [D loss: 0.700235, acc: 0.515625]  [A loss: 0.907046, acc: 0.085938]\n",
      "1862: [D loss: 0.684712, acc: 0.574219]  [A loss: 0.748673, acc: 0.355469]\n",
      "1863: [D loss: 0.699678, acc: 0.531250]  [A loss: 0.923173, acc: 0.085938]\n",
      "1864: [D loss: 0.686425, acc: 0.542969]  [A loss: 0.774110, acc: 0.335938]\n",
      "1865: [D loss: 0.695458, acc: 0.544922]  [A loss: 0.939226, acc: 0.121094]\n",
      "1866: [D loss: 0.678166, acc: 0.572266]  [A loss: 0.778171, acc: 0.332031]\n",
      "1867: [D loss: 0.698353, acc: 0.564453]  [A loss: 0.971872, acc: 0.085938]\n",
      "1868: [D loss: 0.696913, acc: 0.537109]  [A loss: 0.739703, acc: 0.398438]\n",
      "1869: [D loss: 0.690467, acc: 0.535156]  [A loss: 0.968265, acc: 0.062500]\n",
      "1870: [D loss: 0.689614, acc: 0.554688]  [A loss: 0.772721, acc: 0.347656]\n",
      "1871: [D loss: 0.684815, acc: 0.550781]  [A loss: 0.940169, acc: 0.136719]\n",
      "1872: [D loss: 0.700134, acc: 0.525391]  [A loss: 0.720460, acc: 0.460938]\n",
      "1873: [D loss: 0.711589, acc: 0.500000]  [A loss: 0.908589, acc: 0.105469]\n",
      "1874: [D loss: 0.687423, acc: 0.560547]  [A loss: 0.760733, acc: 0.375000]\n",
      "1875: [D loss: 0.702844, acc: 0.515625]  [A loss: 0.953352, acc: 0.101562]\n",
      "1876: [D loss: 0.687102, acc: 0.535156]  [A loss: 0.754339, acc: 0.394531]\n",
      "1877: [D loss: 0.707397, acc: 0.523438]  [A loss: 0.949049, acc: 0.085938]\n",
      "1878: [D loss: 0.690153, acc: 0.542969]  [A loss: 0.753760, acc: 0.347656]\n",
      "1879: [D loss: 0.703652, acc: 0.544922]  [A loss: 0.897249, acc: 0.152344]\n",
      "1880: [D loss: 0.693197, acc: 0.541016]  [A loss: 0.786825, acc: 0.292969]\n",
      "1881: [D loss: 0.696538, acc: 0.552734]  [A loss: 0.914490, acc: 0.101562]\n",
      "1882: [D loss: 0.678861, acc: 0.597656]  [A loss: 0.774398, acc: 0.312500]\n",
      "1883: [D loss: 0.700188, acc: 0.519531]  [A loss: 0.830435, acc: 0.214844]\n",
      "1884: [D loss: 0.699197, acc: 0.517578]  [A loss: 0.867024, acc: 0.140625]\n",
      "1885: [D loss: 0.694319, acc: 0.544922]  [A loss: 0.850279, acc: 0.183594]\n",
      "1886: [D loss: 0.688617, acc: 0.519531]  [A loss: 0.797042, acc: 0.308594]\n",
      "1887: [D loss: 0.694120, acc: 0.544922]  [A loss: 0.839535, acc: 0.214844]\n",
      "1888: [D loss: 0.687833, acc: 0.560547]  [A loss: 0.765849, acc: 0.332031]\n",
      "1889: [D loss: 0.702237, acc: 0.544922]  [A loss: 0.951353, acc: 0.097656]\n",
      "1890: [D loss: 0.674804, acc: 0.580078]  [A loss: 0.754344, acc: 0.363281]\n",
      "1891: [D loss: 0.711871, acc: 0.503906]  [A loss: 0.935354, acc: 0.109375]\n",
      "1892: [D loss: 0.687354, acc: 0.554688]  [A loss: 0.775694, acc: 0.343750]\n",
      "1893: [D loss: 0.703879, acc: 0.513672]  [A loss: 1.039477, acc: 0.035156]\n",
      "1894: [D loss: 0.695516, acc: 0.542969]  [A loss: 0.673000, acc: 0.582031]\n",
      "1895: [D loss: 0.722864, acc: 0.505859]  [A loss: 0.985127, acc: 0.085938]\n",
      "1896: [D loss: 0.677613, acc: 0.564453]  [A loss: 0.714737, acc: 0.488281]\n",
      "1897: [D loss: 0.693174, acc: 0.542969]  [A loss: 0.875995, acc: 0.144531]\n",
      "1898: [D loss: 0.684457, acc: 0.574219]  [A loss: 0.798898, acc: 0.273438]\n",
      "1899: [D loss: 0.701638, acc: 0.537109]  [A loss: 0.850469, acc: 0.195312]\n",
      "1900: [D loss: 0.694435, acc: 0.541016]  [A loss: 0.816016, acc: 0.253906]\n",
      "1901: [D loss: 0.700626, acc: 0.513672]  [A loss: 0.883929, acc: 0.148438]\n",
      "1902: [D loss: 0.688114, acc: 0.533203]  [A loss: 0.764063, acc: 0.382812]\n",
      "1903: [D loss: 0.689875, acc: 0.525391]  [A loss: 0.899513, acc: 0.136719]\n",
      "1904: [D loss: 0.688471, acc: 0.548828]  [A loss: 0.750153, acc: 0.406250]\n",
      "1905: [D loss: 0.698514, acc: 0.537109]  [A loss: 0.920049, acc: 0.148438]\n",
      "1906: [D loss: 0.693749, acc: 0.537109]  [A loss: 0.751272, acc: 0.355469]\n",
      "1907: [D loss: 0.716655, acc: 0.484375]  [A loss: 1.068979, acc: 0.031250]\n",
      "1908: [D loss: 0.702170, acc: 0.511719]  [A loss: 0.608723, acc: 0.742188]\n",
      "1909: [D loss: 0.734850, acc: 0.503906]  [A loss: 0.966791, acc: 0.093750]\n",
      "1910: [D loss: 0.699803, acc: 0.523438]  [A loss: 0.761666, acc: 0.339844]\n",
      "1911: [D loss: 0.716138, acc: 0.521484]  [A loss: 0.861138, acc: 0.207031]\n",
      "1912: [D loss: 0.696491, acc: 0.541016]  [A loss: 0.791017, acc: 0.273438]\n",
      "1913: [D loss: 0.693773, acc: 0.525391]  [A loss: 0.862971, acc: 0.214844]\n",
      "1914: [D loss: 0.692231, acc: 0.517578]  [A loss: 0.770692, acc: 0.328125]\n",
      "1915: [D loss: 0.700626, acc: 0.519531]  [A loss: 0.910257, acc: 0.113281]\n",
      "1916: [D loss: 0.688504, acc: 0.574219]  [A loss: 0.760336, acc: 0.398438]\n",
      "1917: [D loss: 0.692496, acc: 0.539062]  [A loss: 0.844000, acc: 0.171875]\n",
      "1918: [D loss: 0.685338, acc: 0.533203]  [A loss: 0.809252, acc: 0.253906]\n",
      "1919: [D loss: 0.693276, acc: 0.568359]  [A loss: 0.867720, acc: 0.148438]\n",
      "1920: [D loss: 0.681275, acc: 0.554688]  [A loss: 0.827189, acc: 0.230469]\n",
      "1921: [D loss: 0.702773, acc: 0.507812]  [A loss: 0.843170, acc: 0.179688]\n",
      "1922: [D loss: 0.699947, acc: 0.503906]  [A loss: 0.879604, acc: 0.152344]\n",
      "1923: [D loss: 0.700589, acc: 0.531250]  [A loss: 0.799203, acc: 0.277344]\n",
      "1924: [D loss: 0.701635, acc: 0.527344]  [A loss: 0.874774, acc: 0.128906]\n",
      "1925: [D loss: 0.691936, acc: 0.521484]  [A loss: 0.853917, acc: 0.187500]\n",
      "1926: [D loss: 0.683663, acc: 0.552734]  [A loss: 0.813587, acc: 0.261719]\n",
      "1927: [D loss: 0.708224, acc: 0.523438]  [A loss: 0.865431, acc: 0.191406]\n",
      "1928: [D loss: 0.683512, acc: 0.541016]  [A loss: 0.838918, acc: 0.242188]\n",
      "1929: [D loss: 0.699424, acc: 0.535156]  [A loss: 0.901941, acc: 0.121094]\n",
      "1930: [D loss: 0.690389, acc: 0.542969]  [A loss: 0.825891, acc: 0.246094]\n",
      "1931: [D loss: 0.693594, acc: 0.531250]  [A loss: 0.904235, acc: 0.144531]\n",
      "1932: [D loss: 0.701676, acc: 0.503906]  [A loss: 0.738575, acc: 0.406250]\n",
      "1933: [D loss: 0.715864, acc: 0.507812]  [A loss: 1.061405, acc: 0.062500]\n",
      "1934: [D loss: 0.693595, acc: 0.525391]  [A loss: 0.654419, acc: 0.640625]\n",
      "1935: [D loss: 0.714393, acc: 0.521484]  [A loss: 1.032275, acc: 0.070312]\n",
      "1936: [D loss: 0.699536, acc: 0.519531]  [A loss: 0.736523, acc: 0.394531]\n",
      "1937: [D loss: 0.704534, acc: 0.509766]  [A loss: 0.884150, acc: 0.175781]\n",
      "1938: [D loss: 0.696517, acc: 0.529297]  [A loss: 0.763147, acc: 0.312500]\n",
      "1939: [D loss: 0.699303, acc: 0.523438]  [A loss: 0.873707, acc: 0.164062]\n",
      "1940: [D loss: 0.697819, acc: 0.531250]  [A loss: 0.768911, acc: 0.351562]\n",
      "1941: [D loss: 0.697439, acc: 0.527344]  [A loss: 0.881929, acc: 0.148438]\n",
      "1942: [D loss: 0.688973, acc: 0.525391]  [A loss: 0.765473, acc: 0.351562]\n",
      "1943: [D loss: 0.704221, acc: 0.525391]  [A loss: 0.841744, acc: 0.191406]\n",
      "1944: [D loss: 0.687872, acc: 0.541016]  [A loss: 0.781538, acc: 0.324219]\n",
      "1945: [D loss: 0.703539, acc: 0.541016]  [A loss: 0.950922, acc: 0.074219]\n",
      "1946: [D loss: 0.694876, acc: 0.521484]  [A loss: 0.702419, acc: 0.480469]\n",
      "1947: [D loss: 0.701917, acc: 0.523438]  [A loss: 0.911986, acc: 0.089844]\n",
      "1948: [D loss: 0.691467, acc: 0.527344]  [A loss: 0.716158, acc: 0.425781]\n",
      "1949: [D loss: 0.705983, acc: 0.523438]  [A loss: 0.898785, acc: 0.121094]\n",
      "1950: [D loss: 0.684200, acc: 0.554688]  [A loss: 0.767178, acc: 0.312500]\n",
      "1951: [D loss: 0.709444, acc: 0.515625]  [A loss: 0.903776, acc: 0.113281]\n",
      "1952: [D loss: 0.698906, acc: 0.535156]  [A loss: 0.727561, acc: 0.457031]\n",
      "1953: [D loss: 0.715269, acc: 0.500000]  [A loss: 0.918944, acc: 0.113281]\n",
      "1954: [D loss: 0.696212, acc: 0.525391]  [A loss: 0.785570, acc: 0.281250]\n",
      "1955: [D loss: 0.696850, acc: 0.515625]  [A loss: 0.895551, acc: 0.101562]\n",
      "1956: [D loss: 0.697828, acc: 0.535156]  [A loss: 0.725417, acc: 0.414062]\n",
      "1957: [D loss: 0.706521, acc: 0.527344]  [A loss: 0.877126, acc: 0.171875]\n",
      "1958: [D loss: 0.683573, acc: 0.568359]  [A loss: 0.772505, acc: 0.285156]\n",
      "1959: [D loss: 0.693158, acc: 0.546875]  [A loss: 0.888486, acc: 0.132812]\n",
      "1960: [D loss: 0.691930, acc: 0.541016]  [A loss: 0.796618, acc: 0.261719]\n",
      "1961: [D loss: 0.711722, acc: 0.501953]  [A loss: 0.933525, acc: 0.105469]\n",
      "1962: [D loss: 0.692275, acc: 0.539062]  [A loss: 0.795670, acc: 0.265625]\n",
      "1963: [D loss: 0.684630, acc: 0.560547]  [A loss: 0.881301, acc: 0.128906]\n",
      "1964: [D loss: 0.690244, acc: 0.546875]  [A loss: 0.849187, acc: 0.183594]\n",
      "1965: [D loss: 0.696940, acc: 0.544922]  [A loss: 0.858736, acc: 0.164062]\n",
      "1966: [D loss: 0.701869, acc: 0.527344]  [A loss: 0.832819, acc: 0.195312]\n",
      "1967: [D loss: 0.691178, acc: 0.560547]  [A loss: 0.861704, acc: 0.152344]\n",
      "1968: [D loss: 0.693194, acc: 0.513672]  [A loss: 0.876619, acc: 0.132812]\n",
      "1969: [D loss: 0.690949, acc: 0.556641]  [A loss: 0.935478, acc: 0.125000]\n",
      "1970: [D loss: 0.693289, acc: 0.539062]  [A loss: 0.763301, acc: 0.335938]\n",
      "1971: [D loss: 0.696815, acc: 0.541016]  [A loss: 0.928366, acc: 0.085938]\n",
      "1972: [D loss: 0.678299, acc: 0.548828]  [A loss: 0.739607, acc: 0.402344]\n",
      "1973: [D loss: 0.689206, acc: 0.521484]  [A loss: 0.988315, acc: 0.078125]\n",
      "1974: [D loss: 0.683409, acc: 0.572266]  [A loss: 0.710718, acc: 0.449219]\n",
      "1975: [D loss: 0.717896, acc: 0.519531]  [A loss: 0.993816, acc: 0.078125]\n",
      "1976: [D loss: 0.694159, acc: 0.539062]  [A loss: 0.644346, acc: 0.679688]\n",
      "1977: [D loss: 0.720381, acc: 0.496094]  [A loss: 0.955792, acc: 0.070312]\n",
      "1978: [D loss: 0.685018, acc: 0.531250]  [A loss: 0.725491, acc: 0.457031]\n",
      "1979: [D loss: 0.698325, acc: 0.507812]  [A loss: 0.860479, acc: 0.164062]\n",
      "1980: [D loss: 0.692485, acc: 0.544922]  [A loss: 0.717514, acc: 0.453125]\n",
      "1981: [D loss: 0.699955, acc: 0.546875]  [A loss: 0.850551, acc: 0.226562]\n",
      "1982: [D loss: 0.694582, acc: 0.517578]  [A loss: 0.752663, acc: 0.386719]\n",
      "1983: [D loss: 0.710326, acc: 0.529297]  [A loss: 1.010324, acc: 0.074219]\n",
      "1984: [D loss: 0.684058, acc: 0.546875]  [A loss: 0.749031, acc: 0.410156]\n",
      "1985: [D loss: 0.705077, acc: 0.521484]  [A loss: 0.819377, acc: 0.242188]\n",
      "1986: [D loss: 0.680627, acc: 0.562500]  [A loss: 0.845250, acc: 0.207031]\n",
      "1987: [D loss: 0.683462, acc: 0.556641]  [A loss: 0.859075, acc: 0.175781]\n",
      "1988: [D loss: 0.699523, acc: 0.554688]  [A loss: 0.834458, acc: 0.230469]\n",
      "1989: [D loss: 0.705544, acc: 0.513672]  [A loss: 0.980113, acc: 0.078125]\n",
      "1990: [D loss: 0.687019, acc: 0.546875]  [A loss: 0.689317, acc: 0.539062]\n",
      "1991: [D loss: 0.721393, acc: 0.509766]  [A loss: 0.953784, acc: 0.066406]\n",
      "1992: [D loss: 0.696138, acc: 0.554688]  [A loss: 0.759135, acc: 0.359375]\n",
      "1993: [D loss: 0.691536, acc: 0.533203]  [A loss: 0.871262, acc: 0.160156]\n",
      "1994: [D loss: 0.687811, acc: 0.542969]  [A loss: 0.776306, acc: 0.324219]\n",
      "1995: [D loss: 0.689383, acc: 0.544922]  [A loss: 0.814211, acc: 0.234375]\n",
      "1996: [D loss: 0.689556, acc: 0.550781]  [A loss: 0.822266, acc: 0.226562]\n",
      "1997: [D loss: 0.697304, acc: 0.531250]  [A loss: 0.810515, acc: 0.238281]\n",
      "1998: [D loss: 0.708440, acc: 0.515625]  [A loss: 0.929412, acc: 0.082031]\n",
      "1999: [D loss: 0.690283, acc: 0.544922]  [A loss: 0.768605, acc: 0.347656]\n",
      "2000: [D loss: 0.701998, acc: 0.525391]  [A loss: 0.926242, acc: 0.128906]\n",
      "2001: [D loss: 0.689553, acc: 0.564453]  [A loss: 0.780431, acc: 0.304688]\n",
      "2002: [D loss: 0.700187, acc: 0.537109]  [A loss: 0.893952, acc: 0.128906]\n",
      "2003: [D loss: 0.689198, acc: 0.560547]  [A loss: 0.719041, acc: 0.449219]\n",
      "2004: [D loss: 0.717647, acc: 0.525391]  [A loss: 0.976995, acc: 0.085938]\n",
      "2005: [D loss: 0.685521, acc: 0.566406]  [A loss: 0.725512, acc: 0.433594]\n",
      "2006: [D loss: 0.710559, acc: 0.509766]  [A loss: 0.930552, acc: 0.097656]\n",
      "2007: [D loss: 0.695202, acc: 0.515625]  [A loss: 0.735497, acc: 0.453125]\n",
      "2008: [D loss: 0.699738, acc: 0.523438]  [A loss: 0.850946, acc: 0.187500]\n",
      "2009: [D loss: 0.697837, acc: 0.529297]  [A loss: 0.822285, acc: 0.242188]\n",
      "2010: [D loss: 0.701548, acc: 0.515625]  [A loss: 0.804445, acc: 0.253906]\n",
      "2011: [D loss: 0.696999, acc: 0.529297]  [A loss: 0.899142, acc: 0.128906]\n",
      "2012: [D loss: 0.677760, acc: 0.548828]  [A loss: 0.718959, acc: 0.457031]\n",
      "2013: [D loss: 0.705746, acc: 0.509766]  [A loss: 0.984225, acc: 0.074219]\n",
      "2014: [D loss: 0.698310, acc: 0.521484]  [A loss: 0.662121, acc: 0.609375]\n",
      "2015: [D loss: 0.720690, acc: 0.509766]  [A loss: 0.993015, acc: 0.046875]\n",
      "2016: [D loss: 0.694369, acc: 0.541016]  [A loss: 0.726062, acc: 0.437500]\n",
      "2017: [D loss: 0.702621, acc: 0.539062]  [A loss: 0.816416, acc: 0.222656]\n",
      "2018: [D loss: 0.699805, acc: 0.525391]  [A loss: 0.837245, acc: 0.191406]\n",
      "2019: [D loss: 0.693843, acc: 0.535156]  [A loss: 0.746586, acc: 0.386719]\n",
      "2020: [D loss: 0.699633, acc: 0.537109]  [A loss: 0.885598, acc: 0.128906]\n",
      "2021: [D loss: 0.687868, acc: 0.546875]  [A loss: 0.770050, acc: 0.386719]\n",
      "2022: [D loss: 0.694722, acc: 0.542969]  [A loss: 0.837988, acc: 0.214844]\n",
      "2023: [D loss: 0.689956, acc: 0.542969]  [A loss: 0.839967, acc: 0.183594]\n",
      "2024: [D loss: 0.698815, acc: 0.517578]  [A loss: 0.838690, acc: 0.214844]\n",
      "2025: [D loss: 0.707601, acc: 0.515625]  [A loss: 0.860474, acc: 0.187500]\n",
      "2026: [D loss: 0.695049, acc: 0.537109]  [A loss: 0.816215, acc: 0.269531]\n",
      "2027: [D loss: 0.695041, acc: 0.523438]  [A loss: 0.854641, acc: 0.195312]\n",
      "2028: [D loss: 0.688418, acc: 0.558594]  [A loss: 0.838080, acc: 0.195312]\n",
      "2029: [D loss: 0.690177, acc: 0.537109]  [A loss: 0.878478, acc: 0.160156]\n",
      "2030: [D loss: 0.688647, acc: 0.521484]  [A loss: 0.830518, acc: 0.226562]\n",
      "2031: [D loss: 0.694687, acc: 0.546875]  [A loss: 0.907081, acc: 0.152344]\n",
      "2032: [D loss: 0.705095, acc: 0.507812]  [A loss: 0.698131, acc: 0.542969]\n",
      "2033: [D loss: 0.726270, acc: 0.505859]  [A loss: 0.988214, acc: 0.066406]\n",
      "2034: [D loss: 0.689681, acc: 0.537109]  [A loss: 0.737828, acc: 0.425781]\n",
      "2035: [D loss: 0.698292, acc: 0.519531]  [A loss: 0.966939, acc: 0.093750]\n",
      "2036: [D loss: 0.689712, acc: 0.558594]  [A loss: 0.705894, acc: 0.464844]\n",
      "2037: [D loss: 0.712823, acc: 0.535156]  [A loss: 0.980881, acc: 0.054688]\n",
      "2038: [D loss: 0.689077, acc: 0.554688]  [A loss: 0.637026, acc: 0.660156]\n",
      "2039: [D loss: 0.724011, acc: 0.521484]  [A loss: 0.964810, acc: 0.085938]\n",
      "2040: [D loss: 0.706303, acc: 0.515625]  [A loss: 0.744533, acc: 0.425781]\n",
      "2041: [D loss: 0.698178, acc: 0.531250]  [A loss: 0.859967, acc: 0.195312]\n",
      "2042: [D loss: 0.699143, acc: 0.505859]  [A loss: 0.760607, acc: 0.339844]\n",
      "2043: [D loss: 0.692785, acc: 0.529297]  [A loss: 0.857777, acc: 0.164062]\n",
      "2044: [D loss: 0.690238, acc: 0.523438]  [A loss: 0.747121, acc: 0.367188]\n",
      "2045: [D loss: 0.699819, acc: 0.537109]  [A loss: 0.832686, acc: 0.246094]\n",
      "2046: [D loss: 0.698528, acc: 0.519531]  [A loss: 0.769820, acc: 0.304688]\n",
      "2047: [D loss: 0.711176, acc: 0.505859]  [A loss: 0.886000, acc: 0.175781]\n",
      "2048: [D loss: 0.692920, acc: 0.529297]  [A loss: 0.730833, acc: 0.460938]\n",
      "2049: [D loss: 0.701373, acc: 0.556641]  [A loss: 0.910548, acc: 0.101562]\n",
      "2050: [D loss: 0.691668, acc: 0.535156]  [A loss: 0.747042, acc: 0.402344]\n",
      "2051: [D loss: 0.720197, acc: 0.492188]  [A loss: 0.873893, acc: 0.136719]\n",
      "2052: [D loss: 0.687914, acc: 0.513672]  [A loss: 0.786945, acc: 0.261719]\n",
      "2053: [D loss: 0.692238, acc: 0.560547]  [A loss: 0.918335, acc: 0.121094]\n",
      "2054: [D loss: 0.690100, acc: 0.539062]  [A loss: 0.797847, acc: 0.285156]\n",
      "2055: [D loss: 0.689629, acc: 0.535156]  [A loss: 0.917308, acc: 0.136719]\n",
      "2056: [D loss: 0.696707, acc: 0.501953]  [A loss: 0.742303, acc: 0.359375]\n",
      "2057: [D loss: 0.694020, acc: 0.535156]  [A loss: 0.923098, acc: 0.097656]\n",
      "2058: [D loss: 0.687166, acc: 0.537109]  [A loss: 0.782360, acc: 0.308594]\n",
      "2059: [D loss: 0.693117, acc: 0.541016]  [A loss: 0.869435, acc: 0.210938]\n",
      "2060: [D loss: 0.704294, acc: 0.509766]  [A loss: 0.768202, acc: 0.328125]\n",
      "2061: [D loss: 0.699779, acc: 0.548828]  [A loss: 0.877569, acc: 0.136719]\n",
      "2062: [D loss: 0.690929, acc: 0.570312]  [A loss: 0.746482, acc: 0.378906]\n",
      "2063: [D loss: 0.692091, acc: 0.550781]  [A loss: 0.922156, acc: 0.105469]\n",
      "2064: [D loss: 0.693064, acc: 0.519531]  [A loss: 0.690806, acc: 0.531250]\n",
      "2065: [D loss: 0.704265, acc: 0.535156]  [A loss: 0.942694, acc: 0.082031]\n",
      "2066: [D loss: 0.676099, acc: 0.572266]  [A loss: 0.689571, acc: 0.550781]\n",
      "2067: [D loss: 0.697071, acc: 0.535156]  [A loss: 0.836545, acc: 0.187500]\n",
      "2068: [D loss: 0.695812, acc: 0.566406]  [A loss: 0.847868, acc: 0.175781]\n",
      "2069: [D loss: 0.683111, acc: 0.552734]  [A loss: 0.812818, acc: 0.296875]\n",
      "2070: [D loss: 0.698849, acc: 0.531250]  [A loss: 0.823535, acc: 0.261719]\n",
      "2071: [D loss: 0.683698, acc: 0.529297]  [A loss: 0.828429, acc: 0.265625]\n",
      "2072: [D loss: 0.690492, acc: 0.544922]  [A loss: 0.850097, acc: 0.183594]\n",
      "2073: [D loss: 0.688375, acc: 0.552734]  [A loss: 0.785591, acc: 0.289062]\n",
      "2074: [D loss: 0.710356, acc: 0.507812]  [A loss: 0.924091, acc: 0.109375]\n",
      "2075: [D loss: 0.679042, acc: 0.568359]  [A loss: 0.754131, acc: 0.359375]\n",
      "2076: [D loss: 0.704768, acc: 0.498047]  [A loss: 0.968713, acc: 0.046875]\n",
      "2077: [D loss: 0.693283, acc: 0.511719]  [A loss: 0.686492, acc: 0.535156]\n",
      "2078: [D loss: 0.717890, acc: 0.525391]  [A loss: 1.010911, acc: 0.042969]\n",
      "2079: [D loss: 0.679821, acc: 0.552734]  [A loss: 0.677793, acc: 0.566406]\n",
      "2080: [D loss: 0.725775, acc: 0.513672]  [A loss: 0.952352, acc: 0.089844]\n",
      "2081: [D loss: 0.684003, acc: 0.560547]  [A loss: 0.721699, acc: 0.460938]\n",
      "2082: [D loss: 0.727459, acc: 0.478516]  [A loss: 0.977858, acc: 0.070312]\n",
      "2083: [D loss: 0.685922, acc: 0.548828]  [A loss: 0.749085, acc: 0.386719]\n",
      "2084: [D loss: 0.705788, acc: 0.519531]  [A loss: 0.864050, acc: 0.207031]\n",
      "2085: [D loss: 0.686830, acc: 0.550781]  [A loss: 0.840909, acc: 0.199219]\n",
      "2086: [D loss: 0.692705, acc: 0.546875]  [A loss: 0.792499, acc: 0.289062]\n",
      "2087: [D loss: 0.701239, acc: 0.519531]  [A loss: 0.785046, acc: 0.265625]\n",
      "2088: [D loss: 0.691718, acc: 0.519531]  [A loss: 0.805650, acc: 0.265625]\n",
      "2089: [D loss: 0.685569, acc: 0.523438]  [A loss: 0.840016, acc: 0.214844]\n",
      "2090: [D loss: 0.696476, acc: 0.533203]  [A loss: 0.798154, acc: 0.300781]\n",
      "2091: [D loss: 0.700785, acc: 0.519531]  [A loss: 0.827146, acc: 0.218750]\n",
      "2092: [D loss: 0.693545, acc: 0.519531]  [A loss: 0.758250, acc: 0.371094]\n",
      "2093: [D loss: 0.699795, acc: 0.519531]  [A loss: 0.990204, acc: 0.078125]\n",
      "2094: [D loss: 0.696582, acc: 0.525391]  [A loss: 0.697555, acc: 0.507812]\n",
      "2095: [D loss: 0.709998, acc: 0.517578]  [A loss: 0.909123, acc: 0.140625]\n",
      "2096: [D loss: 0.697249, acc: 0.519531]  [A loss: 0.760642, acc: 0.347656]\n",
      "2097: [D loss: 0.699862, acc: 0.517578]  [A loss: 0.903509, acc: 0.128906]\n",
      "2098: [D loss: 0.681144, acc: 0.566406]  [A loss: 0.752674, acc: 0.375000]\n",
      "2099: [D loss: 0.705714, acc: 0.527344]  [A loss: 0.876469, acc: 0.140625]\n",
      "2100: [D loss: 0.687092, acc: 0.570312]  [A loss: 0.747674, acc: 0.347656]\n",
      "2101: [D loss: 0.726999, acc: 0.488281]  [A loss: 0.960122, acc: 0.050781]\n",
      "2102: [D loss: 0.692083, acc: 0.521484]  [A loss: 0.709552, acc: 0.488281]\n",
      "2103: [D loss: 0.703131, acc: 0.519531]  [A loss: 0.946099, acc: 0.089844]\n",
      "2104: [D loss: 0.690614, acc: 0.533203]  [A loss: 0.741724, acc: 0.382812]\n",
      "2105: [D loss: 0.711904, acc: 0.529297]  [A loss: 0.926103, acc: 0.136719]\n",
      "2106: [D loss: 0.690515, acc: 0.515625]  [A loss: 0.728837, acc: 0.441406]\n",
      "2107: [D loss: 0.716729, acc: 0.527344]  [A loss: 0.933663, acc: 0.105469]\n",
      "2108: [D loss: 0.701534, acc: 0.521484]  [A loss: 0.734547, acc: 0.371094]\n",
      "2109: [D loss: 0.705189, acc: 0.515625]  [A loss: 0.860067, acc: 0.167969]\n",
      "2110: [D loss: 0.688462, acc: 0.544922]  [A loss: 0.824024, acc: 0.230469]\n",
      "2111: [D loss: 0.690891, acc: 0.529297]  [A loss: 0.816557, acc: 0.222656]\n",
      "2112: [D loss: 0.697755, acc: 0.533203]  [A loss: 0.821638, acc: 0.242188]\n",
      "2113: [D loss: 0.697690, acc: 0.527344]  [A loss: 0.765768, acc: 0.328125]\n",
      "2114: [D loss: 0.699556, acc: 0.544922]  [A loss: 0.875210, acc: 0.132812]\n",
      "2115: [D loss: 0.680274, acc: 0.558594]  [A loss: 0.772943, acc: 0.339844]\n",
      "2116: [D loss: 0.701502, acc: 0.515625]  [A loss: 0.893030, acc: 0.160156]\n",
      "2117: [D loss: 0.687239, acc: 0.546875]  [A loss: 0.748560, acc: 0.394531]\n",
      "2118: [D loss: 0.702497, acc: 0.544922]  [A loss: 1.012472, acc: 0.078125]\n",
      "2119: [D loss: 0.687110, acc: 0.546875]  [A loss: 0.697790, acc: 0.500000]\n",
      "2120: [D loss: 0.706804, acc: 0.501953]  [A loss: 0.927763, acc: 0.085938]\n",
      "2121: [D loss: 0.690899, acc: 0.560547]  [A loss: 0.706596, acc: 0.496094]\n",
      "2122: [D loss: 0.732515, acc: 0.498047]  [A loss: 1.029165, acc: 0.054688]\n",
      "2123: [D loss: 0.691119, acc: 0.533203]  [A loss: 0.731427, acc: 0.406250]\n",
      "2124: [D loss: 0.706014, acc: 0.513672]  [A loss: 0.869844, acc: 0.140625]\n",
      "2125: [D loss: 0.689282, acc: 0.570312]  [A loss: 0.793068, acc: 0.285156]\n",
      "2126: [D loss: 0.696801, acc: 0.550781]  [A loss: 0.838865, acc: 0.226562]\n",
      "2127: [D loss: 0.691922, acc: 0.523438]  [A loss: 0.835651, acc: 0.191406]\n",
      "2128: [D loss: 0.722778, acc: 0.470703]  [A loss: 0.828534, acc: 0.222656]\n",
      "2129: [D loss: 0.693347, acc: 0.535156]  [A loss: 0.823341, acc: 0.250000]\n",
      "2130: [D loss: 0.700717, acc: 0.509766]  [A loss: 0.802842, acc: 0.246094]\n",
      "2131: [D loss: 0.697078, acc: 0.527344]  [A loss: 0.884959, acc: 0.179688]\n",
      "2132: [D loss: 0.685809, acc: 0.568359]  [A loss: 0.737924, acc: 0.375000]\n",
      "2133: [D loss: 0.703139, acc: 0.503906]  [A loss: 0.924343, acc: 0.089844]\n",
      "2134: [D loss: 0.682977, acc: 0.564453]  [A loss: 0.748792, acc: 0.378906]\n",
      "2135: [D loss: 0.695585, acc: 0.554688]  [A loss: 0.862476, acc: 0.156250]\n",
      "2136: [D loss: 0.691754, acc: 0.535156]  [A loss: 0.806984, acc: 0.246094]\n",
      "2137: [D loss: 0.681713, acc: 0.546875]  [A loss: 0.820548, acc: 0.203125]\n",
      "2138: [D loss: 0.680677, acc: 0.568359]  [A loss: 0.835409, acc: 0.187500]\n",
      "2139: [D loss: 0.689298, acc: 0.560547]  [A loss: 0.847770, acc: 0.156250]\n",
      "2140: [D loss: 0.684241, acc: 0.542969]  [A loss: 0.803906, acc: 0.253906]\n",
      "2141: [D loss: 0.699338, acc: 0.523438]  [A loss: 0.916743, acc: 0.101562]\n",
      "2142: [D loss: 0.684369, acc: 0.552734]  [A loss: 0.734766, acc: 0.398438]\n",
      "2143: [D loss: 0.690443, acc: 0.574219]  [A loss: 0.844427, acc: 0.210938]\n",
      "2144: [D loss: 0.689552, acc: 0.574219]  [A loss: 0.826394, acc: 0.230469]\n",
      "2145: [D loss: 0.687632, acc: 0.529297]  [A loss: 0.768160, acc: 0.363281]\n",
      "2146: [D loss: 0.695453, acc: 0.542969]  [A loss: 0.982380, acc: 0.097656]\n",
      "2147: [D loss: 0.688421, acc: 0.558594]  [A loss: 0.745879, acc: 0.371094]\n",
      "2148: [D loss: 0.703789, acc: 0.507812]  [A loss: 0.978095, acc: 0.093750]\n",
      "2149: [D loss: 0.678197, acc: 0.564453]  [A loss: 0.710155, acc: 0.441406]\n",
      "2150: [D loss: 0.717217, acc: 0.525391]  [A loss: 0.912307, acc: 0.136719]\n",
      "2151: [D loss: 0.694793, acc: 0.525391]  [A loss: 0.743360, acc: 0.402344]\n",
      "2152: [D loss: 0.699439, acc: 0.541016]  [A loss: 0.891174, acc: 0.152344]\n",
      "2153: [D loss: 0.678271, acc: 0.578125]  [A loss: 0.784623, acc: 0.304688]\n",
      "2154: [D loss: 0.684161, acc: 0.550781]  [A loss: 0.926458, acc: 0.101562]\n",
      "2155: [D loss: 0.681719, acc: 0.576172]  [A loss: 0.707681, acc: 0.484375]\n",
      "2156: [D loss: 0.695306, acc: 0.552734]  [A loss: 0.929958, acc: 0.097656]\n",
      "2157: [D loss: 0.694406, acc: 0.515625]  [A loss: 0.674931, acc: 0.550781]\n",
      "2158: [D loss: 0.733143, acc: 0.486328]  [A loss: 1.038866, acc: 0.039062]\n",
      "2159: [D loss: 0.705690, acc: 0.501953]  [A loss: 0.722131, acc: 0.406250]\n",
      "2160: [D loss: 0.707401, acc: 0.517578]  [A loss: 0.874964, acc: 0.171875]\n",
      "2161: [D loss: 0.686558, acc: 0.562500]  [A loss: 0.743212, acc: 0.421875]\n",
      "2162: [D loss: 0.717540, acc: 0.523438]  [A loss: 0.893777, acc: 0.132812]\n",
      "2163: [D loss: 0.705513, acc: 0.523438]  [A loss: 0.809116, acc: 0.257812]\n",
      "2164: [D loss: 0.696468, acc: 0.533203]  [A loss: 0.841347, acc: 0.226562]\n",
      "2165: [D loss: 0.681197, acc: 0.564453]  [A loss: 0.788000, acc: 0.332031]\n",
      "2166: [D loss: 0.694195, acc: 0.550781]  [A loss: 0.888657, acc: 0.203125]\n",
      "2167: [D loss: 0.700942, acc: 0.503906]  [A loss: 0.839817, acc: 0.195312]\n",
      "2168: [D loss: 0.691461, acc: 0.554688]  [A loss: 0.843872, acc: 0.222656]\n",
      "2169: [D loss: 0.701029, acc: 0.507812]  [A loss: 0.760544, acc: 0.347656]\n",
      "2170: [D loss: 0.714396, acc: 0.486328]  [A loss: 0.849518, acc: 0.191406]\n",
      "2171: [D loss: 0.699199, acc: 0.494141]  [A loss: 0.826665, acc: 0.218750]\n",
      "2172: [D loss: 0.686335, acc: 0.583984]  [A loss: 0.857246, acc: 0.164062]\n",
      "2173: [D loss: 0.705983, acc: 0.494141]  [A loss: 0.900144, acc: 0.148438]\n",
      "2174: [D loss: 0.697232, acc: 0.541016]  [A loss: 0.771707, acc: 0.316406]\n",
      "2175: [D loss: 0.702143, acc: 0.546875]  [A loss: 0.893894, acc: 0.132812]\n",
      "2176: [D loss: 0.711128, acc: 0.486328]  [A loss: 0.731191, acc: 0.453125]\n",
      "2177: [D loss: 0.709633, acc: 0.498047]  [A loss: 0.886177, acc: 0.136719]\n",
      "2178: [D loss: 0.685575, acc: 0.539062]  [A loss: 0.789944, acc: 0.289062]\n",
      "2179: [D loss: 0.701696, acc: 0.521484]  [A loss: 0.918529, acc: 0.113281]\n",
      "2180: [D loss: 0.692792, acc: 0.521484]  [A loss: 0.651835, acc: 0.652344]\n",
      "2181: [D loss: 0.715235, acc: 0.515625]  [A loss: 0.944188, acc: 0.066406]\n",
      "2182: [D loss: 0.694690, acc: 0.527344]  [A loss: 0.674298, acc: 0.585938]\n",
      "2183: [D loss: 0.731758, acc: 0.480469]  [A loss: 1.001649, acc: 0.066406]\n",
      "2184: [D loss: 0.688359, acc: 0.541016]  [A loss: 0.688140, acc: 0.566406]\n",
      "2185: [D loss: 0.712451, acc: 0.529297]  [A loss: 0.866203, acc: 0.160156]\n",
      "2186: [D loss: 0.691401, acc: 0.539062]  [A loss: 0.794627, acc: 0.289062]\n",
      "2187: [D loss: 0.703047, acc: 0.517578]  [A loss: 0.793399, acc: 0.265625]\n",
      "2188: [D loss: 0.693184, acc: 0.541016]  [A loss: 0.807130, acc: 0.253906]\n",
      "2189: [D loss: 0.690249, acc: 0.533203]  [A loss: 0.818739, acc: 0.250000]\n",
      "2190: [D loss: 0.707681, acc: 0.507812]  [A loss: 0.832297, acc: 0.183594]\n",
      "2191: [D loss: 0.692084, acc: 0.541016]  [A loss: 0.857457, acc: 0.156250]\n",
      "2192: [D loss: 0.697072, acc: 0.525391]  [A loss: 0.818565, acc: 0.238281]\n",
      "2193: [D loss: 0.712271, acc: 0.515625]  [A loss: 0.902992, acc: 0.140625]\n",
      "2194: [D loss: 0.683645, acc: 0.585938]  [A loss: 0.747138, acc: 0.394531]\n",
      "2195: [D loss: 0.708226, acc: 0.531250]  [A loss: 0.942588, acc: 0.093750]\n",
      "2196: [D loss: 0.677956, acc: 0.574219]  [A loss: 0.762424, acc: 0.359375]\n",
      "2197: [D loss: 0.714007, acc: 0.537109]  [A loss: 0.889086, acc: 0.136719]\n",
      "2198: [D loss: 0.686311, acc: 0.542969]  [A loss: 0.792709, acc: 0.257812]\n",
      "2199: [D loss: 0.689503, acc: 0.531250]  [A loss: 0.851041, acc: 0.191406]\n",
      "2200: [D loss: 0.698656, acc: 0.517578]  [A loss: 0.835770, acc: 0.210938]\n",
      "2201: [D loss: 0.699471, acc: 0.515625]  [A loss: 0.834910, acc: 0.199219]\n",
      "2202: [D loss: 0.703809, acc: 0.476562]  [A loss: 0.862599, acc: 0.195312]\n",
      "2203: [D loss: 0.683549, acc: 0.568359]  [A loss: 0.785026, acc: 0.332031]\n",
      "2204: [D loss: 0.698836, acc: 0.521484]  [A loss: 0.881984, acc: 0.128906]\n",
      "2205: [D loss: 0.698554, acc: 0.498047]  [A loss: 0.843323, acc: 0.183594]\n",
      "2206: [D loss: 0.695544, acc: 0.519531]  [A loss: 0.791820, acc: 0.273438]\n",
      "2207: [D loss: 0.700676, acc: 0.509766]  [A loss: 0.910469, acc: 0.113281]\n",
      "2208: [D loss: 0.692991, acc: 0.531250]  [A loss: 0.801471, acc: 0.246094]\n",
      "2209: [D loss: 0.697260, acc: 0.519531]  [A loss: 0.914234, acc: 0.109375]\n",
      "2210: [D loss: 0.686994, acc: 0.564453]  [A loss: 0.741398, acc: 0.398438]\n",
      "2211: [D loss: 0.705349, acc: 0.515625]  [A loss: 0.951948, acc: 0.101562]\n",
      "2212: [D loss: 0.697611, acc: 0.537109]  [A loss: 0.640482, acc: 0.648438]\n",
      "2213: [D loss: 0.724543, acc: 0.503906]  [A loss: 1.015478, acc: 0.054688]\n",
      "2214: [D loss: 0.695820, acc: 0.531250]  [A loss: 0.677244, acc: 0.527344]\n",
      "2215: [D loss: 0.718658, acc: 0.544922]  [A loss: 0.947331, acc: 0.093750]\n",
      "2216: [D loss: 0.696622, acc: 0.521484]  [A loss: 0.770593, acc: 0.339844]\n",
      "2217: [D loss: 0.704293, acc: 0.525391]  [A loss: 0.890499, acc: 0.152344]\n",
      "2218: [D loss: 0.692420, acc: 0.523438]  [A loss: 0.762003, acc: 0.328125]\n",
      "2219: [D loss: 0.706668, acc: 0.505859]  [A loss: 0.871147, acc: 0.148438]\n",
      "2220: [D loss: 0.698987, acc: 0.533203]  [A loss: 0.767390, acc: 0.308594]\n",
      "2221: [D loss: 0.697265, acc: 0.533203]  [A loss: 0.853874, acc: 0.187500]\n",
      "2222: [D loss: 0.679587, acc: 0.580078]  [A loss: 0.718856, acc: 0.445312]\n",
      "2223: [D loss: 0.707206, acc: 0.507812]  [A loss: 0.910586, acc: 0.121094]\n",
      "2224: [D loss: 0.700378, acc: 0.527344]  [A loss: 0.754216, acc: 0.386719]\n",
      "2225: [D loss: 0.722600, acc: 0.498047]  [A loss: 0.992409, acc: 0.066406]\n",
      "2226: [D loss: 0.686045, acc: 0.554688]  [A loss: 0.708370, acc: 0.523438]\n",
      "2227: [D loss: 0.718585, acc: 0.519531]  [A loss: 0.879903, acc: 0.128906]\n",
      "2228: [D loss: 0.693061, acc: 0.539062]  [A loss: 0.781223, acc: 0.308594]\n",
      "2229: [D loss: 0.716459, acc: 0.488281]  [A loss: 0.863945, acc: 0.179688]\n",
      "2230: [D loss: 0.691042, acc: 0.529297]  [A loss: 0.744113, acc: 0.386719]\n",
      "2231: [D loss: 0.716007, acc: 0.509766]  [A loss: 0.934604, acc: 0.089844]\n",
      "2232: [D loss: 0.684970, acc: 0.527344]  [A loss: 0.732774, acc: 0.433594]\n",
      "2233: [D loss: 0.688086, acc: 0.542969]  [A loss: 0.917163, acc: 0.132812]\n",
      "2234: [D loss: 0.692292, acc: 0.535156]  [A loss: 0.780355, acc: 0.308594]\n",
      "2235: [D loss: 0.697496, acc: 0.523438]  [A loss: 0.843191, acc: 0.218750]\n",
      "2236: [D loss: 0.681354, acc: 0.568359]  [A loss: 0.792371, acc: 0.324219]\n",
      "2237: [D loss: 0.701011, acc: 0.523438]  [A loss: 0.861427, acc: 0.195312]\n",
      "2238: [D loss: 0.680345, acc: 0.562500]  [A loss: 0.729609, acc: 0.464844]\n",
      "2239: [D loss: 0.706338, acc: 0.554688]  [A loss: 0.965257, acc: 0.089844]\n",
      "2240: [D loss: 0.698673, acc: 0.507812]  [A loss: 0.657850, acc: 0.621094]\n",
      "2241: [D loss: 0.711732, acc: 0.535156]  [A loss: 0.898938, acc: 0.125000]\n",
      "2242: [D loss: 0.697812, acc: 0.527344]  [A loss: 0.703029, acc: 0.511719]\n",
      "2243: [D loss: 0.732652, acc: 0.505859]  [A loss: 0.968896, acc: 0.082031]\n",
      "2244: [D loss: 0.697030, acc: 0.492188]  [A loss: 0.753569, acc: 0.355469]\n",
      "2245: [D loss: 0.707810, acc: 0.537109]  [A loss: 0.887466, acc: 0.113281]\n",
      "2246: [D loss: 0.693465, acc: 0.519531]  [A loss: 0.765485, acc: 0.359375]\n",
      "2247: [D loss: 0.701578, acc: 0.519531]  [A loss: 0.865562, acc: 0.179688]\n",
      "2248: [D loss: 0.676769, acc: 0.580078]  [A loss: 0.751880, acc: 0.406250]\n",
      "2249: [D loss: 0.713047, acc: 0.515625]  [A loss: 0.895253, acc: 0.089844]\n",
      "2250: [D loss: 0.699265, acc: 0.505859]  [A loss: 0.786514, acc: 0.300781]\n",
      "2251: [D loss: 0.694310, acc: 0.537109]  [A loss: 0.902835, acc: 0.148438]\n",
      "2252: [D loss: 0.700567, acc: 0.515625]  [A loss: 0.704115, acc: 0.500000]\n",
      "2253: [D loss: 0.705561, acc: 0.529297]  [A loss: 0.990849, acc: 0.066406]\n",
      "2254: [D loss: 0.701553, acc: 0.503906]  [A loss: 0.669420, acc: 0.593750]\n",
      "2255: [D loss: 0.716255, acc: 0.519531]  [A loss: 0.910508, acc: 0.105469]\n",
      "2256: [D loss: 0.693331, acc: 0.503906]  [A loss: 0.748460, acc: 0.375000]\n",
      "2257: [D loss: 0.703740, acc: 0.523438]  [A loss: 0.874393, acc: 0.132812]\n",
      "2258: [D loss: 0.700518, acc: 0.517578]  [A loss: 0.784985, acc: 0.304688]\n",
      "2259: [D loss: 0.711414, acc: 0.527344]  [A loss: 0.882888, acc: 0.140625]\n",
      "2260: [D loss: 0.694897, acc: 0.535156]  [A loss: 0.769634, acc: 0.332031]\n",
      "2261: [D loss: 0.689442, acc: 0.558594]  [A loss: 0.807881, acc: 0.265625]\n",
      "2262: [D loss: 0.690125, acc: 0.521484]  [A loss: 0.834263, acc: 0.273438]\n",
      "2263: [D loss: 0.684636, acc: 0.531250]  [A loss: 0.860667, acc: 0.195312]\n",
      "2264: [D loss: 0.689104, acc: 0.554688]  [A loss: 0.790709, acc: 0.273438]\n",
      "2265: [D loss: 0.696961, acc: 0.527344]  [A loss: 0.884168, acc: 0.117188]\n",
      "2266: [D loss: 0.695140, acc: 0.523438]  [A loss: 0.750534, acc: 0.363281]\n",
      "2267: [D loss: 0.713664, acc: 0.492188]  [A loss: 0.924462, acc: 0.113281]\n",
      "2268: [D loss: 0.688439, acc: 0.566406]  [A loss: 0.676228, acc: 0.601562]\n",
      "2269: [D loss: 0.709116, acc: 0.533203]  [A loss: 0.956806, acc: 0.101562]\n",
      "2270: [D loss: 0.689405, acc: 0.546875]  [A loss: 0.746720, acc: 0.414062]\n",
      "2271: [D loss: 0.709775, acc: 0.535156]  [A loss: 0.874864, acc: 0.191406]\n",
      "2272: [D loss: 0.693728, acc: 0.544922]  [A loss: 0.730168, acc: 0.433594]\n",
      "2273: [D loss: 0.716833, acc: 0.527344]  [A loss: 0.830565, acc: 0.230469]\n",
      "2274: [D loss: 0.685448, acc: 0.527344]  [A loss: 0.909927, acc: 0.125000]\n",
      "2275: [D loss: 0.705890, acc: 0.488281]  [A loss: 0.802742, acc: 0.277344]\n",
      "2276: [D loss: 0.707355, acc: 0.505859]  [A loss: 0.796670, acc: 0.308594]\n",
      "2277: [D loss: 0.703022, acc: 0.490234]  [A loss: 0.856380, acc: 0.187500]\n",
      "2278: [D loss: 0.699924, acc: 0.511719]  [A loss: 0.773584, acc: 0.328125]\n",
      "2279: [D loss: 0.702542, acc: 0.509766]  [A loss: 0.920214, acc: 0.101562]\n",
      "2280: [D loss: 0.701384, acc: 0.537109]  [A loss: 0.681771, acc: 0.593750]\n",
      "2281: [D loss: 0.733213, acc: 0.498047]  [A loss: 0.987240, acc: 0.039062]\n",
      "2282: [D loss: 0.696877, acc: 0.511719]  [A loss: 0.700659, acc: 0.527344]\n",
      "2283: [D loss: 0.725406, acc: 0.527344]  [A loss: 0.914952, acc: 0.085938]\n",
      "2284: [D loss: 0.685032, acc: 0.552734]  [A loss: 0.716497, acc: 0.468750]\n",
      "2285: [D loss: 0.726700, acc: 0.513672]  [A loss: 0.903779, acc: 0.132812]\n",
      "2286: [D loss: 0.691841, acc: 0.531250]  [A loss: 0.764009, acc: 0.320312]\n",
      "2287: [D loss: 0.717076, acc: 0.492188]  [A loss: 0.840777, acc: 0.164062]\n",
      "2288: [D loss: 0.701081, acc: 0.529297]  [A loss: 0.852747, acc: 0.179688]\n",
      "2289: [D loss: 0.691570, acc: 0.558594]  [A loss: 0.801728, acc: 0.281250]\n",
      "2290: [D loss: 0.703934, acc: 0.519531]  [A loss: 0.794829, acc: 0.304688]\n",
      "2291: [D loss: 0.691913, acc: 0.519531]  [A loss: 0.799294, acc: 0.296875]\n",
      "2292: [D loss: 0.695394, acc: 0.523438]  [A loss: 0.868727, acc: 0.152344]\n",
      "2293: [D loss: 0.698595, acc: 0.494141]  [A loss: 0.756894, acc: 0.371094]\n",
      "2294: [D loss: 0.696165, acc: 0.541016]  [A loss: 0.938932, acc: 0.093750]\n",
      "2295: [D loss: 0.704768, acc: 0.503906]  [A loss: 0.787520, acc: 0.304688]\n",
      "2296: [D loss: 0.699633, acc: 0.556641]  [A loss: 0.872958, acc: 0.152344]\n",
      "2297: [D loss: 0.688648, acc: 0.572266]  [A loss: 0.779268, acc: 0.316406]\n",
      "2298: [D loss: 0.705129, acc: 0.490234]  [A loss: 0.865608, acc: 0.125000]\n",
      "2299: [D loss: 0.689704, acc: 0.533203]  [A loss: 0.813491, acc: 0.261719]\n",
      "2300: [D loss: 0.698261, acc: 0.517578]  [A loss: 0.834791, acc: 0.242188]\n",
      "2301: [D loss: 0.701626, acc: 0.517578]  [A loss: 0.835297, acc: 0.222656]\n",
      "2302: [D loss: 0.697098, acc: 0.517578]  [A loss: 0.754827, acc: 0.375000]\n",
      "2303: [D loss: 0.711996, acc: 0.507812]  [A loss: 0.980597, acc: 0.078125]\n",
      "2304: [D loss: 0.698998, acc: 0.513672]  [A loss: 0.657482, acc: 0.609375]\n",
      "2305: [D loss: 0.719675, acc: 0.517578]  [A loss: 0.983473, acc: 0.042969]\n",
      "2306: [D loss: 0.696543, acc: 0.511719]  [A loss: 0.694767, acc: 0.507812]\n",
      "2307: [D loss: 0.729318, acc: 0.496094]  [A loss: 0.979446, acc: 0.089844]\n",
      "2308: [D loss: 0.696668, acc: 0.505859]  [A loss: 0.697797, acc: 0.519531]\n",
      "2309: [D loss: 0.708451, acc: 0.509766]  [A loss: 0.857546, acc: 0.175781]\n",
      "2310: [D loss: 0.697221, acc: 0.537109]  [A loss: 0.687137, acc: 0.527344]\n",
      "2311: [D loss: 0.713838, acc: 0.507812]  [A loss: 0.921524, acc: 0.113281]\n",
      "2312: [D loss: 0.702118, acc: 0.511719]  [A loss: 0.693907, acc: 0.488281]\n",
      "2313: [D loss: 0.710369, acc: 0.513672]  [A loss: 0.848094, acc: 0.160156]\n",
      "2314: [D loss: 0.688986, acc: 0.539062]  [A loss: 0.787217, acc: 0.277344]\n",
      "2315: [D loss: 0.697418, acc: 0.527344]  [A loss: 0.827703, acc: 0.234375]\n",
      "2316: [D loss: 0.697248, acc: 0.535156]  [A loss: 0.811486, acc: 0.269531]\n",
      "2317: [D loss: 0.697315, acc: 0.515625]  [A loss: 0.793847, acc: 0.277344]\n",
      "2318: [D loss: 0.708004, acc: 0.511719]  [A loss: 0.831952, acc: 0.203125]\n",
      "2319: [D loss: 0.692025, acc: 0.535156]  [A loss: 0.805875, acc: 0.246094]\n",
      "2320: [D loss: 0.711178, acc: 0.480469]  [A loss: 0.830372, acc: 0.183594]\n",
      "2321: [D loss: 0.681429, acc: 0.562500]  [A loss: 0.801206, acc: 0.277344]\n",
      "2322: [D loss: 0.694627, acc: 0.523438]  [A loss: 0.863618, acc: 0.160156]\n",
      "2323: [D loss: 0.690198, acc: 0.548828]  [A loss: 0.744039, acc: 0.359375]\n",
      "2324: [D loss: 0.700702, acc: 0.525391]  [A loss: 0.927446, acc: 0.121094]\n",
      "2325: [D loss: 0.693426, acc: 0.517578]  [A loss: 0.740943, acc: 0.421875]\n",
      "2326: [D loss: 0.718605, acc: 0.478516]  [A loss: 0.950778, acc: 0.062500]\n",
      "2327: [D loss: 0.689046, acc: 0.562500]  [A loss: 0.738933, acc: 0.414062]\n",
      "2328: [D loss: 0.709825, acc: 0.496094]  [A loss: 0.955809, acc: 0.089844]\n",
      "2329: [D loss: 0.699429, acc: 0.546875]  [A loss: 0.731089, acc: 0.406250]\n",
      "2330: [D loss: 0.710698, acc: 0.503906]  [A loss: 0.879240, acc: 0.085938]\n",
      "2331: [D loss: 0.696631, acc: 0.539062]  [A loss: 0.720425, acc: 0.449219]\n",
      "2332: [D loss: 0.716063, acc: 0.498047]  [A loss: 0.949181, acc: 0.074219]\n",
      "2333: [D loss: 0.686947, acc: 0.550781]  [A loss: 0.731422, acc: 0.433594]\n",
      "2334: [D loss: 0.703814, acc: 0.513672]  [A loss: 0.850548, acc: 0.203125]\n",
      "2335: [D loss: 0.689911, acc: 0.535156]  [A loss: 0.782746, acc: 0.308594]\n",
      "2336: [D loss: 0.718716, acc: 0.486328]  [A loss: 0.811192, acc: 0.234375]\n",
      "2337: [D loss: 0.702525, acc: 0.509766]  [A loss: 0.945999, acc: 0.085938]\n",
      "2338: [D loss: 0.696382, acc: 0.552734]  [A loss: 0.683008, acc: 0.531250]\n",
      "2339: [D loss: 0.707496, acc: 0.517578]  [A loss: 0.905742, acc: 0.125000]\n",
      "2340: [D loss: 0.686456, acc: 0.554688]  [A loss: 0.728371, acc: 0.453125]\n",
      "2341: [D loss: 0.696690, acc: 0.509766]  [A loss: 0.863592, acc: 0.167969]\n",
      "2342: [D loss: 0.689507, acc: 0.537109]  [A loss: 0.823003, acc: 0.250000]\n",
      "2343: [D loss: 0.701338, acc: 0.494141]  [A loss: 0.820209, acc: 0.265625]\n",
      "2344: [D loss: 0.698427, acc: 0.503906]  [A loss: 0.815658, acc: 0.242188]\n",
      "2345: [D loss: 0.693642, acc: 0.556641]  [A loss: 0.804850, acc: 0.273438]\n",
      "2346: [D loss: 0.700427, acc: 0.501953]  [A loss: 0.809086, acc: 0.238281]\n",
      "2347: [D loss: 0.693106, acc: 0.535156]  [A loss: 0.840028, acc: 0.222656]\n",
      "2348: [D loss: 0.695922, acc: 0.531250]  [A loss: 0.780525, acc: 0.343750]\n",
      "2349: [D loss: 0.704791, acc: 0.523438]  [A loss: 0.879422, acc: 0.140625]\n",
      "2350: [D loss: 0.704184, acc: 0.492188]  [A loss: 0.731153, acc: 0.433594]\n",
      "2351: [D loss: 0.707487, acc: 0.531250]  [A loss: 0.973029, acc: 0.054688]\n",
      "2352: [D loss: 0.691201, acc: 0.548828]  [A loss: 0.746759, acc: 0.417969]\n",
      "2353: [D loss: 0.695860, acc: 0.550781]  [A loss: 0.857001, acc: 0.140625]\n",
      "2354: [D loss: 0.696722, acc: 0.513672]  [A loss: 0.764711, acc: 0.351562]\n",
      "2355: [D loss: 0.719063, acc: 0.501953]  [A loss: 0.950357, acc: 0.066406]\n",
      "2356: [D loss: 0.692048, acc: 0.578125]  [A loss: 0.771213, acc: 0.324219]\n",
      "2357: [D loss: 0.706702, acc: 0.490234]  [A loss: 0.894513, acc: 0.125000]\n",
      "2358: [D loss: 0.685703, acc: 0.558594]  [A loss: 0.765138, acc: 0.367188]\n",
      "2359: [D loss: 0.699199, acc: 0.527344]  [A loss: 0.871877, acc: 0.148438]\n",
      "2360: [D loss: 0.687123, acc: 0.570312]  [A loss: 0.722031, acc: 0.457031]\n",
      "2361: [D loss: 0.708809, acc: 0.531250]  [A loss: 0.910192, acc: 0.164062]\n",
      "2362: [D loss: 0.712114, acc: 0.466797]  [A loss: 0.796795, acc: 0.292969]\n",
      "2363: [D loss: 0.698543, acc: 0.533203]  [A loss: 0.827621, acc: 0.179688]\n",
      "2364: [D loss: 0.701225, acc: 0.523438]  [A loss: 0.761340, acc: 0.335938]\n",
      "2365: [D loss: 0.686326, acc: 0.564453]  [A loss: 0.901341, acc: 0.125000]\n",
      "2366: [D loss: 0.693946, acc: 0.523438]  [A loss: 0.788479, acc: 0.328125]\n",
      "2367: [D loss: 0.697842, acc: 0.539062]  [A loss: 0.813789, acc: 0.250000]\n",
      "2368: [D loss: 0.703458, acc: 0.503906]  [A loss: 0.988867, acc: 0.085938]\n",
      "2369: [D loss: 0.694970, acc: 0.539062]  [A loss: 0.672886, acc: 0.582031]\n",
      "2370: [D loss: 0.706995, acc: 0.533203]  [A loss: 0.939976, acc: 0.125000]\n",
      "2371: [D loss: 0.694892, acc: 0.539062]  [A loss: 0.711801, acc: 0.492188]\n",
      "2372: [D loss: 0.700116, acc: 0.503906]  [A loss: 0.854819, acc: 0.191406]\n",
      "2373: [D loss: 0.683680, acc: 0.566406]  [A loss: 0.762831, acc: 0.347656]\n",
      "2374: [D loss: 0.712570, acc: 0.511719]  [A loss: 0.852833, acc: 0.191406]\n",
      "2375: [D loss: 0.695187, acc: 0.527344]  [A loss: 0.780235, acc: 0.312500]\n",
      "2376: [D loss: 0.701371, acc: 0.521484]  [A loss: 0.796339, acc: 0.277344]\n",
      "2377: [D loss: 0.698172, acc: 0.535156]  [A loss: 0.862446, acc: 0.152344]\n",
      "2378: [D loss: 0.698535, acc: 0.529297]  [A loss: 0.808397, acc: 0.269531]\n",
      "2379: [D loss: 0.698272, acc: 0.494141]  [A loss: 0.851078, acc: 0.203125]\n",
      "2380: [D loss: 0.694010, acc: 0.552734]  [A loss: 0.806391, acc: 0.324219]\n",
      "2381: [D loss: 0.696535, acc: 0.542969]  [A loss: 0.866755, acc: 0.160156]\n",
      "2382: [D loss: 0.689699, acc: 0.560547]  [A loss: 0.752213, acc: 0.378906]\n",
      "2383: [D loss: 0.708164, acc: 0.533203]  [A loss: 0.948311, acc: 0.101562]\n",
      "2384: [D loss: 0.705905, acc: 0.500000]  [A loss: 0.726925, acc: 0.421875]\n",
      "2385: [D loss: 0.694289, acc: 0.523438]  [A loss: 0.913766, acc: 0.128906]\n",
      "2386: [D loss: 0.702067, acc: 0.498047]  [A loss: 0.729498, acc: 0.417969]\n",
      "2387: [D loss: 0.704975, acc: 0.498047]  [A loss: 0.889467, acc: 0.128906]\n",
      "2388: [D loss: 0.705124, acc: 0.509766]  [A loss: 0.870023, acc: 0.140625]\n",
      "2389: [D loss: 0.694211, acc: 0.554688]  [A loss: 0.832998, acc: 0.191406]\n",
      "2390: [D loss: 0.691536, acc: 0.535156]  [A loss: 0.829624, acc: 0.199219]\n",
      "2391: [D loss: 0.698890, acc: 0.529297]  [A loss: 0.892994, acc: 0.144531]\n",
      "2392: [D loss: 0.680389, acc: 0.554688]  [A loss: 0.752565, acc: 0.324219]\n",
      "2393: [D loss: 0.698922, acc: 0.529297]  [A loss: 0.974539, acc: 0.125000]\n",
      "2394: [D loss: 0.693426, acc: 0.541016]  [A loss: 0.680680, acc: 0.554688]\n",
      "2395: [D loss: 0.720659, acc: 0.537109]  [A loss: 1.008507, acc: 0.035156]\n",
      "2396: [D loss: 0.683411, acc: 0.554688]  [A loss: 0.668202, acc: 0.601562]\n",
      "2397: [D loss: 0.723674, acc: 0.486328]  [A loss: 0.951276, acc: 0.101562]\n",
      "2398: [D loss: 0.685243, acc: 0.544922]  [A loss: 0.742757, acc: 0.417969]\n",
      "2399: [D loss: 0.689037, acc: 0.552734]  [A loss: 0.819745, acc: 0.230469]\n",
      "2400: [D loss: 0.694041, acc: 0.542969]  [A loss: 0.746370, acc: 0.390625]\n",
      "2401: [D loss: 0.704652, acc: 0.535156]  [A loss: 0.871219, acc: 0.148438]\n",
      "2402: [D loss: 0.693924, acc: 0.531250]  [A loss: 0.786620, acc: 0.285156]\n",
      "2403: [D loss: 0.692861, acc: 0.535156]  [A loss: 0.798652, acc: 0.253906]\n",
      "2404: [D loss: 0.709805, acc: 0.511719]  [A loss: 0.861963, acc: 0.164062]\n",
      "2405: [D loss: 0.700940, acc: 0.525391]  [A loss: 0.810084, acc: 0.265625]\n",
      "2406: [D loss: 0.710302, acc: 0.492188]  [A loss: 0.875677, acc: 0.164062]\n",
      "2407: [D loss: 0.686012, acc: 0.566406]  [A loss: 0.724863, acc: 0.437500]\n",
      "2408: [D loss: 0.706222, acc: 0.501953]  [A loss: 0.946183, acc: 0.082031]\n",
      "2409: [D loss: 0.694626, acc: 0.515625]  [A loss: 0.683364, acc: 0.546875]\n",
      "2410: [D loss: 0.708591, acc: 0.494141]  [A loss: 0.955116, acc: 0.093750]\n",
      "2411: [D loss: 0.684088, acc: 0.548828]  [A loss: 0.738056, acc: 0.429688]\n",
      "2412: [D loss: 0.691139, acc: 0.550781]  [A loss: 0.923802, acc: 0.089844]\n",
      "2413: [D loss: 0.685123, acc: 0.537109]  [A loss: 0.694215, acc: 0.519531]\n",
      "2414: [D loss: 0.710129, acc: 0.531250]  [A loss: 0.946154, acc: 0.074219]\n",
      "2415: [D loss: 0.700797, acc: 0.544922]  [A loss: 0.705320, acc: 0.515625]\n",
      "2416: [D loss: 0.709067, acc: 0.533203]  [A loss: 0.900147, acc: 0.167969]\n",
      "2417: [D loss: 0.698835, acc: 0.519531]  [A loss: 0.772261, acc: 0.312500]\n",
      "2418: [D loss: 0.707550, acc: 0.511719]  [A loss: 0.833985, acc: 0.210938]\n",
      "2419: [D loss: 0.699506, acc: 0.519531]  [A loss: 0.813809, acc: 0.253906]\n",
      "2420: [D loss: 0.694267, acc: 0.529297]  [A loss: 0.811270, acc: 0.238281]\n",
      "2421: [D loss: 0.722960, acc: 0.472656]  [A loss: 0.881890, acc: 0.128906]\n",
      "2422: [D loss: 0.686762, acc: 0.568359]  [A loss: 0.787022, acc: 0.289062]\n",
      "2423: [D loss: 0.698553, acc: 0.525391]  [A loss: 0.862040, acc: 0.132812]\n",
      "2424: [D loss: 0.695606, acc: 0.521484]  [A loss: 0.807708, acc: 0.230469]\n",
      "2425: [D loss: 0.682176, acc: 0.560547]  [A loss: 0.902995, acc: 0.132812]\n",
      "2426: [D loss: 0.702212, acc: 0.521484]  [A loss: 0.747922, acc: 0.382812]\n",
      "2427: [D loss: 0.711897, acc: 0.503906]  [A loss: 1.004066, acc: 0.054688]\n",
      "2428: [D loss: 0.712375, acc: 0.474609]  [A loss: 0.683758, acc: 0.531250]\n",
      "2429: [D loss: 0.715961, acc: 0.527344]  [A loss: 0.931838, acc: 0.070312]\n",
      "2430: [D loss: 0.694151, acc: 0.552734]  [A loss: 0.704604, acc: 0.519531]\n",
      "2431: [D loss: 0.706228, acc: 0.498047]  [A loss: 0.941458, acc: 0.074219]\n",
      "2432: [D loss: 0.695669, acc: 0.535156]  [A loss: 0.762188, acc: 0.351562]\n",
      "2433: [D loss: 0.698188, acc: 0.537109]  [A loss: 0.891068, acc: 0.128906]\n",
      "2434: [D loss: 0.690790, acc: 0.541016]  [A loss: 0.737569, acc: 0.425781]\n",
      "2435: [D loss: 0.718691, acc: 0.500000]  [A loss: 0.948768, acc: 0.085938]\n",
      "2436: [D loss: 0.686632, acc: 0.552734]  [A loss: 0.768968, acc: 0.300781]\n",
      "2437: [D loss: 0.711796, acc: 0.501953]  [A loss: 0.890450, acc: 0.128906]\n",
      "2438: [D loss: 0.695479, acc: 0.525391]  [A loss: 0.806683, acc: 0.253906]\n",
      "2439: [D loss: 0.714183, acc: 0.496094]  [A loss: 0.871291, acc: 0.171875]\n",
      "2440: [D loss: 0.687133, acc: 0.566406]  [A loss: 0.761876, acc: 0.355469]\n",
      "2441: [D loss: 0.703929, acc: 0.519531]  [A loss: 0.900501, acc: 0.148438]\n",
      "2442: [D loss: 0.682673, acc: 0.548828]  [A loss: 0.716739, acc: 0.457031]\n",
      "2443: [D loss: 0.712850, acc: 0.509766]  [A loss: 0.984456, acc: 0.089844]\n",
      "2444: [D loss: 0.690873, acc: 0.566406]  [A loss: 0.708385, acc: 0.488281]\n",
      "2445: [D loss: 0.705099, acc: 0.537109]  [A loss: 0.884225, acc: 0.140625]\n",
      "2446: [D loss: 0.687032, acc: 0.554688]  [A loss: 0.750572, acc: 0.378906]\n",
      "2447: [D loss: 0.702798, acc: 0.521484]  [A loss: 0.853173, acc: 0.171875]\n",
      "2448: [D loss: 0.682948, acc: 0.560547]  [A loss: 0.741028, acc: 0.410156]\n",
      "2449: [D loss: 0.706810, acc: 0.498047]  [A loss: 0.885524, acc: 0.125000]\n",
      "2450: [D loss: 0.687951, acc: 0.554688]  [A loss: 0.782588, acc: 0.296875]\n",
      "2451: [D loss: 0.706272, acc: 0.513672]  [A loss: 0.826587, acc: 0.222656]\n",
      "2452: [D loss: 0.694568, acc: 0.548828]  [A loss: 0.804179, acc: 0.261719]\n",
      "2453: [D loss: 0.700050, acc: 0.541016]  [A loss: 0.856634, acc: 0.199219]\n",
      "2454: [D loss: 0.702181, acc: 0.498047]  [A loss: 0.819750, acc: 0.246094]\n",
      "2455: [D loss: 0.701405, acc: 0.503906]  [A loss: 0.863421, acc: 0.167969]\n",
      "2456: [D loss: 0.704198, acc: 0.513672]  [A loss: 0.776739, acc: 0.308594]\n",
      "2457: [D loss: 0.686919, acc: 0.544922]  [A loss: 0.930076, acc: 0.085938]\n",
      "2458: [D loss: 0.687914, acc: 0.527344]  [A loss: 0.759241, acc: 0.375000]\n",
      "2459: [D loss: 0.710375, acc: 0.507812]  [A loss: 0.886406, acc: 0.125000]\n",
      "2460: [D loss: 0.701459, acc: 0.533203]  [A loss: 0.808502, acc: 0.281250]\n",
      "2461: [D loss: 0.698642, acc: 0.537109]  [A loss: 0.815851, acc: 0.253906]\n",
      "2462: [D loss: 0.703256, acc: 0.513672]  [A loss: 0.865043, acc: 0.156250]\n",
      "2463: [D loss: 0.699654, acc: 0.515625]  [A loss: 0.910006, acc: 0.152344]\n",
      "2464: [D loss: 0.686000, acc: 0.535156]  [A loss: 0.718616, acc: 0.437500]\n",
      "2465: [D loss: 0.720874, acc: 0.498047]  [A loss: 0.989621, acc: 0.066406]\n",
      "2466: [D loss: 0.698338, acc: 0.533203]  [A loss: 0.712956, acc: 0.468750]\n",
      "2467: [D loss: 0.719061, acc: 0.519531]  [A loss: 1.040418, acc: 0.050781]\n",
      "2468: [D loss: 0.698372, acc: 0.533203]  [A loss: 0.713120, acc: 0.464844]\n",
      "2469: [D loss: 0.727402, acc: 0.496094]  [A loss: 0.932265, acc: 0.097656]\n",
      "2470: [D loss: 0.697834, acc: 0.505859]  [A loss: 0.744543, acc: 0.394531]\n",
      "2471: [D loss: 0.715770, acc: 0.492188]  [A loss: 0.851315, acc: 0.195312]\n",
      "2472: [D loss: 0.699334, acc: 0.498047]  [A loss: 0.727285, acc: 0.414062]\n",
      "2473: [D loss: 0.706060, acc: 0.507812]  [A loss: 0.851582, acc: 0.175781]\n",
      "2474: [D loss: 0.699811, acc: 0.523438]  [A loss: 0.794463, acc: 0.285156]\n",
      "2475: [D loss: 0.705323, acc: 0.509766]  [A loss: 0.866763, acc: 0.175781]\n",
      "2476: [D loss: 0.691938, acc: 0.537109]  [A loss: 0.752112, acc: 0.347656]\n",
      "2477: [D loss: 0.690879, acc: 0.582031]  [A loss: 0.833860, acc: 0.265625]\n",
      "2478: [D loss: 0.680428, acc: 0.554688]  [A loss: 0.736229, acc: 0.398438]\n",
      "2479: [D loss: 0.709985, acc: 0.529297]  [A loss: 0.947706, acc: 0.054688]\n",
      "2480: [D loss: 0.682290, acc: 0.574219]  [A loss: 0.727619, acc: 0.429688]\n",
      "2481: [D loss: 0.704010, acc: 0.525391]  [A loss: 0.927461, acc: 0.140625]\n",
      "2482: [D loss: 0.694862, acc: 0.541016]  [A loss: 0.680731, acc: 0.542969]\n",
      "2483: [D loss: 0.715338, acc: 0.513672]  [A loss: 0.952475, acc: 0.089844]\n",
      "2484: [D loss: 0.694965, acc: 0.531250]  [A loss: 0.687736, acc: 0.531250]\n",
      "2485: [D loss: 0.715351, acc: 0.517578]  [A loss: 0.933863, acc: 0.132812]\n",
      "2486: [D loss: 0.682829, acc: 0.587891]  [A loss: 0.690680, acc: 0.546875]\n",
      "2487: [D loss: 0.720977, acc: 0.521484]  [A loss: 0.885178, acc: 0.160156]\n",
      "2488: [D loss: 0.693989, acc: 0.541016]  [A loss: 0.772786, acc: 0.324219]\n",
      "2489: [D loss: 0.702195, acc: 0.535156]  [A loss: 0.862510, acc: 0.167969]\n",
      "2490: [D loss: 0.701053, acc: 0.523438]  [A loss: 0.825885, acc: 0.222656]\n",
      "2491: [D loss: 0.701952, acc: 0.521484]  [A loss: 0.776955, acc: 0.312500]\n",
      "2492: [D loss: 0.685158, acc: 0.542969]  [A loss: 0.841046, acc: 0.250000]\n",
      "2493: [D loss: 0.702662, acc: 0.513672]  [A loss: 0.746981, acc: 0.371094]\n",
      "2494: [D loss: 0.704079, acc: 0.500000]  [A loss: 0.877099, acc: 0.160156]\n",
      "2495: [D loss: 0.690129, acc: 0.541016]  [A loss: 0.757239, acc: 0.378906]\n",
      "2496: [D loss: 0.711056, acc: 0.523438]  [A loss: 0.967856, acc: 0.070312]\n",
      "2497: [D loss: 0.690545, acc: 0.554688]  [A loss: 0.721516, acc: 0.453125]\n",
      "2498: [D loss: 0.698871, acc: 0.531250]  [A loss: 0.946541, acc: 0.105469]\n",
      "2499: [D loss: 0.697355, acc: 0.548828]  [A loss: 0.722370, acc: 0.457031]\n",
      "2500: [D loss: 0.712514, acc: 0.494141]  [A loss: 0.943241, acc: 0.113281]\n",
      "2501: [D loss: 0.694397, acc: 0.511719]  [A loss: 0.683416, acc: 0.531250]\n",
      "2502: [D loss: 0.715999, acc: 0.517578]  [A loss: 0.883376, acc: 0.160156]\n",
      "2503: [D loss: 0.701430, acc: 0.505859]  [A loss: 0.721539, acc: 0.429688]\n",
      "2504: [D loss: 0.706971, acc: 0.501953]  [A loss: 0.895697, acc: 0.121094]\n",
      "2505: [D loss: 0.688087, acc: 0.546875]  [A loss: 0.756256, acc: 0.355469]\n",
      "2506: [D loss: 0.707449, acc: 0.550781]  [A loss: 0.978000, acc: 0.066406]\n",
      "2507: [D loss: 0.707138, acc: 0.476562]  [A loss: 0.684705, acc: 0.558594]\n",
      "2508: [D loss: 0.715426, acc: 0.494141]  [A loss: 0.889089, acc: 0.136719]\n",
      "2509: [D loss: 0.691974, acc: 0.550781]  [A loss: 0.720890, acc: 0.445312]\n",
      "2510: [D loss: 0.723527, acc: 0.519531]  [A loss: 0.973205, acc: 0.062500]\n",
      "2511: [D loss: 0.693561, acc: 0.531250]  [A loss: 0.739532, acc: 0.378906]\n",
      "2512: [D loss: 0.702653, acc: 0.531250]  [A loss: 0.909020, acc: 0.105469]\n",
      "2513: [D loss: 0.708200, acc: 0.505859]  [A loss: 0.775051, acc: 0.304688]\n",
      "2514: [D loss: 0.701191, acc: 0.527344]  [A loss: 0.859081, acc: 0.191406]\n",
      "2515: [D loss: 0.688316, acc: 0.541016]  [A loss: 0.789607, acc: 0.320312]\n",
      "2516: [D loss: 0.690114, acc: 0.560547]  [A loss: 0.855105, acc: 0.195312]\n",
      "2517: [D loss: 0.707762, acc: 0.484375]  [A loss: 0.789918, acc: 0.289062]\n",
      "2518: [D loss: 0.699382, acc: 0.517578]  [A loss: 0.876815, acc: 0.156250]\n",
      "2519: [D loss: 0.715228, acc: 0.488281]  [A loss: 0.804820, acc: 0.265625]\n",
      "2520: [D loss: 0.703517, acc: 0.525391]  [A loss: 0.859062, acc: 0.207031]\n",
      "2521: [D loss: 0.695342, acc: 0.533203]  [A loss: 0.750205, acc: 0.386719]\n",
      "2522: [D loss: 0.713096, acc: 0.494141]  [A loss: 0.854663, acc: 0.187500]\n",
      "2523: [D loss: 0.680925, acc: 0.576172]  [A loss: 0.743706, acc: 0.359375]\n",
      "2524: [D loss: 0.691481, acc: 0.552734]  [A loss: 0.902062, acc: 0.121094]\n",
      "2525: [D loss: 0.693031, acc: 0.542969]  [A loss: 0.779831, acc: 0.300781]\n",
      "2526: [D loss: 0.714933, acc: 0.478516]  [A loss: 0.946385, acc: 0.074219]\n",
      "2527: [D loss: 0.684695, acc: 0.587891]  [A loss: 0.650188, acc: 0.640625]\n",
      "2528: [D loss: 0.723144, acc: 0.515625]  [A loss: 0.994972, acc: 0.062500]\n",
      "2529: [D loss: 0.698967, acc: 0.513672]  [A loss: 0.644503, acc: 0.648438]\n",
      "2530: [D loss: 0.711248, acc: 0.517578]  [A loss: 0.866810, acc: 0.164062]\n",
      "2531: [D loss: 0.686221, acc: 0.550781]  [A loss: 0.741016, acc: 0.398438]\n",
      "2532: [D loss: 0.711084, acc: 0.488281]  [A loss: 0.898872, acc: 0.132812]\n",
      "2533: [D loss: 0.692160, acc: 0.511719]  [A loss: 0.731994, acc: 0.406250]\n",
      "2534: [D loss: 0.699941, acc: 0.531250]  [A loss: 0.875590, acc: 0.171875]\n",
      "2535: [D loss: 0.705175, acc: 0.490234]  [A loss: 0.778277, acc: 0.304688]\n",
      "2536: [D loss: 0.698577, acc: 0.513672]  [A loss: 0.858887, acc: 0.195312]\n",
      "2537: [D loss: 0.696863, acc: 0.521484]  [A loss: 0.768517, acc: 0.339844]\n",
      "2538: [D loss: 0.701362, acc: 0.527344]  [A loss: 0.810705, acc: 0.257812]\n",
      "2539: [D loss: 0.702014, acc: 0.511719]  [A loss: 0.792699, acc: 0.242188]\n",
      "2540: [D loss: 0.703103, acc: 0.507812]  [A loss: 0.850271, acc: 0.164062]\n",
      "2541: [D loss: 0.700126, acc: 0.515625]  [A loss: 0.814552, acc: 0.238281]\n",
      "2542: [D loss: 0.689186, acc: 0.541016]  [A loss: 0.759801, acc: 0.378906]\n",
      "2543: [D loss: 0.705450, acc: 0.500000]  [A loss: 0.892847, acc: 0.136719]\n",
      "2544: [D loss: 0.695156, acc: 0.513672]  [A loss: 0.750373, acc: 0.382812]\n",
      "2545: [D loss: 0.692374, acc: 0.560547]  [A loss: 0.917655, acc: 0.136719]\n",
      "2546: [D loss: 0.698566, acc: 0.523438]  [A loss: 0.761442, acc: 0.390625]\n",
      "2547: [D loss: 0.705542, acc: 0.535156]  [A loss: 0.868392, acc: 0.187500]\n",
      "2548: [D loss: 0.694656, acc: 0.521484]  [A loss: 0.833149, acc: 0.234375]\n",
      "2549: [D loss: 0.700054, acc: 0.527344]  [A loss: 0.841783, acc: 0.191406]\n",
      "2550: [D loss: 0.709283, acc: 0.488281]  [A loss: 0.838669, acc: 0.203125]\n",
      "2551: [D loss: 0.696840, acc: 0.541016]  [A loss: 0.880702, acc: 0.164062]\n",
      "2552: [D loss: 0.688967, acc: 0.560547]  [A loss: 0.789300, acc: 0.312500]\n",
      "2553: [D loss: 0.715803, acc: 0.501953]  [A loss: 1.012629, acc: 0.093750]\n",
      "2554: [D loss: 0.693220, acc: 0.525391]  [A loss: 0.678709, acc: 0.539062]\n",
      "2555: [D loss: 0.710079, acc: 0.542969]  [A loss: 0.932908, acc: 0.117188]\n",
      "2556: [D loss: 0.690322, acc: 0.550781]  [A loss: 0.790894, acc: 0.289062]\n",
      "2557: [D loss: 0.700098, acc: 0.556641]  [A loss: 0.846565, acc: 0.191406]\n",
      "2558: [D loss: 0.689796, acc: 0.519531]  [A loss: 0.781621, acc: 0.324219]\n",
      "2559: [D loss: 0.700819, acc: 0.529297]  [A loss: 0.898283, acc: 0.164062]\n",
      "2560: [D loss: 0.696012, acc: 0.521484]  [A loss: 0.711724, acc: 0.460938]\n",
      "2561: [D loss: 0.704842, acc: 0.531250]  [A loss: 0.900014, acc: 0.113281]\n",
      "2562: [D loss: 0.692116, acc: 0.539062]  [A loss: 0.767412, acc: 0.320312]\n",
      "2563: [D loss: 0.706024, acc: 0.511719]  [A loss: 0.878204, acc: 0.160156]\n",
      "2564: [D loss: 0.712080, acc: 0.503906]  [A loss: 0.797144, acc: 0.269531]\n",
      "2565: [D loss: 0.700057, acc: 0.513672]  [A loss: 0.915956, acc: 0.117188]\n",
      "2566: [D loss: 0.707223, acc: 0.498047]  [A loss: 0.731909, acc: 0.417969]\n",
      "2567: [D loss: 0.705759, acc: 0.525391]  [A loss: 0.888779, acc: 0.148438]\n",
      "2568: [D loss: 0.685769, acc: 0.529297]  [A loss: 0.748552, acc: 0.363281]\n",
      "2569: [D loss: 0.704903, acc: 0.503906]  [A loss: 0.916096, acc: 0.125000]\n",
      "2570: [D loss: 0.694187, acc: 0.539062]  [A loss: 0.828661, acc: 0.214844]\n",
      "2571: [D loss: 0.703312, acc: 0.509766]  [A loss: 0.906762, acc: 0.167969]\n",
      "2572: [D loss: 0.702227, acc: 0.511719]  [A loss: 0.749980, acc: 0.414062]\n",
      "2573: [D loss: 0.690487, acc: 0.533203]  [A loss: 0.915718, acc: 0.113281]\n",
      "2574: [D loss: 0.699162, acc: 0.517578]  [A loss: 0.715904, acc: 0.460938]\n",
      "2575: [D loss: 0.712929, acc: 0.535156]  [A loss: 0.931936, acc: 0.105469]\n",
      "2576: [D loss: 0.685013, acc: 0.537109]  [A loss: 0.739259, acc: 0.425781]\n",
      "2577: [D loss: 0.698467, acc: 0.556641]  [A loss: 0.862876, acc: 0.234375]\n",
      "2578: [D loss: 0.695761, acc: 0.517578]  [A loss: 0.818842, acc: 0.242188]\n",
      "2579: [D loss: 0.688807, acc: 0.541016]  [A loss: 0.783816, acc: 0.324219]\n",
      "2580: [D loss: 0.702627, acc: 0.541016]  [A loss: 0.904348, acc: 0.136719]\n",
      "2581: [D loss: 0.719286, acc: 0.492188]  [A loss: 0.753737, acc: 0.316406]\n",
      "2582: [D loss: 0.712924, acc: 0.519531]  [A loss: 0.997018, acc: 0.082031]\n",
      "2583: [D loss: 0.695391, acc: 0.525391]  [A loss: 0.658656, acc: 0.621094]\n",
      "2584: [D loss: 0.704695, acc: 0.541016]  [A loss: 0.919237, acc: 0.105469]\n",
      "2585: [D loss: 0.688227, acc: 0.541016]  [A loss: 0.693616, acc: 0.546875]\n",
      "2586: [D loss: 0.708556, acc: 0.517578]  [A loss: 0.877437, acc: 0.156250]\n",
      "2587: [D loss: 0.673736, acc: 0.556641]  [A loss: 0.769665, acc: 0.332031]\n",
      "2588: [D loss: 0.691965, acc: 0.537109]  [A loss: 0.839342, acc: 0.238281]\n",
      "2589: [D loss: 0.693261, acc: 0.533203]  [A loss: 0.822854, acc: 0.250000]\n",
      "2590: [D loss: 0.692846, acc: 0.552734]  [A loss: 0.781979, acc: 0.355469]\n",
      "2591: [D loss: 0.694193, acc: 0.542969]  [A loss: 0.782776, acc: 0.332031]\n",
      "2592: [D loss: 0.698784, acc: 0.519531]  [A loss: 0.815993, acc: 0.246094]\n",
      "2593: [D loss: 0.713506, acc: 0.507812]  [A loss: 0.870514, acc: 0.164062]\n",
      "2594: [D loss: 0.683886, acc: 0.583984]  [A loss: 0.764382, acc: 0.363281]\n",
      "2595: [D loss: 0.708487, acc: 0.494141]  [A loss: 0.967940, acc: 0.105469]\n",
      "2596: [D loss: 0.708918, acc: 0.494141]  [A loss: 0.822293, acc: 0.238281]\n",
      "2597: [D loss: 0.714566, acc: 0.496094]  [A loss: 0.811999, acc: 0.214844]\n",
      "2598: [D loss: 0.693364, acc: 0.537109]  [A loss: 0.845611, acc: 0.234375]\n",
      "2599: [D loss: 0.696971, acc: 0.511719]  [A loss: 0.798061, acc: 0.300781]\n",
      "2600: [D loss: 0.688631, acc: 0.554688]  [A loss: 0.864679, acc: 0.187500]\n",
      "2601: [D loss: 0.705531, acc: 0.513672]  [A loss: 0.879767, acc: 0.156250]\n",
      "2602: [D loss: 0.681902, acc: 0.562500]  [A loss: 0.754197, acc: 0.363281]\n",
      "2603: [D loss: 0.721774, acc: 0.523438]  [A loss: 0.948286, acc: 0.113281]\n",
      "2604: [D loss: 0.711241, acc: 0.500000]  [A loss: 0.743245, acc: 0.410156]\n",
      "2605: [D loss: 0.705447, acc: 0.521484]  [A loss: 0.977014, acc: 0.089844]\n",
      "2606: [D loss: 0.708050, acc: 0.482422]  [A loss: 0.630723, acc: 0.640625]\n",
      "2607: [D loss: 0.730992, acc: 0.494141]  [A loss: 0.980728, acc: 0.054688]\n",
      "2608: [D loss: 0.701788, acc: 0.509766]  [A loss: 0.707884, acc: 0.445312]\n",
      "2609: [D loss: 0.720043, acc: 0.515625]  [A loss: 0.877465, acc: 0.179688]\n",
      "2610: [D loss: 0.692144, acc: 0.554688]  [A loss: 0.747972, acc: 0.390625]\n",
      "2611: [D loss: 0.689200, acc: 0.533203]  [A loss: 0.823878, acc: 0.246094]\n",
      "2612: [D loss: 0.697419, acc: 0.539062]  [A loss: 0.874573, acc: 0.175781]\n",
      "2613: [D loss: 0.701668, acc: 0.523438]  [A loss: 0.780870, acc: 0.296875]\n",
      "2614: [D loss: 0.700747, acc: 0.509766]  [A loss: 0.837913, acc: 0.238281]\n",
      "2615: [D loss: 0.696727, acc: 0.529297]  [A loss: 0.783771, acc: 0.304688]\n",
      "2616: [D loss: 0.698551, acc: 0.527344]  [A loss: 0.887565, acc: 0.167969]\n",
      "2617: [D loss: 0.686309, acc: 0.556641]  [A loss: 0.780185, acc: 0.324219]\n",
      "2618: [D loss: 0.711105, acc: 0.484375]  [A loss: 0.866401, acc: 0.187500]\n",
      "2619: [D loss: 0.692118, acc: 0.525391]  [A loss: 0.793251, acc: 0.289062]\n",
      "2620: [D loss: 0.700255, acc: 0.533203]  [A loss: 0.870056, acc: 0.199219]\n",
      "2621: [D loss: 0.691254, acc: 0.535156]  [A loss: 0.718391, acc: 0.457031]\n",
      "2622: [D loss: 0.710582, acc: 0.539062]  [A loss: 1.030029, acc: 0.046875]\n",
      "2623: [D loss: 0.687602, acc: 0.525391]  [A loss: 0.682351, acc: 0.554688]\n",
      "2624: [D loss: 0.710365, acc: 0.531250]  [A loss: 0.880019, acc: 0.125000]\n",
      "2625: [D loss: 0.705924, acc: 0.521484]  [A loss: 0.787755, acc: 0.296875]\n",
      "2626: [D loss: 0.698094, acc: 0.548828]  [A loss: 0.882667, acc: 0.167969]\n",
      "2627: [D loss: 0.682831, acc: 0.566406]  [A loss: 0.771353, acc: 0.343750]\n",
      "2628: [D loss: 0.703174, acc: 0.496094]  [A loss: 0.933488, acc: 0.117188]\n",
      "2629: [D loss: 0.699164, acc: 0.535156]  [A loss: 0.714100, acc: 0.457031]\n",
      "2630: [D loss: 0.717912, acc: 0.494141]  [A loss: 0.934871, acc: 0.121094]\n",
      "2631: [D loss: 0.693970, acc: 0.513672]  [A loss: 0.736425, acc: 0.437500]\n",
      "2632: [D loss: 0.701502, acc: 0.525391]  [A loss: 0.942752, acc: 0.121094]\n",
      "2633: [D loss: 0.695928, acc: 0.515625]  [A loss: 0.742615, acc: 0.429688]\n",
      "2634: [D loss: 0.712265, acc: 0.500000]  [A loss: 0.913582, acc: 0.125000]\n",
      "2635: [D loss: 0.682066, acc: 0.544922]  [A loss: 0.706879, acc: 0.500000]\n",
      "2636: [D loss: 0.731563, acc: 0.484375]  [A loss: 0.987394, acc: 0.066406]\n",
      "2637: [D loss: 0.699481, acc: 0.515625]  [A loss: 0.698041, acc: 0.503906]\n",
      "2638: [D loss: 0.727729, acc: 0.513672]  [A loss: 0.916096, acc: 0.136719]\n",
      "2639: [D loss: 0.709343, acc: 0.511719]  [A loss: 0.781445, acc: 0.332031]\n",
      "2640: [D loss: 0.711947, acc: 0.492188]  [A loss: 0.852962, acc: 0.167969]\n",
      "2641: [D loss: 0.715170, acc: 0.474609]  [A loss: 0.826849, acc: 0.226562]\n",
      "2642: [D loss: 0.705438, acc: 0.519531]  [A loss: 0.781599, acc: 0.300781]\n",
      "2643: [D loss: 0.696131, acc: 0.539062]  [A loss: 0.833030, acc: 0.210938]\n",
      "2644: [D loss: 0.699437, acc: 0.527344]  [A loss: 0.777415, acc: 0.347656]\n",
      "2645: [D loss: 0.700110, acc: 0.511719]  [A loss: 0.825384, acc: 0.234375]\n",
      "2646: [D loss: 0.702651, acc: 0.544922]  [A loss: 0.782348, acc: 0.289062]\n",
      "2647: [D loss: 0.695823, acc: 0.541016]  [A loss: 0.928203, acc: 0.097656]\n",
      "2648: [D loss: 0.701968, acc: 0.505859]  [A loss: 0.711530, acc: 0.480469]\n",
      "2649: [D loss: 0.711052, acc: 0.533203]  [A loss: 0.897915, acc: 0.136719]\n",
      "2650: [D loss: 0.697984, acc: 0.525391]  [A loss: 0.671005, acc: 0.593750]\n",
      "2651: [D loss: 0.739147, acc: 0.498047]  [A loss: 1.023289, acc: 0.054688]\n",
      "2652: [D loss: 0.699652, acc: 0.509766]  [A loss: 0.713726, acc: 0.507812]\n",
      "2653: [D loss: 0.732306, acc: 0.511719]  [A loss: 0.899027, acc: 0.101562]\n",
      "2654: [D loss: 0.702125, acc: 0.517578]  [A loss: 0.808790, acc: 0.265625]\n",
      "2655: [D loss: 0.707316, acc: 0.546875]  [A loss: 0.821119, acc: 0.222656]\n",
      "2656: [D loss: 0.688715, acc: 0.546875]  [A loss: 0.766672, acc: 0.355469]\n",
      "2657: [D loss: 0.702953, acc: 0.525391]  [A loss: 0.860794, acc: 0.191406]\n",
      "2658: [D loss: 0.697234, acc: 0.509766]  [A loss: 0.849783, acc: 0.179688]\n",
      "2659: [D loss: 0.684875, acc: 0.560547]  [A loss: 0.798609, acc: 0.250000]\n",
      "2660: [D loss: 0.712793, acc: 0.523438]  [A loss: 0.886776, acc: 0.136719]\n",
      "2661: [D loss: 0.691514, acc: 0.546875]  [A loss: 0.749137, acc: 0.375000]\n",
      "2662: [D loss: 0.696400, acc: 0.529297]  [A loss: 0.891453, acc: 0.144531]\n",
      "2663: [D loss: 0.696190, acc: 0.537109]  [A loss: 0.777500, acc: 0.292969]\n",
      "2664: [D loss: 0.696468, acc: 0.539062]  [A loss: 0.807599, acc: 0.292969]\n",
      "2665: [D loss: 0.699220, acc: 0.527344]  [A loss: 0.834850, acc: 0.203125]\n",
      "2666: [D loss: 0.689316, acc: 0.539062]  [A loss: 0.819624, acc: 0.187500]\n",
      "2667: [D loss: 0.695489, acc: 0.519531]  [A loss: 0.873960, acc: 0.144531]\n",
      "2668: [D loss: 0.690759, acc: 0.535156]  [A loss: 0.767038, acc: 0.339844]\n",
      "2669: [D loss: 0.707705, acc: 0.529297]  [A loss: 0.972921, acc: 0.085938]\n",
      "2670: [D loss: 0.704487, acc: 0.501953]  [A loss: 0.679647, acc: 0.539062]\n",
      "2671: [D loss: 0.710244, acc: 0.517578]  [A loss: 0.931449, acc: 0.082031]\n",
      "2672: [D loss: 0.698412, acc: 0.511719]  [A loss: 0.627679, acc: 0.695312]\n",
      "2673: [D loss: 0.752359, acc: 0.484375]  [A loss: 1.028117, acc: 0.031250]\n",
      "2674: [D loss: 0.692458, acc: 0.546875]  [A loss: 0.715464, acc: 0.433594]\n",
      "2675: [D loss: 0.705574, acc: 0.544922]  [A loss: 0.836146, acc: 0.242188]\n",
      "2676: [D loss: 0.711328, acc: 0.511719]  [A loss: 0.781590, acc: 0.308594]\n",
      "2677: [D loss: 0.699189, acc: 0.523438]  [A loss: 0.815411, acc: 0.273438]\n",
      "2678: [D loss: 0.682296, acc: 0.568359]  [A loss: 0.774036, acc: 0.324219]\n",
      "2679: [D loss: 0.712012, acc: 0.513672]  [A loss: 0.936079, acc: 0.085938]\n",
      "2680: [D loss: 0.691610, acc: 0.507812]  [A loss: 0.757724, acc: 0.375000]\n",
      "2681: [D loss: 0.694999, acc: 0.552734]  [A loss: 0.861317, acc: 0.183594]\n",
      "2682: [D loss: 0.680955, acc: 0.564453]  [A loss: 0.777610, acc: 0.312500]\n",
      "2683: [D loss: 0.690424, acc: 0.568359]  [A loss: 0.851512, acc: 0.175781]\n",
      "2684: [D loss: 0.707839, acc: 0.527344]  [A loss: 0.766557, acc: 0.324219]\n",
      "2685: [D loss: 0.707938, acc: 0.529297]  [A loss: 0.920588, acc: 0.113281]\n",
      "2686: [D loss: 0.694285, acc: 0.544922]  [A loss: 0.731246, acc: 0.429688]\n",
      "2687: [D loss: 0.693674, acc: 0.546875]  [A loss: 0.818490, acc: 0.261719]\n",
      "2688: [D loss: 0.714082, acc: 0.484375]  [A loss: 0.927197, acc: 0.109375]\n",
      "2689: [D loss: 0.692659, acc: 0.513672]  [A loss: 0.767876, acc: 0.339844]\n",
      "2690: [D loss: 0.700304, acc: 0.552734]  [A loss: 0.892064, acc: 0.136719]\n",
      "2691: [D loss: 0.702034, acc: 0.505859]  [A loss: 0.713678, acc: 0.480469]\n",
      "2692: [D loss: 0.699033, acc: 0.529297]  [A loss: 0.921644, acc: 0.101562]\n",
      "2693: [D loss: 0.695377, acc: 0.533203]  [A loss: 0.686208, acc: 0.542969]\n",
      "2694: [D loss: 0.717322, acc: 0.521484]  [A loss: 0.949002, acc: 0.117188]\n",
      "2695: [D loss: 0.688838, acc: 0.531250]  [A loss: 0.777724, acc: 0.328125]\n",
      "2696: [D loss: 0.697145, acc: 0.531250]  [A loss: 0.809803, acc: 0.265625]\n",
      "2697: [D loss: 0.695941, acc: 0.550781]  [A loss: 0.804430, acc: 0.300781]\n",
      "2698: [D loss: 0.695569, acc: 0.541016]  [A loss: 0.862901, acc: 0.183594]\n",
      "2699: [D loss: 0.708572, acc: 0.515625]  [A loss: 0.807930, acc: 0.238281]\n",
      "2700: [D loss: 0.701731, acc: 0.527344]  [A loss: 0.905811, acc: 0.152344]\n",
      "2701: [D loss: 0.689625, acc: 0.531250]  [A loss: 0.816138, acc: 0.250000]\n",
      "2702: [D loss: 0.713590, acc: 0.484375]  [A loss: 0.846537, acc: 0.207031]\n",
      "2703: [D loss: 0.696647, acc: 0.537109]  [A loss: 0.878323, acc: 0.191406]\n",
      "2704: [D loss: 0.696315, acc: 0.535156]  [A loss: 0.783043, acc: 0.316406]\n",
      "2705: [D loss: 0.694918, acc: 0.527344]  [A loss: 0.850391, acc: 0.238281]\n",
      "2706: [D loss: 0.704544, acc: 0.503906]  [A loss: 0.826679, acc: 0.214844]\n",
      "2707: [D loss: 0.688772, acc: 0.546875]  [A loss: 0.903033, acc: 0.136719]\n",
      "2708: [D loss: 0.690937, acc: 0.525391]  [A loss: 0.743834, acc: 0.386719]\n",
      "2709: [D loss: 0.707960, acc: 0.490234]  [A loss: 0.930794, acc: 0.109375]\n",
      "2710: [D loss: 0.698442, acc: 0.527344]  [A loss: 0.805885, acc: 0.230469]\n",
      "2711: [D loss: 0.704742, acc: 0.525391]  [A loss: 0.868768, acc: 0.171875]\n",
      "2712: [D loss: 0.694682, acc: 0.517578]  [A loss: 0.796257, acc: 0.304688]\n",
      "2713: [D loss: 0.704641, acc: 0.525391]  [A loss: 0.925040, acc: 0.105469]\n",
      "2714: [D loss: 0.700995, acc: 0.515625]  [A loss: 0.748411, acc: 0.375000]\n",
      "2715: [D loss: 0.719087, acc: 0.535156]  [A loss: 1.057536, acc: 0.058594]\n",
      "2716: [D loss: 0.706335, acc: 0.515625]  [A loss: 0.614473, acc: 0.726562]\n",
      "2717: [D loss: 0.727541, acc: 0.507812]  [A loss: 0.946304, acc: 0.109375]\n",
      "2718: [D loss: 0.683815, acc: 0.535156]  [A loss: 0.739692, acc: 0.410156]\n",
      "2719: [D loss: 0.724631, acc: 0.496094]  [A loss: 0.854809, acc: 0.148438]\n",
      "2720: [D loss: 0.701603, acc: 0.511719]  [A loss: 0.772067, acc: 0.367188]\n",
      "2721: [D loss: 0.701918, acc: 0.537109]  [A loss: 0.890580, acc: 0.148438]\n",
      "2722: [D loss: 0.687030, acc: 0.552734]  [A loss: 0.818063, acc: 0.273438]\n",
      "2723: [D loss: 0.703448, acc: 0.529297]  [A loss: 0.825318, acc: 0.253906]\n",
      "2724: [D loss: 0.702263, acc: 0.521484]  [A loss: 0.805086, acc: 0.269531]\n",
      "2725: [D loss: 0.699829, acc: 0.509766]  [A loss: 0.899182, acc: 0.113281]\n",
      "2726: [D loss: 0.678496, acc: 0.603516]  [A loss: 0.691059, acc: 0.566406]\n",
      "2727: [D loss: 0.696391, acc: 0.537109]  [A loss: 0.943336, acc: 0.093750]\n",
      "2728: [D loss: 0.690017, acc: 0.523438]  [A loss: 0.766416, acc: 0.332031]\n",
      "2729: [D loss: 0.695286, acc: 0.552734]  [A loss: 0.877780, acc: 0.195312]\n",
      "2730: [D loss: 0.698767, acc: 0.515625]  [A loss: 0.700302, acc: 0.535156]\n",
      "2731: [D loss: 0.714451, acc: 0.501953]  [A loss: 0.901299, acc: 0.179688]\n",
      "2732: [D loss: 0.699506, acc: 0.519531]  [A loss: 0.727906, acc: 0.417969]\n",
      "2733: [D loss: 0.685189, acc: 0.574219]  [A loss: 0.872114, acc: 0.207031]\n",
      "2734: [D loss: 0.692506, acc: 0.556641]  [A loss: 0.768317, acc: 0.351562]\n",
      "2735: [D loss: 0.703004, acc: 0.517578]  [A loss: 0.924128, acc: 0.085938]\n",
      "2736: [D loss: 0.696123, acc: 0.498047]  [A loss: 0.719709, acc: 0.460938]\n",
      "2737: [D loss: 0.707008, acc: 0.541016]  [A loss: 1.021726, acc: 0.050781]\n",
      "2738: [D loss: 0.701572, acc: 0.523438]  [A loss: 0.659715, acc: 0.589844]\n",
      "2739: [D loss: 0.741987, acc: 0.501953]  [A loss: 0.913145, acc: 0.132812]\n",
      "2740: [D loss: 0.695366, acc: 0.513672]  [A loss: 0.793719, acc: 0.304688]\n",
      "2741: [D loss: 0.705722, acc: 0.509766]  [A loss: 0.884004, acc: 0.144531]\n",
      "2742: [D loss: 0.693071, acc: 0.544922]  [A loss: 0.767116, acc: 0.312500]\n",
      "2743: [D loss: 0.704636, acc: 0.521484]  [A loss: 0.823689, acc: 0.250000]\n",
      "2744: [D loss: 0.696260, acc: 0.529297]  [A loss: 0.770807, acc: 0.312500]\n",
      "2745: [D loss: 0.692695, acc: 0.548828]  [A loss: 0.860878, acc: 0.199219]\n",
      "2746: [D loss: 0.704951, acc: 0.509766]  [A loss: 0.829918, acc: 0.253906]\n",
      "2747: [D loss: 0.703163, acc: 0.494141]  [A loss: 0.783138, acc: 0.289062]\n",
      "2748: [D loss: 0.699956, acc: 0.529297]  [A loss: 0.871827, acc: 0.195312]\n",
      "2749: [D loss: 0.695313, acc: 0.560547]  [A loss: 0.750806, acc: 0.410156]\n",
      "2750: [D loss: 0.720293, acc: 0.529297]  [A loss: 0.948191, acc: 0.097656]\n",
      "2751: [D loss: 0.704359, acc: 0.503906]  [A loss: 0.703246, acc: 0.515625]\n",
      "2752: [D loss: 0.714229, acc: 0.509766]  [A loss: 0.976231, acc: 0.078125]\n",
      "2753: [D loss: 0.700643, acc: 0.529297]  [A loss: 0.683172, acc: 0.601562]\n",
      "2754: [D loss: 0.719426, acc: 0.494141]  [A loss: 0.850863, acc: 0.171875]\n",
      "2755: [D loss: 0.686797, acc: 0.541016]  [A loss: 0.765792, acc: 0.371094]\n",
      "2756: [D loss: 0.707053, acc: 0.509766]  [A loss: 0.841972, acc: 0.234375]\n",
      "2757: [D loss: 0.697056, acc: 0.527344]  [A loss: 0.766558, acc: 0.324219]\n",
      "2758: [D loss: 0.702191, acc: 0.527344]  [A loss: 0.897891, acc: 0.179688]\n",
      "2759: [D loss: 0.700522, acc: 0.531250]  [A loss: 0.765950, acc: 0.347656]\n",
      "2760: [D loss: 0.713806, acc: 0.511719]  [A loss: 0.847270, acc: 0.214844]\n",
      "2761: [D loss: 0.698223, acc: 0.519531]  [A loss: 0.826691, acc: 0.214844]\n",
      "2762: [D loss: 0.695508, acc: 0.521484]  [A loss: 0.916178, acc: 0.101562]\n",
      "2763: [D loss: 0.698773, acc: 0.511719]  [A loss: 0.755705, acc: 0.367188]\n",
      "2764: [D loss: 0.726665, acc: 0.478516]  [A loss: 0.978340, acc: 0.089844]\n",
      "2765: [D loss: 0.703438, acc: 0.525391]  [A loss: 0.669993, acc: 0.585938]\n",
      "2766: [D loss: 0.725913, acc: 0.503906]  [A loss: 0.951887, acc: 0.070312]\n",
      "2767: [D loss: 0.706750, acc: 0.492188]  [A loss: 0.705699, acc: 0.488281]\n",
      "2768: [D loss: 0.712901, acc: 0.521484]  [A loss: 0.867493, acc: 0.144531]\n",
      "2769: [D loss: 0.708003, acc: 0.494141]  [A loss: 0.732972, acc: 0.406250]\n",
      "2770: [D loss: 0.702384, acc: 0.531250]  [A loss: 0.859458, acc: 0.167969]\n",
      "2771: [D loss: 0.687588, acc: 0.560547]  [A loss: 0.763492, acc: 0.328125]\n",
      "2772: [D loss: 0.715055, acc: 0.476562]  [A loss: 0.867966, acc: 0.164062]\n",
      "2773: [D loss: 0.699890, acc: 0.527344]  [A loss: 0.750751, acc: 0.433594]\n",
      "2774: [D loss: 0.700477, acc: 0.541016]  [A loss: 0.945015, acc: 0.093750]\n",
      "2775: [D loss: 0.704060, acc: 0.507812]  [A loss: 0.708013, acc: 0.507812]\n",
      "2776: [D loss: 0.713581, acc: 0.496094]  [A loss: 0.903445, acc: 0.152344]\n",
      "2777: [D loss: 0.696576, acc: 0.539062]  [A loss: 0.766050, acc: 0.351562]\n",
      "2778: [D loss: 0.692346, acc: 0.558594]  [A loss: 0.836574, acc: 0.226562]\n",
      "2779: [D loss: 0.709873, acc: 0.480469]  [A loss: 0.842788, acc: 0.167969]\n",
      "2780: [D loss: 0.696525, acc: 0.523438]  [A loss: 0.822177, acc: 0.222656]\n",
      "2781: [D loss: 0.677434, acc: 0.560547]  [A loss: 0.818876, acc: 0.226562]\n",
      "2782: [D loss: 0.704285, acc: 0.517578]  [A loss: 0.776327, acc: 0.339844]\n",
      "2783: [D loss: 0.697552, acc: 0.574219]  [A loss: 0.908849, acc: 0.132812]\n",
      "2784: [D loss: 0.693799, acc: 0.541016]  [A loss: 0.812317, acc: 0.261719]\n",
      "2785: [D loss: 0.696943, acc: 0.521484]  [A loss: 0.842410, acc: 0.253906]\n",
      "2786: [D loss: 0.700216, acc: 0.535156]  [A loss: 0.790931, acc: 0.281250]\n",
      "2787: [D loss: 0.724448, acc: 0.482422]  [A loss: 1.072493, acc: 0.035156]\n",
      "2788: [D loss: 0.700660, acc: 0.531250]  [A loss: 0.638828, acc: 0.660156]\n",
      "2789: [D loss: 0.739441, acc: 0.498047]  [A loss: 0.998621, acc: 0.058594]\n",
      "2790: [D loss: 0.693223, acc: 0.539062]  [A loss: 0.705429, acc: 0.500000]\n",
      "2791: [D loss: 0.728526, acc: 0.511719]  [A loss: 0.933038, acc: 0.109375]\n",
      "2792: [D loss: 0.686563, acc: 0.548828]  [A loss: 0.729114, acc: 0.441406]\n",
      "2793: [D loss: 0.709108, acc: 0.531250]  [A loss: 0.836807, acc: 0.199219]\n",
      "2794: [D loss: 0.683298, acc: 0.541016]  [A loss: 0.800909, acc: 0.281250]\n",
      "2795: [D loss: 0.701462, acc: 0.533203]  [A loss: 0.826192, acc: 0.265625]\n",
      "2796: [D loss: 0.712909, acc: 0.513672]  [A loss: 0.837912, acc: 0.207031]\n",
      "2797: [D loss: 0.704005, acc: 0.494141]  [A loss: 0.779315, acc: 0.332031]\n",
      "2798: [D loss: 0.704871, acc: 0.519531]  [A loss: 0.855746, acc: 0.167969]\n",
      "2799: [D loss: 0.699040, acc: 0.519531]  [A loss: 0.829755, acc: 0.246094]\n",
      "2800: [D loss: 0.705816, acc: 0.523438]  [A loss: 0.807787, acc: 0.292969]\n",
      "2801: [D loss: 0.704737, acc: 0.505859]  [A loss: 0.804803, acc: 0.238281]\n",
      "2802: [D loss: 0.695084, acc: 0.519531]  [A loss: 0.886673, acc: 0.109375]\n",
      "2803: [D loss: 0.694370, acc: 0.542969]  [A loss: 0.822331, acc: 0.257812]\n",
      "2804: [D loss: 0.696172, acc: 0.527344]  [A loss: 0.866642, acc: 0.195312]\n",
      "2805: [D loss: 0.699321, acc: 0.515625]  [A loss: 0.896125, acc: 0.144531]\n",
      "2806: [D loss: 0.711889, acc: 0.492188]  [A loss: 0.797910, acc: 0.312500]\n",
      "2807: [D loss: 0.693594, acc: 0.519531]  [A loss: 0.881824, acc: 0.160156]\n",
      "2808: [D loss: 0.701914, acc: 0.531250]  [A loss: 0.748544, acc: 0.382812]\n",
      "2809: [D loss: 0.723773, acc: 0.519531]  [A loss: 1.172215, acc: 0.019531]\n",
      "2810: [D loss: 0.724713, acc: 0.488281]  [A loss: 0.590535, acc: 0.777344]\n",
      "2811: [D loss: 0.772269, acc: 0.482422]  [A loss: 0.910431, acc: 0.128906]\n",
      "2812: [D loss: 0.712148, acc: 0.470703]  [A loss: 0.730546, acc: 0.433594]\n",
      "2813: [D loss: 0.724271, acc: 0.507812]  [A loss: 0.939903, acc: 0.093750]\n",
      "2814: [D loss: 0.686363, acc: 0.554688]  [A loss: 0.693310, acc: 0.503906]\n",
      "2815: [D loss: 0.711783, acc: 0.501953]  [A loss: 0.857165, acc: 0.164062]\n",
      "2816: [D loss: 0.693819, acc: 0.539062]  [A loss: 0.762282, acc: 0.343750]\n",
      "2817: [D loss: 0.704132, acc: 0.505859]  [A loss: 0.854909, acc: 0.164062]\n",
      "2818: [D loss: 0.684078, acc: 0.550781]  [A loss: 0.745474, acc: 0.378906]\n",
      "2819: [D loss: 0.694965, acc: 0.541016]  [A loss: 0.860868, acc: 0.214844]\n",
      "2820: [D loss: 0.703488, acc: 0.525391]  [A loss: 0.787011, acc: 0.316406]\n",
      "2821: [D loss: 0.708319, acc: 0.494141]  [A loss: 0.824599, acc: 0.210938]\n",
      "2822: [D loss: 0.688603, acc: 0.525391]  [A loss: 0.815701, acc: 0.257812]\n",
      "2823: [D loss: 0.707247, acc: 0.501953]  [A loss: 0.834356, acc: 0.207031]\n",
      "2824: [D loss: 0.700147, acc: 0.519531]  [A loss: 0.820640, acc: 0.277344]\n",
      "2825: [D loss: 0.702582, acc: 0.525391]  [A loss: 0.850171, acc: 0.179688]\n",
      "2826: [D loss: 0.703736, acc: 0.521484]  [A loss: 0.763327, acc: 0.339844]\n",
      "2827: [D loss: 0.714421, acc: 0.515625]  [A loss: 0.958409, acc: 0.097656]\n",
      "2828: [D loss: 0.703563, acc: 0.492188]  [A loss: 0.700892, acc: 0.515625]\n",
      "2829: [D loss: 0.706913, acc: 0.511719]  [A loss: 0.886945, acc: 0.156250]\n",
      "2830: [D loss: 0.691727, acc: 0.531250]  [A loss: 0.718478, acc: 0.457031]\n",
      "2831: [D loss: 0.700690, acc: 0.541016]  [A loss: 0.901942, acc: 0.125000]\n",
      "2832: [D loss: 0.691915, acc: 0.546875]  [A loss: 0.726950, acc: 0.468750]\n",
      "2833: [D loss: 0.709258, acc: 0.525391]  [A loss: 0.835649, acc: 0.210938]\n",
      "2834: [D loss: 0.694336, acc: 0.513672]  [A loss: 0.764813, acc: 0.390625]\n",
      "2835: [D loss: 0.717286, acc: 0.503906]  [A loss: 0.993848, acc: 0.062500]\n",
      "2836: [D loss: 0.706605, acc: 0.529297]  [A loss: 0.695845, acc: 0.531250]\n",
      "2837: [D loss: 0.713859, acc: 0.511719]  [A loss: 0.926943, acc: 0.105469]\n",
      "2838: [D loss: 0.692361, acc: 0.523438]  [A loss: 0.707990, acc: 0.480469]\n",
      "2839: [D loss: 0.711737, acc: 0.496094]  [A loss: 0.900829, acc: 0.140625]\n",
      "2840: [D loss: 0.699061, acc: 0.527344]  [A loss: 0.767972, acc: 0.347656]\n",
      "2841: [D loss: 0.709778, acc: 0.511719]  [A loss: 0.876290, acc: 0.195312]\n",
      "2842: [D loss: 0.703220, acc: 0.523438]  [A loss: 0.877464, acc: 0.183594]\n",
      "2843: [D loss: 0.694796, acc: 0.537109]  [A loss: 0.773512, acc: 0.339844]\n",
      "2844: [D loss: 0.705909, acc: 0.501953]  [A loss: 0.833652, acc: 0.218750]\n",
      "2845: [D loss: 0.694382, acc: 0.552734]  [A loss: 0.860928, acc: 0.156250]\n",
      "2846: [D loss: 0.710797, acc: 0.464844]  [A loss: 0.762882, acc: 0.351562]\n",
      "2847: [D loss: 0.708457, acc: 0.515625]  [A loss: 0.918963, acc: 0.164062]\n",
      "2848: [D loss: 0.696401, acc: 0.523438]  [A loss: 0.764816, acc: 0.335938]\n",
      "2849: [D loss: 0.720062, acc: 0.505859]  [A loss: 0.942741, acc: 0.101562]\n",
      "2850: [D loss: 0.696916, acc: 0.523438]  [A loss: 0.730612, acc: 0.472656]\n",
      "2851: [D loss: 0.701509, acc: 0.517578]  [A loss: 0.869291, acc: 0.195312]\n",
      "2852: [D loss: 0.692006, acc: 0.535156]  [A loss: 0.741563, acc: 0.398438]\n",
      "2853: [D loss: 0.700566, acc: 0.525391]  [A loss: 0.873480, acc: 0.156250]\n",
      "2854: [D loss: 0.697615, acc: 0.509766]  [A loss: 0.793646, acc: 0.269531]\n",
      "2855: [D loss: 0.708414, acc: 0.500000]  [A loss: 0.938499, acc: 0.140625]\n",
      "2856: [D loss: 0.697061, acc: 0.505859]  [A loss: 0.683872, acc: 0.574219]\n",
      "2857: [D loss: 0.727790, acc: 0.509766]  [A loss: 0.970496, acc: 0.082031]\n",
      "2858: [D loss: 0.710432, acc: 0.505859]  [A loss: 0.777120, acc: 0.308594]\n",
      "2859: [D loss: 0.689271, acc: 0.552734]  [A loss: 0.804097, acc: 0.292969]\n",
      "2860: [D loss: 0.706989, acc: 0.531250]  [A loss: 0.877142, acc: 0.175781]\n",
      "2861: [D loss: 0.697850, acc: 0.515625]  [A loss: 0.811078, acc: 0.269531]\n",
      "2862: [D loss: 0.721526, acc: 0.505859]  [A loss: 0.865801, acc: 0.191406]\n",
      "2863: [D loss: 0.700968, acc: 0.537109]  [A loss: 0.899948, acc: 0.167969]\n",
      "2864: [D loss: 0.692425, acc: 0.521484]  [A loss: 0.756679, acc: 0.371094]\n",
      "2865: [D loss: 0.716196, acc: 0.501953]  [A loss: 1.034292, acc: 0.054688]\n",
      "2866: [D loss: 0.697749, acc: 0.523438]  [A loss: 0.628262, acc: 0.640625]\n",
      "2867: [D loss: 0.725386, acc: 0.527344]  [A loss: 1.002360, acc: 0.078125]\n",
      "2868: [D loss: 0.689127, acc: 0.558594]  [A loss: 0.708527, acc: 0.468750]\n",
      "2869: [D loss: 0.717917, acc: 0.498047]  [A loss: 0.847379, acc: 0.218750]\n",
      "2870: [D loss: 0.703659, acc: 0.529297]  [A loss: 0.795796, acc: 0.289062]\n",
      "2871: [D loss: 0.693881, acc: 0.519531]  [A loss: 0.799769, acc: 0.285156]\n",
      "2872: [D loss: 0.689243, acc: 0.542969]  [A loss: 0.842848, acc: 0.171875]\n",
      "2873: [D loss: 0.694578, acc: 0.556641]  [A loss: 0.761346, acc: 0.355469]\n",
      "2874: [D loss: 0.741512, acc: 0.470703]  [A loss: 0.894215, acc: 0.156250]\n",
      "2875: [D loss: 0.707868, acc: 0.541016]  [A loss: 0.842995, acc: 0.187500]\n",
      "2876: [D loss: 0.718473, acc: 0.484375]  [A loss: 0.830234, acc: 0.246094]\n",
      "2877: [D loss: 0.700800, acc: 0.529297]  [A loss: 0.843706, acc: 0.195312]\n",
      "2878: [D loss: 0.708211, acc: 0.492188]  [A loss: 0.824222, acc: 0.238281]\n",
      "2879: [D loss: 0.699167, acc: 0.513672]  [A loss: 0.850120, acc: 0.218750]\n",
      "2880: [D loss: 0.703135, acc: 0.539062]  [A loss: 0.881777, acc: 0.167969]\n",
      "2881: [D loss: 0.688424, acc: 0.525391]  [A loss: 0.751559, acc: 0.375000]\n",
      "2882: [D loss: 0.697349, acc: 0.533203]  [A loss: 0.984668, acc: 0.082031]\n",
      "2883: [D loss: 0.683398, acc: 0.558594]  [A loss: 0.638507, acc: 0.636719]\n",
      "2884: [D loss: 0.736070, acc: 0.509766]  [A loss: 0.944169, acc: 0.082031]\n",
      "2885: [D loss: 0.702818, acc: 0.511719]  [A loss: 0.678582, acc: 0.542969]\n",
      "2886: [D loss: 0.738145, acc: 0.498047]  [A loss: 1.046446, acc: 0.042969]\n",
      "2887: [D loss: 0.710447, acc: 0.505859]  [A loss: 0.687998, acc: 0.523438]\n",
      "2888: [D loss: 0.709425, acc: 0.515625]  [A loss: 0.828576, acc: 0.207031]\n",
      "2889: [D loss: 0.705018, acc: 0.501953]  [A loss: 0.739343, acc: 0.375000]\n",
      "2890: [D loss: 0.710063, acc: 0.523438]  [A loss: 0.829084, acc: 0.214844]\n",
      "2891: [D loss: 0.706401, acc: 0.488281]  [A loss: 0.732684, acc: 0.437500]\n",
      "2892: [D loss: 0.695694, acc: 0.537109]  [A loss: 0.843007, acc: 0.195312]\n",
      "2893: [D loss: 0.686176, acc: 0.546875]  [A loss: 0.742386, acc: 0.390625]\n",
      "2894: [D loss: 0.715576, acc: 0.509766]  [A loss: 0.939470, acc: 0.085938]\n",
      "2895: [D loss: 0.689804, acc: 0.560547]  [A loss: 0.731223, acc: 0.425781]\n",
      "2896: [D loss: 0.710737, acc: 0.515625]  [A loss: 0.896161, acc: 0.125000]\n",
      "2897: [D loss: 0.696980, acc: 0.542969]  [A loss: 0.738730, acc: 0.414062]\n",
      "2898: [D loss: 0.710720, acc: 0.500000]  [A loss: 0.862426, acc: 0.179688]\n",
      "2899: [D loss: 0.684932, acc: 0.576172]  [A loss: 0.775579, acc: 0.308594]\n",
      "2900: [D loss: 0.719583, acc: 0.460938]  [A loss: 0.868436, acc: 0.117188]\n",
      "2901: [D loss: 0.703194, acc: 0.505859]  [A loss: 0.766974, acc: 0.339844]\n",
      "2902: [D loss: 0.706017, acc: 0.501953]  [A loss: 0.852595, acc: 0.195312]\n",
      "2903: [D loss: 0.691137, acc: 0.531250]  [A loss: 0.754649, acc: 0.355469]\n",
      "2904: [D loss: 0.729579, acc: 0.509766]  [A loss: 1.032901, acc: 0.039062]\n",
      "2905: [D loss: 0.687832, acc: 0.560547]  [A loss: 0.672206, acc: 0.558594]\n",
      "2906: [D loss: 0.722078, acc: 0.486328]  [A loss: 0.895199, acc: 0.121094]\n",
      "2907: [D loss: 0.693068, acc: 0.533203]  [A loss: 0.736713, acc: 0.394531]\n",
      "2908: [D loss: 0.707502, acc: 0.507812]  [A loss: 0.876813, acc: 0.183594]\n",
      "2909: [D loss: 0.693375, acc: 0.531250]  [A loss: 0.744642, acc: 0.414062]\n",
      "2910: [D loss: 0.708153, acc: 0.515625]  [A loss: 0.862854, acc: 0.175781]\n",
      "2911: [D loss: 0.697716, acc: 0.527344]  [A loss: 0.776473, acc: 0.343750]\n",
      "2912: [D loss: 0.712299, acc: 0.515625]  [A loss: 0.855438, acc: 0.164062]\n",
      "2913: [D loss: 0.698820, acc: 0.531250]  [A loss: 0.813715, acc: 0.257812]\n",
      "2914: [D loss: 0.701033, acc: 0.519531]  [A loss: 0.816238, acc: 0.253906]\n",
      "2915: [D loss: 0.701185, acc: 0.501953]  [A loss: 0.801030, acc: 0.273438]\n",
      "2916: [D loss: 0.698904, acc: 0.515625]  [A loss: 0.837465, acc: 0.214844]\n",
      "2917: [D loss: 0.689134, acc: 0.533203]  [A loss: 0.871863, acc: 0.156250]\n",
      "2918: [D loss: 0.685135, acc: 0.558594]  [A loss: 0.724271, acc: 0.445312]\n",
      "2919: [D loss: 0.701822, acc: 0.529297]  [A loss: 1.026011, acc: 0.046875]\n",
      "2920: [D loss: 0.705110, acc: 0.507812]  [A loss: 0.676307, acc: 0.554688]\n",
      "2921: [D loss: 0.717140, acc: 0.505859]  [A loss: 0.973798, acc: 0.050781]\n",
      "2922: [D loss: 0.693414, acc: 0.544922]  [A loss: 0.725004, acc: 0.441406]\n",
      "2923: [D loss: 0.701494, acc: 0.541016]  [A loss: 0.839480, acc: 0.214844]\n",
      "2924: [D loss: 0.699904, acc: 0.531250]  [A loss: 0.737377, acc: 0.406250]\n",
      "2925: [D loss: 0.709932, acc: 0.541016]  [A loss: 0.915262, acc: 0.105469]\n",
      "2926: [D loss: 0.697994, acc: 0.533203]  [A loss: 0.760897, acc: 0.343750]\n",
      "2927: [D loss: 0.707149, acc: 0.505859]  [A loss: 0.881235, acc: 0.132812]\n",
      "2928: [D loss: 0.689039, acc: 0.533203]  [A loss: 0.770464, acc: 0.375000]\n",
      "2929: [D loss: 0.704510, acc: 0.509766]  [A loss: 0.915990, acc: 0.121094]\n",
      "2930: [D loss: 0.704093, acc: 0.503906]  [A loss: 0.806369, acc: 0.289062]\n",
      "2931: [D loss: 0.720178, acc: 0.482422]  [A loss: 0.856671, acc: 0.179688]\n",
      "2932: [D loss: 0.714169, acc: 0.484375]  [A loss: 0.797799, acc: 0.300781]\n",
      "2933: [D loss: 0.691890, acc: 0.552734]  [A loss: 0.810630, acc: 0.265625]\n",
      "2934: [D loss: 0.718514, acc: 0.494141]  [A loss: 0.930948, acc: 0.125000]\n",
      "2935: [D loss: 0.688403, acc: 0.548828]  [A loss: 0.729218, acc: 0.421875]\n",
      "2936: [D loss: 0.696452, acc: 0.531250]  [A loss: 0.927822, acc: 0.105469]\n",
      "2937: [D loss: 0.688693, acc: 0.544922]  [A loss: 0.706634, acc: 0.496094]\n",
      "2938: [D loss: 0.723272, acc: 0.488281]  [A loss: 0.990482, acc: 0.074219]\n",
      "2939: [D loss: 0.691411, acc: 0.513672]  [A loss: 0.647669, acc: 0.613281]\n",
      "2940: [D loss: 0.720726, acc: 0.505859]  [A loss: 0.901906, acc: 0.128906]\n",
      "2941: [D loss: 0.699545, acc: 0.519531]  [A loss: 0.736026, acc: 0.437500]\n",
      "2942: [D loss: 0.703375, acc: 0.529297]  [A loss: 0.838961, acc: 0.195312]\n",
      "2943: [D loss: 0.704927, acc: 0.494141]  [A loss: 0.750768, acc: 0.335938]\n",
      "2944: [D loss: 0.704645, acc: 0.517578]  [A loss: 0.900730, acc: 0.156250]\n",
      "2945: [D loss: 0.705272, acc: 0.496094]  [A loss: 0.709397, acc: 0.468750]\n",
      "2946: [D loss: 0.710577, acc: 0.496094]  [A loss: 0.939375, acc: 0.125000]\n",
      "2947: [D loss: 0.699154, acc: 0.501953]  [A loss: 0.724937, acc: 0.433594]\n",
      "2948: [D loss: 0.717520, acc: 0.535156]  [A loss: 0.830368, acc: 0.250000]\n",
      "2949: [D loss: 0.697191, acc: 0.519531]  [A loss: 0.753567, acc: 0.382812]\n",
      "2950: [D loss: 0.705575, acc: 0.517578]  [A loss: 0.914309, acc: 0.132812]\n",
      "2951: [D loss: 0.690291, acc: 0.544922]  [A loss: 0.741777, acc: 0.425781]\n",
      "2952: [D loss: 0.702896, acc: 0.539062]  [A loss: 0.861445, acc: 0.152344]\n",
      "2953: [D loss: 0.686283, acc: 0.537109]  [A loss: 0.747506, acc: 0.378906]\n",
      "2954: [D loss: 0.707828, acc: 0.513672]  [A loss: 0.982504, acc: 0.089844]\n",
      "2955: [D loss: 0.706404, acc: 0.517578]  [A loss: 0.718220, acc: 0.472656]\n",
      "2956: [D loss: 0.708622, acc: 0.531250]  [A loss: 0.924444, acc: 0.097656]\n",
      "2957: [D loss: 0.702136, acc: 0.498047]  [A loss: 0.721555, acc: 0.421875]\n",
      "2958: [D loss: 0.702850, acc: 0.542969]  [A loss: 0.820694, acc: 0.218750]\n",
      "2959: [D loss: 0.704533, acc: 0.501953]  [A loss: 0.802464, acc: 0.246094]\n",
      "2960: [D loss: 0.703370, acc: 0.498047]  [A loss: 0.839386, acc: 0.207031]\n",
      "2961: [D loss: 0.698330, acc: 0.521484]  [A loss: 0.802226, acc: 0.296875]\n",
      "2962: [D loss: 0.697093, acc: 0.527344]  [A loss: 0.814932, acc: 0.257812]\n",
      "2963: [D loss: 0.695039, acc: 0.541016]  [A loss: 0.823827, acc: 0.230469]\n",
      "2964: [D loss: 0.689431, acc: 0.544922]  [A loss: 0.917999, acc: 0.152344]\n",
      "2965: [D loss: 0.698447, acc: 0.515625]  [A loss: 0.695672, acc: 0.507812]\n",
      "2966: [D loss: 0.709802, acc: 0.523438]  [A loss: 0.911573, acc: 0.140625]\n",
      "2967: [D loss: 0.711367, acc: 0.486328]  [A loss: 0.687371, acc: 0.566406]\n",
      "2968: [D loss: 0.724629, acc: 0.500000]  [A loss: 1.073443, acc: 0.031250]\n",
      "2969: [D loss: 0.700488, acc: 0.509766]  [A loss: 0.672736, acc: 0.578125]\n",
      "2970: [D loss: 0.729015, acc: 0.503906]  [A loss: 0.822840, acc: 0.214844]\n",
      "2971: [D loss: 0.700502, acc: 0.523438]  [A loss: 0.751549, acc: 0.371094]\n",
      "2972: [D loss: 0.707210, acc: 0.535156]  [A loss: 0.787798, acc: 0.328125]\n",
      "2973: [D loss: 0.707663, acc: 0.527344]  [A loss: 0.856542, acc: 0.191406]\n",
      "2974: [D loss: 0.701688, acc: 0.541016]  [A loss: 0.841191, acc: 0.230469]\n",
      "2975: [D loss: 0.705871, acc: 0.476562]  [A loss: 0.788176, acc: 0.289062]\n",
      "2976: [D loss: 0.696783, acc: 0.527344]  [A loss: 0.862825, acc: 0.179688]\n",
      "2977: [D loss: 0.704848, acc: 0.509766]  [A loss: 0.736179, acc: 0.414062]\n",
      "2978: [D loss: 0.709201, acc: 0.527344]  [A loss: 0.882764, acc: 0.179688]\n",
      "2979: [D loss: 0.698471, acc: 0.519531]  [A loss: 0.727134, acc: 0.441406]\n",
      "2980: [D loss: 0.717530, acc: 0.511719]  [A loss: 0.858854, acc: 0.203125]\n",
      "2981: [D loss: 0.711584, acc: 0.494141]  [A loss: 0.850813, acc: 0.160156]\n",
      "2982: [D loss: 0.701813, acc: 0.535156]  [A loss: 0.724708, acc: 0.449219]\n",
      "2983: [D loss: 0.710333, acc: 0.517578]  [A loss: 0.879933, acc: 0.128906]\n",
      "2984: [D loss: 0.692059, acc: 0.546875]  [A loss: 0.779519, acc: 0.281250]\n",
      "2985: [D loss: 0.697830, acc: 0.519531]  [A loss: 0.836471, acc: 0.207031]\n",
      "2986: [D loss: 0.701854, acc: 0.492188]  [A loss: 0.779427, acc: 0.320312]\n",
      "2987: [D loss: 0.696929, acc: 0.535156]  [A loss: 0.837629, acc: 0.253906]\n",
      "2988: [D loss: 0.697474, acc: 0.509766]  [A loss: 0.770604, acc: 0.328125]\n",
      "2989: [D loss: 0.708577, acc: 0.527344]  [A loss: 0.844064, acc: 0.179688]\n",
      "2990: [D loss: 0.691595, acc: 0.531250]  [A loss: 0.892159, acc: 0.152344]\n",
      "2991: [D loss: 0.696961, acc: 0.533203]  [A loss: 0.697169, acc: 0.515625]\n",
      "2992: [D loss: 0.716485, acc: 0.472656]  [A loss: 0.934602, acc: 0.125000]\n",
      "2993: [D loss: 0.699240, acc: 0.525391]  [A loss: 0.647571, acc: 0.640625]\n",
      "2994: [D loss: 0.729929, acc: 0.494141]  [A loss: 0.998595, acc: 0.058594]\n",
      "2995: [D loss: 0.702788, acc: 0.500000]  [A loss: 0.708170, acc: 0.500000]\n",
      "2996: [D loss: 0.707610, acc: 0.511719]  [A loss: 0.907407, acc: 0.109375]\n",
      "2997: [D loss: 0.702352, acc: 0.509766]  [A loss: 0.741824, acc: 0.382812]\n",
      "2998: [D loss: 0.701000, acc: 0.531250]  [A loss: 0.820617, acc: 0.246094]\n",
      "2999: [D loss: 0.702311, acc: 0.527344]  [A loss: 0.778191, acc: 0.324219]\n",
      "3000: [D loss: 0.708732, acc: 0.498047]  [A loss: 0.811668, acc: 0.253906]\n",
      "3001: [D loss: 0.701984, acc: 0.511719]  [A loss: 0.809667, acc: 0.250000]\n",
      "3002: [D loss: 0.697410, acc: 0.488281]  [A loss: 0.779481, acc: 0.347656]\n",
      "3003: [D loss: 0.697797, acc: 0.519531]  [A loss: 0.834662, acc: 0.234375]\n",
      "3004: [D loss: 0.691008, acc: 0.527344]  [A loss: 0.814947, acc: 0.246094]\n",
      "3005: [D loss: 0.697155, acc: 0.509766]  [A loss: 0.797744, acc: 0.261719]\n",
      "3006: [D loss: 0.702131, acc: 0.511719]  [A loss: 0.923841, acc: 0.101562]\n",
      "3007: [D loss: 0.687311, acc: 0.570312]  [A loss: 0.727647, acc: 0.425781]\n",
      "3008: [D loss: 0.704261, acc: 0.515625]  [A loss: 0.897134, acc: 0.171875]\n",
      "3009: [D loss: 0.694560, acc: 0.527344]  [A loss: 0.775260, acc: 0.375000]\n",
      "3010: [D loss: 0.717523, acc: 0.492188]  [A loss: 0.834019, acc: 0.226562]\n",
      "3011: [D loss: 0.701499, acc: 0.531250]  [A loss: 0.864485, acc: 0.144531]\n",
      "3012: [D loss: 0.697481, acc: 0.531250]  [A loss: 0.763955, acc: 0.367188]\n",
      "3013: [D loss: 0.712554, acc: 0.492188]  [A loss: 1.044776, acc: 0.031250]\n",
      "3014: [D loss: 0.714471, acc: 0.496094]  [A loss: 0.670688, acc: 0.593750]\n",
      "3015: [D loss: 0.724206, acc: 0.498047]  [A loss: 1.016285, acc: 0.074219]\n",
      "3016: [D loss: 0.687534, acc: 0.541016]  [A loss: 0.679722, acc: 0.542969]\n",
      "3017: [D loss: 0.737374, acc: 0.496094]  [A loss: 0.906025, acc: 0.128906]\n",
      "3018: [D loss: 0.694830, acc: 0.544922]  [A loss: 0.693983, acc: 0.496094]\n",
      "3019: [D loss: 0.720191, acc: 0.507812]  [A loss: 0.847220, acc: 0.175781]\n",
      "3020: [D loss: 0.687127, acc: 0.539062]  [A loss: 0.722627, acc: 0.429688]\n",
      "3021: [D loss: 0.699471, acc: 0.548828]  [A loss: 0.820340, acc: 0.250000]\n",
      "3022: [D loss: 0.700932, acc: 0.523438]  [A loss: 0.821118, acc: 0.222656]\n",
      "3023: [D loss: 0.692819, acc: 0.519531]  [A loss: 0.834081, acc: 0.191406]\n",
      "3024: [D loss: 0.695518, acc: 0.542969]  [A loss: 0.855033, acc: 0.199219]\n",
      "3025: [D loss: 0.700548, acc: 0.505859]  [A loss: 0.716221, acc: 0.449219]\n",
      "3026: [D loss: 0.713188, acc: 0.501953]  [A loss: 0.891477, acc: 0.132812]\n",
      "3027: [D loss: 0.695115, acc: 0.552734]  [A loss: 0.731864, acc: 0.468750]\n",
      "3028: [D loss: 0.722716, acc: 0.515625]  [A loss: 0.867667, acc: 0.164062]\n",
      "3029: [D loss: 0.688130, acc: 0.546875]  [A loss: 0.753695, acc: 0.386719]\n",
      "3030: [D loss: 0.689777, acc: 0.541016]  [A loss: 0.825169, acc: 0.257812]\n",
      "3031: [D loss: 0.700843, acc: 0.498047]  [A loss: 0.844233, acc: 0.175781]\n",
      "3032: [D loss: 0.693920, acc: 0.550781]  [A loss: 0.737815, acc: 0.417969]\n",
      "3033: [D loss: 0.715876, acc: 0.513672]  [A loss: 0.926605, acc: 0.109375]\n",
      "3034: [D loss: 0.699496, acc: 0.501953]  [A loss: 0.733011, acc: 0.378906]\n",
      "3035: [D loss: 0.702818, acc: 0.501953]  [A loss: 0.939414, acc: 0.089844]\n",
      "3036: [D loss: 0.694087, acc: 0.511719]  [A loss: 0.733182, acc: 0.382812]\n",
      "3037: [D loss: 0.706958, acc: 0.521484]  [A loss: 0.865454, acc: 0.183594]\n",
      "3038: [D loss: 0.689449, acc: 0.531250]  [A loss: 0.697611, acc: 0.500000]\n",
      "3039: [D loss: 0.735900, acc: 0.496094]  [A loss: 0.992585, acc: 0.074219]\n",
      "3040: [D loss: 0.711967, acc: 0.503906]  [A loss: 0.744255, acc: 0.390625]\n",
      "3041: [D loss: 0.714708, acc: 0.542969]  [A loss: 0.913191, acc: 0.101562]\n",
      "3042: [D loss: 0.689945, acc: 0.542969]  [A loss: 0.700108, acc: 0.503906]\n",
      "3043: [D loss: 0.714210, acc: 0.511719]  [A loss: 1.035755, acc: 0.042969]\n",
      "3044: [D loss: 0.697476, acc: 0.539062]  [A loss: 0.673039, acc: 0.601562]\n",
      "3045: [D loss: 0.706775, acc: 0.529297]  [A loss: 0.825110, acc: 0.214844]\n",
      "3046: [D loss: 0.695563, acc: 0.542969]  [A loss: 0.793683, acc: 0.308594]\n",
      "3047: [D loss: 0.702446, acc: 0.496094]  [A loss: 0.837865, acc: 0.214844]\n",
      "3048: [D loss: 0.698880, acc: 0.533203]  [A loss: 0.837353, acc: 0.218750]\n",
      "3049: [D loss: 0.687461, acc: 0.562500]  [A loss: 0.765896, acc: 0.371094]\n",
      "3050: [D loss: 0.709769, acc: 0.513672]  [A loss: 0.842572, acc: 0.203125]\n",
      "3051: [D loss: 0.715685, acc: 0.492188]  [A loss: 0.803140, acc: 0.265625]\n",
      "3052: [D loss: 0.705904, acc: 0.521484]  [A loss: 0.836716, acc: 0.250000]\n",
      "3053: [D loss: 0.695460, acc: 0.552734]  [A loss: 0.777600, acc: 0.351562]\n",
      "3054: [D loss: 0.714637, acc: 0.503906]  [A loss: 0.879206, acc: 0.144531]\n",
      "3055: [D loss: 0.691894, acc: 0.548828]  [A loss: 0.751970, acc: 0.371094]\n",
      "3056: [D loss: 0.714481, acc: 0.490234]  [A loss: 0.856790, acc: 0.160156]\n",
      "3057: [D loss: 0.683935, acc: 0.564453]  [A loss: 0.779886, acc: 0.332031]\n",
      "3058: [D loss: 0.711084, acc: 0.498047]  [A loss: 0.856957, acc: 0.214844]\n",
      "3059: [D loss: 0.699864, acc: 0.546875]  [A loss: 0.826807, acc: 0.250000]\n",
      "3060: [D loss: 0.695719, acc: 0.525391]  [A loss: 0.798412, acc: 0.292969]\n",
      "3061: [D loss: 0.696721, acc: 0.505859]  [A loss: 0.871437, acc: 0.160156]\n",
      "3062: [D loss: 0.686122, acc: 0.554688]  [A loss: 0.769681, acc: 0.320312]\n",
      "3063: [D loss: 0.698974, acc: 0.537109]  [A loss: 0.930125, acc: 0.113281]\n",
      "3064: [D loss: 0.681378, acc: 0.580078]  [A loss: 0.727821, acc: 0.425781]\n",
      "3065: [D loss: 0.693805, acc: 0.531250]  [A loss: 0.908989, acc: 0.144531]\n",
      "3066: [D loss: 0.687940, acc: 0.542969]  [A loss: 0.751880, acc: 0.406250]\n",
      "3067: [D loss: 0.728078, acc: 0.500000]  [A loss: 0.952080, acc: 0.070312]\n",
      "3068: [D loss: 0.693874, acc: 0.527344]  [A loss: 0.686311, acc: 0.546875]\n",
      "3069: [D loss: 0.740124, acc: 0.509766]  [A loss: 1.009416, acc: 0.074219]\n",
      "3070: [D loss: 0.699371, acc: 0.521484]  [A loss: 0.639200, acc: 0.648438]\n",
      "3071: [D loss: 0.735495, acc: 0.484375]  [A loss: 0.912147, acc: 0.132812]\n",
      "3072: [D loss: 0.691319, acc: 0.550781]  [A loss: 0.713395, acc: 0.460938]\n",
      "3073: [D loss: 0.708413, acc: 0.513672]  [A loss: 0.854825, acc: 0.199219]\n",
      "3074: [D loss: 0.695140, acc: 0.529297]  [A loss: 0.755450, acc: 0.359375]\n",
      "3075: [D loss: 0.705460, acc: 0.494141]  [A loss: 0.916996, acc: 0.128906]\n",
      "3076: [D loss: 0.691470, acc: 0.537109]  [A loss: 0.715950, acc: 0.441406]\n",
      "3077: [D loss: 0.705542, acc: 0.521484]  [A loss: 0.889319, acc: 0.164062]\n",
      "3078: [D loss: 0.690692, acc: 0.550781]  [A loss: 0.752028, acc: 0.406250]\n",
      "3079: [D loss: 0.698449, acc: 0.525391]  [A loss: 0.854925, acc: 0.175781]\n",
      "3080: [D loss: 0.690770, acc: 0.519531]  [A loss: 0.775496, acc: 0.332031]\n",
      "3081: [D loss: 0.696100, acc: 0.527344]  [A loss: 0.872799, acc: 0.210938]\n",
      "3082: [D loss: 0.698907, acc: 0.511719]  [A loss: 0.819344, acc: 0.250000]\n",
      "3083: [D loss: 0.693654, acc: 0.503906]  [A loss: 0.788301, acc: 0.289062]\n",
      "3084: [D loss: 0.703335, acc: 0.517578]  [A loss: 0.887875, acc: 0.132812]\n",
      "3085: [D loss: 0.694697, acc: 0.523438]  [A loss: 0.803821, acc: 0.269531]\n",
      "3086: [D loss: 0.694169, acc: 0.541016]  [A loss: 0.890534, acc: 0.156250]\n",
      "3087: [D loss: 0.709410, acc: 0.515625]  [A loss: 0.869555, acc: 0.175781]\n",
      "3088: [D loss: 0.698698, acc: 0.515625]  [A loss: 0.809667, acc: 0.308594]\n",
      "3089: [D loss: 0.681938, acc: 0.585938]  [A loss: 0.843612, acc: 0.230469]\n",
      "3090: [D loss: 0.701017, acc: 0.537109]  [A loss: 0.864134, acc: 0.230469]\n",
      "3091: [D loss: 0.694299, acc: 0.525391]  [A loss: 0.847963, acc: 0.207031]\n",
      "3092: [D loss: 0.695062, acc: 0.558594]  [A loss: 0.872930, acc: 0.179688]\n",
      "3093: [D loss: 0.696494, acc: 0.519531]  [A loss: 0.903916, acc: 0.152344]\n",
      "3094: [D loss: 0.685224, acc: 0.562500]  [A loss: 0.687379, acc: 0.546875]\n",
      "3095: [D loss: 0.741656, acc: 0.453125]  [A loss: 1.154028, acc: 0.019531]\n",
      "3096: [D loss: 0.716016, acc: 0.515625]  [A loss: 0.617923, acc: 0.703125]\n",
      "3097: [D loss: 0.746616, acc: 0.492188]  [A loss: 0.982888, acc: 0.062500]\n",
      "3098: [D loss: 0.701882, acc: 0.517578]  [A loss: 0.699471, acc: 0.484375]\n",
      "3099: [D loss: 0.705662, acc: 0.523438]  [A loss: 0.838022, acc: 0.175781]\n",
      "3100: [D loss: 0.702147, acc: 0.523438]  [A loss: 0.743612, acc: 0.371094]\n",
      "3101: [D loss: 0.718021, acc: 0.492188]  [A loss: 0.878913, acc: 0.167969]\n",
      "3102: [D loss: 0.694887, acc: 0.527344]  [A loss: 0.782394, acc: 0.308594]\n",
      "3103: [D loss: 0.684726, acc: 0.546875]  [A loss: 0.850827, acc: 0.218750]\n",
      "3104: [D loss: 0.696566, acc: 0.535156]  [A loss: 0.829659, acc: 0.238281]\n",
      "3105: [D loss: 0.702086, acc: 0.533203]  [A loss: 0.825532, acc: 0.261719]\n",
      "3106: [D loss: 0.698533, acc: 0.513672]  [A loss: 0.820864, acc: 0.285156]\n",
      "3107: [D loss: 0.691032, acc: 0.554688]  [A loss: 0.844505, acc: 0.203125]\n",
      "3108: [D loss: 0.696918, acc: 0.550781]  [A loss: 0.927038, acc: 0.128906]\n",
      "3109: [D loss: 0.704889, acc: 0.511719]  [A loss: 0.748237, acc: 0.394531]\n",
      "3110: [D loss: 0.710438, acc: 0.527344]  [A loss: 0.938666, acc: 0.078125]\n",
      "3111: [D loss: 0.672728, acc: 0.560547]  [A loss: 0.768210, acc: 0.378906]\n",
      "3112: [D loss: 0.700198, acc: 0.509766]  [A loss: 0.833038, acc: 0.257812]\n",
      "3113: [D loss: 0.697229, acc: 0.523438]  [A loss: 0.757260, acc: 0.382812]\n",
      "3114: [D loss: 0.729262, acc: 0.509766]  [A loss: 1.078502, acc: 0.023438]\n",
      "3115: [D loss: 0.704401, acc: 0.511719]  [A loss: 0.630406, acc: 0.664062]\n",
      "3116: [D loss: 0.741080, acc: 0.505859]  [A loss: 0.966191, acc: 0.058594]\n",
      "3117: [D loss: 0.702233, acc: 0.509766]  [A loss: 0.723049, acc: 0.437500]\n",
      "3118: [D loss: 0.699747, acc: 0.519531]  [A loss: 0.836049, acc: 0.230469]\n",
      "3119: [D loss: 0.693789, acc: 0.519531]  [A loss: 0.782135, acc: 0.320312]\n",
      "3120: [D loss: 0.700602, acc: 0.519531]  [A loss: 0.842834, acc: 0.261719]\n",
      "3121: [D loss: 0.706124, acc: 0.513672]  [A loss: 0.820051, acc: 0.261719]\n",
      "3122: [D loss: 0.695267, acc: 0.560547]  [A loss: 0.797269, acc: 0.304688]\n",
      "3123: [D loss: 0.700507, acc: 0.501953]  [A loss: 0.915702, acc: 0.140625]\n",
      "3124: [D loss: 0.697001, acc: 0.509766]  [A loss: 0.744231, acc: 0.417969]\n",
      "3125: [D loss: 0.719392, acc: 0.517578]  [A loss: 1.020353, acc: 0.050781]\n",
      "3126: [D loss: 0.699750, acc: 0.496094]  [A loss: 0.652918, acc: 0.628906]\n",
      "3127: [D loss: 0.723737, acc: 0.509766]  [A loss: 1.027873, acc: 0.066406]\n",
      "3128: [D loss: 0.714446, acc: 0.517578]  [A loss: 0.735140, acc: 0.421875]\n",
      "3129: [D loss: 0.703419, acc: 0.511719]  [A loss: 0.859713, acc: 0.171875]\n",
      "3130: [D loss: 0.690446, acc: 0.546875]  [A loss: 0.814044, acc: 0.257812]\n",
      "3131: [D loss: 0.714019, acc: 0.519531]  [A loss: 0.809010, acc: 0.273438]\n",
      "3132: [D loss: 0.710099, acc: 0.509766]  [A loss: 0.815144, acc: 0.277344]\n",
      "3133: [D loss: 0.710078, acc: 0.509766]  [A loss: 0.859458, acc: 0.187500]\n",
      "3134: [D loss: 0.702859, acc: 0.519531]  [A loss: 0.889459, acc: 0.164062]\n",
      "3135: [D loss: 0.703165, acc: 0.511719]  [A loss: 0.730292, acc: 0.457031]\n",
      "3136: [D loss: 0.708434, acc: 0.496094]  [A loss: 0.902954, acc: 0.117188]\n",
      "3137: [D loss: 0.693859, acc: 0.550781]  [A loss: 0.795441, acc: 0.339844]\n",
      "3138: [D loss: 0.704232, acc: 0.529297]  [A loss: 0.941247, acc: 0.128906]\n",
      "3139: [D loss: 0.695339, acc: 0.533203]  [A loss: 0.708302, acc: 0.464844]\n",
      "3140: [D loss: 0.709732, acc: 0.511719]  [A loss: 1.005617, acc: 0.089844]\n",
      "3141: [D loss: 0.700833, acc: 0.509766]  [A loss: 0.675300, acc: 0.566406]\n",
      "3142: [D loss: 0.727401, acc: 0.513672]  [A loss: 0.974365, acc: 0.105469]\n",
      "3143: [D loss: 0.700173, acc: 0.521484]  [A loss: 0.747898, acc: 0.367188]\n",
      "3144: [D loss: 0.711697, acc: 0.519531]  [A loss: 0.847876, acc: 0.218750]\n",
      "3145: [D loss: 0.703488, acc: 0.513672]  [A loss: 0.730059, acc: 0.464844]\n",
      "3146: [D loss: 0.706202, acc: 0.527344]  [A loss: 0.863503, acc: 0.195312]\n",
      "3147: [D loss: 0.709234, acc: 0.496094]  [A loss: 0.771972, acc: 0.355469]\n",
      "3148: [D loss: 0.691081, acc: 0.519531]  [A loss: 0.850742, acc: 0.210938]\n",
      "3149: [D loss: 0.693622, acc: 0.531250]  [A loss: 0.732432, acc: 0.402344]\n",
      "3150: [D loss: 0.720858, acc: 0.484375]  [A loss: 0.907558, acc: 0.121094]\n",
      "3151: [D loss: 0.694588, acc: 0.515625]  [A loss: 0.800185, acc: 0.296875]\n",
      "3152: [D loss: 0.718345, acc: 0.486328]  [A loss: 0.875986, acc: 0.167969]\n",
      "3153: [D loss: 0.691943, acc: 0.548828]  [A loss: 0.740984, acc: 0.445312]\n",
      "3154: [D loss: 0.721717, acc: 0.472656]  [A loss: 0.980285, acc: 0.093750]\n",
      "3155: [D loss: 0.693681, acc: 0.531250]  [A loss: 0.647808, acc: 0.636719]\n",
      "3156: [D loss: 0.729724, acc: 0.507812]  [A loss: 0.958092, acc: 0.078125]\n",
      "3157: [D loss: 0.689054, acc: 0.554688]  [A loss: 0.674531, acc: 0.546875]\n",
      "3158: [D loss: 0.722772, acc: 0.515625]  [A loss: 1.025732, acc: 0.066406]\n",
      "3159: [D loss: 0.711329, acc: 0.501953]  [A loss: 0.701221, acc: 0.511719]\n",
      "3160: [D loss: 0.697234, acc: 0.525391]  [A loss: 0.908097, acc: 0.160156]\n",
      "3161: [D loss: 0.684424, acc: 0.578125]  [A loss: 0.770575, acc: 0.363281]\n",
      "3162: [D loss: 0.704382, acc: 0.539062]  [A loss: 0.798938, acc: 0.316406]\n",
      "3163: [D loss: 0.706386, acc: 0.541016]  [A loss: 0.820624, acc: 0.285156]\n",
      "3164: [D loss: 0.695460, acc: 0.539062]  [A loss: 0.811879, acc: 0.285156]\n",
      "3165: [D loss: 0.703642, acc: 0.519531]  [A loss: 0.754702, acc: 0.386719]\n",
      "3166: [D loss: 0.708711, acc: 0.556641]  [A loss: 0.889212, acc: 0.144531]\n",
      "3167: [D loss: 0.705641, acc: 0.494141]  [A loss: 0.767706, acc: 0.390625]\n",
      "3168: [D loss: 0.698284, acc: 0.531250]  [A loss: 0.921018, acc: 0.078125]\n",
      "3169: [D loss: 0.695723, acc: 0.539062]  [A loss: 0.712554, acc: 0.460938]\n",
      "3170: [D loss: 0.737159, acc: 0.482422]  [A loss: 1.018667, acc: 0.042969]\n",
      "3171: [D loss: 0.698188, acc: 0.496094]  [A loss: 0.679641, acc: 0.539062]\n",
      "3172: [D loss: 0.722928, acc: 0.496094]  [A loss: 0.931997, acc: 0.082031]\n",
      "3173: [D loss: 0.698759, acc: 0.519531]  [A loss: 0.694766, acc: 0.527344]\n",
      "3174: [D loss: 0.725556, acc: 0.472656]  [A loss: 0.894737, acc: 0.160156]\n",
      "3175: [D loss: 0.697322, acc: 0.521484]  [A loss: 0.750025, acc: 0.398438]\n",
      "3176: [D loss: 0.699491, acc: 0.548828]  [A loss: 0.817560, acc: 0.250000]\n",
      "3177: [D loss: 0.683399, acc: 0.574219]  [A loss: 0.817387, acc: 0.222656]\n",
      "3178: [D loss: 0.698489, acc: 0.548828]  [A loss: 0.870221, acc: 0.156250]\n",
      "3179: [D loss: 0.683276, acc: 0.562500]  [A loss: 0.796183, acc: 0.242188]\n",
      "3180: [D loss: 0.698947, acc: 0.517578]  [A loss: 0.855097, acc: 0.214844]\n",
      "3181: [D loss: 0.678632, acc: 0.568359]  [A loss: 0.764176, acc: 0.382812]\n",
      "3182: [D loss: 0.706155, acc: 0.521484]  [A loss: 0.985946, acc: 0.101562]\n",
      "3183: [D loss: 0.689405, acc: 0.515625]  [A loss: 0.691484, acc: 0.558594]\n",
      "3184: [D loss: 0.709633, acc: 0.529297]  [A loss: 0.908517, acc: 0.136719]\n",
      "3185: [D loss: 0.697272, acc: 0.521484]  [A loss: 0.756198, acc: 0.367188]\n",
      "3186: [D loss: 0.698845, acc: 0.523438]  [A loss: 0.894037, acc: 0.160156]\n",
      "3187: [D loss: 0.693189, acc: 0.542969]  [A loss: 0.735656, acc: 0.398438]\n",
      "3188: [D loss: 0.706932, acc: 0.513672]  [A loss: 0.819350, acc: 0.277344]\n",
      "3189: [D loss: 0.695027, acc: 0.500000]  [A loss: 0.780953, acc: 0.328125]\n",
      "3190: [D loss: 0.696282, acc: 0.531250]  [A loss: 0.795269, acc: 0.316406]\n",
      "3191: [D loss: 0.698692, acc: 0.519531]  [A loss: 0.898418, acc: 0.125000]\n",
      "3192: [D loss: 0.698426, acc: 0.527344]  [A loss: 0.724830, acc: 0.449219]\n",
      "3193: [D loss: 0.708633, acc: 0.509766]  [A loss: 0.990369, acc: 0.078125]\n",
      "3194: [D loss: 0.696006, acc: 0.511719]  [A loss: 0.748641, acc: 0.390625]\n",
      "3195: [D loss: 0.714190, acc: 0.519531]  [A loss: 0.945981, acc: 0.121094]\n",
      "3196: [D loss: 0.698703, acc: 0.535156]  [A loss: 0.795889, acc: 0.296875]\n",
      "3197: [D loss: 0.704515, acc: 0.521484]  [A loss: 0.790414, acc: 0.265625]\n",
      "3198: [D loss: 0.700600, acc: 0.515625]  [A loss: 0.821821, acc: 0.226562]\n",
      "3199: [D loss: 0.691637, acc: 0.541016]  [A loss: 0.873279, acc: 0.175781]\n",
      "3200: [D loss: 0.703572, acc: 0.515625]  [A loss: 0.878319, acc: 0.167969]\n",
      "3201: [D loss: 0.697341, acc: 0.515625]  [A loss: 0.750275, acc: 0.402344]\n",
      "3202: [D loss: 0.712039, acc: 0.511719]  [A loss: 1.032386, acc: 0.101562]\n",
      "3203: [D loss: 0.701268, acc: 0.527344]  [A loss: 0.701007, acc: 0.503906]\n",
      "3204: [D loss: 0.717621, acc: 0.509766]  [A loss: 0.931107, acc: 0.140625]\n",
      "3205: [D loss: 0.695568, acc: 0.542969]  [A loss: 0.779440, acc: 0.328125]\n",
      "3206: [D loss: 0.711782, acc: 0.537109]  [A loss: 1.003834, acc: 0.066406]\n",
      "3207: [D loss: 0.695244, acc: 0.542969]  [A loss: 0.694290, acc: 0.496094]\n",
      "3208: [D loss: 0.712220, acc: 0.509766]  [A loss: 0.925005, acc: 0.152344]\n",
      "3209: [D loss: 0.698668, acc: 0.525391]  [A loss: 0.766662, acc: 0.371094]\n",
      "3210: [D loss: 0.705078, acc: 0.529297]  [A loss: 0.882685, acc: 0.179688]\n",
      "3211: [D loss: 0.702769, acc: 0.517578]  [A loss: 0.741894, acc: 0.417969]\n",
      "3212: [D loss: 0.712414, acc: 0.523438]  [A loss: 0.921094, acc: 0.105469]\n",
      "3213: [D loss: 0.701800, acc: 0.517578]  [A loss: 0.741566, acc: 0.425781]\n",
      "3214: [D loss: 0.702162, acc: 0.535156]  [A loss: 0.976608, acc: 0.109375]\n",
      "3215: [D loss: 0.696313, acc: 0.533203]  [A loss: 0.714739, acc: 0.488281]\n",
      "3216: [D loss: 0.717274, acc: 0.523438]  [A loss: 0.956233, acc: 0.074219]\n",
      "3217: [D loss: 0.717915, acc: 0.468750]  [A loss: 0.722910, acc: 0.441406]\n",
      "3218: [D loss: 0.728052, acc: 0.498047]  [A loss: 0.979501, acc: 0.093750]\n",
      "3219: [D loss: 0.691528, acc: 0.541016]  [A loss: 0.690358, acc: 0.519531]\n",
      "3220: [D loss: 0.720107, acc: 0.515625]  [A loss: 0.819257, acc: 0.273438]\n",
      "3221: [D loss: 0.702425, acc: 0.531250]  [A loss: 0.820936, acc: 0.234375]\n",
      "3222: [D loss: 0.707873, acc: 0.496094]  [A loss: 0.785318, acc: 0.335938]\n",
      "3223: [D loss: 0.698122, acc: 0.533203]  [A loss: 0.833085, acc: 0.234375]\n",
      "3224: [D loss: 0.699888, acc: 0.537109]  [A loss: 0.768526, acc: 0.347656]\n",
      "3225: [D loss: 0.700160, acc: 0.525391]  [A loss: 0.962911, acc: 0.117188]\n",
      "3226: [D loss: 0.690143, acc: 0.542969]  [A loss: 0.749537, acc: 0.371094]\n",
      "3227: [D loss: 0.723234, acc: 0.492188]  [A loss: 0.979528, acc: 0.082031]\n",
      "3228: [D loss: 0.713664, acc: 0.509766]  [A loss: 0.689544, acc: 0.554688]\n",
      "3229: [D loss: 0.718107, acc: 0.503906]  [A loss: 0.910110, acc: 0.140625]\n",
      "3230: [D loss: 0.706461, acc: 0.484375]  [A loss: 0.694110, acc: 0.496094]\n",
      "3231: [D loss: 0.728146, acc: 0.511719]  [A loss: 0.953205, acc: 0.109375]\n",
      "3232: [D loss: 0.700174, acc: 0.505859]  [A loss: 0.728397, acc: 0.449219]\n",
      "3233: [D loss: 0.711809, acc: 0.494141]  [A loss: 0.839742, acc: 0.230469]\n",
      "3234: [D loss: 0.705697, acc: 0.500000]  [A loss: 0.813941, acc: 0.257812]\n",
      "3235: [D loss: 0.702245, acc: 0.513672]  [A loss: 0.829328, acc: 0.246094]\n",
      "3236: [D loss: 0.715588, acc: 0.494141]  [A loss: 0.799847, acc: 0.308594]\n",
      "3237: [D loss: 0.697528, acc: 0.535156]  [A loss: 0.825085, acc: 0.261719]\n",
      "3238: [D loss: 0.693932, acc: 0.531250]  [A loss: 0.835829, acc: 0.265625]\n",
      "3239: [D loss: 0.694296, acc: 0.564453]  [A loss: 0.740164, acc: 0.425781]\n",
      "3240: [D loss: 0.714258, acc: 0.507812]  [A loss: 0.990865, acc: 0.085938]\n",
      "3241: [D loss: 0.705160, acc: 0.511719]  [A loss: 0.674128, acc: 0.605469]\n",
      "3242: [D loss: 0.727451, acc: 0.509766]  [A loss: 0.981696, acc: 0.085938]\n",
      "3243: [D loss: 0.694684, acc: 0.541016]  [A loss: 0.677631, acc: 0.542969]\n",
      "3244: [D loss: 0.721853, acc: 0.505859]  [A loss: 0.899291, acc: 0.132812]\n",
      "3245: [D loss: 0.710825, acc: 0.498047]  [A loss: 0.738224, acc: 0.410156]\n",
      "3246: [D loss: 0.700910, acc: 0.503906]  [A loss: 0.829231, acc: 0.230469]\n",
      "3247: [D loss: 0.704366, acc: 0.505859]  [A loss: 0.849605, acc: 0.214844]\n",
      "3248: [D loss: 0.696651, acc: 0.533203]  [A loss: 0.858621, acc: 0.199219]\n",
      "3249: [D loss: 0.690866, acc: 0.544922]  [A loss: 0.834475, acc: 0.226562]\n",
      "3250: [D loss: 0.704326, acc: 0.511719]  [A loss: 0.838957, acc: 0.250000]\n",
      "3251: [D loss: 0.693522, acc: 0.535156]  [A loss: 0.812137, acc: 0.285156]\n",
      "3252: [D loss: 0.707465, acc: 0.503906]  [A loss: 0.870232, acc: 0.203125]\n",
      "3253: [D loss: 0.694479, acc: 0.529297]  [A loss: 0.820136, acc: 0.277344]\n",
      "3254: [D loss: 0.699292, acc: 0.533203]  [A loss: 0.904105, acc: 0.148438]\n",
      "3255: [D loss: 0.701921, acc: 0.505859]  [A loss: 0.710079, acc: 0.503906]\n",
      "3256: [D loss: 0.715687, acc: 0.533203]  [A loss: 0.902140, acc: 0.175781]\n",
      "3257: [D loss: 0.712363, acc: 0.513672]  [A loss: 0.791613, acc: 0.335938]\n",
      "3258: [D loss: 0.709644, acc: 0.505859]  [A loss: 0.941197, acc: 0.101562]\n",
      "3259: [D loss: 0.707054, acc: 0.505859]  [A loss: 0.737629, acc: 0.429688]\n",
      "3260: [D loss: 0.729802, acc: 0.496094]  [A loss: 0.929602, acc: 0.148438]\n",
      "3261: [D loss: 0.699783, acc: 0.533203]  [A loss: 0.701070, acc: 0.507812]\n",
      "3262: [D loss: 0.723818, acc: 0.511719]  [A loss: 1.031381, acc: 0.078125]\n",
      "3263: [D loss: 0.696660, acc: 0.523438]  [A loss: 0.702578, acc: 0.511719]\n",
      "3264: [D loss: 0.733277, acc: 0.484375]  [A loss: 0.890793, acc: 0.148438]\n",
      "3265: [D loss: 0.697641, acc: 0.511719]  [A loss: 0.691025, acc: 0.574219]\n",
      "3266: [D loss: 0.722531, acc: 0.496094]  [A loss: 0.953432, acc: 0.097656]\n",
      "3267: [D loss: 0.703123, acc: 0.496094]  [A loss: 0.750230, acc: 0.390625]\n",
      "3268: [D loss: 0.705772, acc: 0.513672]  [A loss: 0.850353, acc: 0.207031]\n",
      "3269: [D loss: 0.690065, acc: 0.531250]  [A loss: 0.733452, acc: 0.441406]\n",
      "3270: [D loss: 0.722019, acc: 0.503906]  [A loss: 0.885332, acc: 0.164062]\n",
      "3271: [D loss: 0.692546, acc: 0.529297]  [A loss: 0.740887, acc: 0.437500]\n",
      "3272: [D loss: 0.711468, acc: 0.494141]  [A loss: 0.920330, acc: 0.132812]\n",
      "3273: [D loss: 0.695859, acc: 0.535156]  [A loss: 0.809064, acc: 0.281250]\n",
      "3274: [D loss: 0.709720, acc: 0.488281]  [A loss: 0.876603, acc: 0.179688]\n",
      "3275: [D loss: 0.700545, acc: 0.511719]  [A loss: 0.775515, acc: 0.304688]\n",
      "3276: [D loss: 0.693289, acc: 0.535156]  [A loss: 0.830670, acc: 0.257812]\n",
      "3277: [D loss: 0.695799, acc: 0.560547]  [A loss: 0.882974, acc: 0.218750]\n",
      "3278: [D loss: 0.687557, acc: 0.533203]  [A loss: 0.827835, acc: 0.250000]\n",
      "3279: [D loss: 0.709050, acc: 0.517578]  [A loss: 0.874967, acc: 0.164062]\n",
      "3280: [D loss: 0.715277, acc: 0.478516]  [A loss: 0.842731, acc: 0.207031]\n",
      "3281: [D loss: 0.693737, acc: 0.574219]  [A loss: 0.810273, acc: 0.300781]\n",
      "3282: [D loss: 0.693577, acc: 0.515625]  [A loss: 0.876742, acc: 0.187500]\n",
      "3283: [D loss: 0.694350, acc: 0.544922]  [A loss: 0.718205, acc: 0.453125]\n",
      "3284: [D loss: 0.710098, acc: 0.535156]  [A loss: 1.115420, acc: 0.039062]\n",
      "3285: [D loss: 0.701562, acc: 0.525391]  [A loss: 0.647600, acc: 0.613281]\n",
      "3286: [D loss: 0.738087, acc: 0.484375]  [A loss: 0.901189, acc: 0.132812]\n",
      "3287: [D loss: 0.696436, acc: 0.535156]  [A loss: 0.751532, acc: 0.402344]\n",
      "3288: [D loss: 0.710226, acc: 0.517578]  [A loss: 0.937423, acc: 0.113281]\n",
      "3289: [D loss: 0.709041, acc: 0.490234]  [A loss: 0.715554, acc: 0.476562]\n",
      "3290: [D loss: 0.722854, acc: 0.482422]  [A loss: 0.886404, acc: 0.148438]\n",
      "3291: [D loss: 0.700198, acc: 0.517578]  [A loss: 0.798650, acc: 0.296875]\n",
      "3292: [D loss: 0.693733, acc: 0.552734]  [A loss: 0.856418, acc: 0.214844]\n",
      "3293: [D loss: 0.700060, acc: 0.523438]  [A loss: 0.750040, acc: 0.417969]\n",
      "3294: [D loss: 0.702781, acc: 0.523438]  [A loss: 0.915727, acc: 0.125000]\n",
      "3295: [D loss: 0.702983, acc: 0.505859]  [A loss: 0.829994, acc: 0.242188]\n",
      "3296: [D loss: 0.713423, acc: 0.482422]  [A loss: 0.838792, acc: 0.242188]\n",
      "3297: [D loss: 0.709681, acc: 0.492188]  [A loss: 0.794447, acc: 0.281250]\n",
      "3298: [D loss: 0.698499, acc: 0.531250]  [A loss: 0.930395, acc: 0.109375]\n",
      "3299: [D loss: 0.699286, acc: 0.525391]  [A loss: 0.839190, acc: 0.257812]\n",
      "3300: [D loss: 0.695500, acc: 0.525391]  [A loss: 0.785352, acc: 0.332031]\n",
      "3301: [D loss: 0.713336, acc: 0.505859]  [A loss: 1.021849, acc: 0.101562]\n",
      "3302: [D loss: 0.714408, acc: 0.517578]  [A loss: 0.676332, acc: 0.574219]\n",
      "3303: [D loss: 0.741997, acc: 0.484375]  [A loss: 0.997284, acc: 0.050781]\n",
      "3304: [D loss: 0.715792, acc: 0.476562]  [A loss: 0.672229, acc: 0.597656]\n",
      "3305: [D loss: 0.730663, acc: 0.503906]  [A loss: 1.023493, acc: 0.078125]\n",
      "3306: [D loss: 0.706629, acc: 0.503906]  [A loss: 0.715215, acc: 0.492188]\n",
      "3307: [D loss: 0.720905, acc: 0.501953]  [A loss: 0.866019, acc: 0.195312]\n",
      "3308: [D loss: 0.697156, acc: 0.523438]  [A loss: 0.762156, acc: 0.367188]\n",
      "3309: [D loss: 0.714676, acc: 0.472656]  [A loss: 0.918471, acc: 0.156250]\n",
      "3310: [D loss: 0.687936, acc: 0.542969]  [A loss: 0.748907, acc: 0.390625]\n",
      "3311: [D loss: 0.701768, acc: 0.519531]  [A loss: 0.817808, acc: 0.273438]\n",
      "3312: [D loss: 0.702313, acc: 0.488281]  [A loss: 0.773146, acc: 0.335938]\n",
      "3313: [D loss: 0.699738, acc: 0.535156]  [A loss: 0.832149, acc: 0.234375]\n",
      "3314: [D loss: 0.704449, acc: 0.519531]  [A loss: 0.783096, acc: 0.300781]\n",
      "3315: [D loss: 0.717776, acc: 0.482422]  [A loss: 0.884386, acc: 0.144531]\n",
      "3316: [D loss: 0.701996, acc: 0.535156]  [A loss: 0.814215, acc: 0.246094]\n",
      "3317: [D loss: 0.707625, acc: 0.517578]  [A loss: 0.951271, acc: 0.074219]\n",
      "3318: [D loss: 0.693179, acc: 0.531250]  [A loss: 0.709020, acc: 0.480469]\n",
      "3319: [D loss: 0.717551, acc: 0.513672]  [A loss: 0.980673, acc: 0.121094]\n",
      "3320: [D loss: 0.700224, acc: 0.505859]  [A loss: 0.712192, acc: 0.468750]\n",
      "3321: [D loss: 0.706779, acc: 0.533203]  [A loss: 0.837604, acc: 0.230469]\n",
      "3322: [D loss: 0.708057, acc: 0.513672]  [A loss: 0.813566, acc: 0.253906]\n",
      "3323: [D loss: 0.715386, acc: 0.501953]  [A loss: 0.837186, acc: 0.207031]\n",
      "3324: [D loss: 0.685169, acc: 0.566406]  [A loss: 0.818340, acc: 0.269531]\n",
      "3325: [D loss: 0.715032, acc: 0.498047]  [A loss: 0.839797, acc: 0.187500]\n",
      "3326: [D loss: 0.699861, acc: 0.517578]  [A loss: 0.836729, acc: 0.222656]\n",
      "3327: [D loss: 0.707951, acc: 0.505859]  [A loss: 0.914849, acc: 0.152344]\n",
      "3328: [D loss: 0.698768, acc: 0.531250]  [A loss: 0.750517, acc: 0.386719]\n",
      "3329: [D loss: 0.693439, acc: 0.554688]  [A loss: 0.880244, acc: 0.167969]\n",
      "3330: [D loss: 0.701196, acc: 0.503906]  [A loss: 0.755810, acc: 0.351562]\n",
      "3331: [D loss: 0.700830, acc: 0.546875]  [A loss: 0.882964, acc: 0.160156]\n",
      "3332: [D loss: 0.691965, acc: 0.546875]  [A loss: 0.747458, acc: 0.390625]\n",
      "3333: [D loss: 0.709558, acc: 0.517578]  [A loss: 0.983044, acc: 0.097656]\n",
      "3334: [D loss: 0.717756, acc: 0.494141]  [A loss: 0.705196, acc: 0.527344]\n",
      "3335: [D loss: 0.726012, acc: 0.517578]  [A loss: 1.026401, acc: 0.054688]\n",
      "3336: [D loss: 0.714385, acc: 0.490234]  [A loss: 0.677510, acc: 0.531250]\n",
      "3337: [D loss: 0.712966, acc: 0.519531]  [A loss: 0.880527, acc: 0.179688]\n",
      "3338: [D loss: 0.689309, acc: 0.527344]  [A loss: 0.759858, acc: 0.347656]\n",
      "3339: [D loss: 0.715323, acc: 0.515625]  [A loss: 0.875324, acc: 0.191406]\n",
      "3340: [D loss: 0.707345, acc: 0.521484]  [A loss: 0.766107, acc: 0.394531]\n",
      "3341: [D loss: 0.701017, acc: 0.535156]  [A loss: 0.893635, acc: 0.164062]\n",
      "3342: [D loss: 0.702519, acc: 0.515625]  [A loss: 0.745171, acc: 0.402344]\n",
      "3343: [D loss: 0.725840, acc: 0.468750]  [A loss: 0.961047, acc: 0.093750]\n",
      "3344: [D loss: 0.694400, acc: 0.535156]  [A loss: 0.728448, acc: 0.441406]\n",
      "3345: [D loss: 0.724774, acc: 0.501953]  [A loss: 0.926854, acc: 0.101562]\n",
      "3346: [D loss: 0.691271, acc: 0.562500]  [A loss: 0.721297, acc: 0.476562]\n",
      "3347: [D loss: 0.712240, acc: 0.511719]  [A loss: 0.904294, acc: 0.179688]\n",
      "3348: [D loss: 0.712132, acc: 0.503906]  [A loss: 0.723742, acc: 0.441406]\n",
      "3349: [D loss: 0.732394, acc: 0.490234]  [A loss: 0.951742, acc: 0.101562]\n",
      "3350: [D loss: 0.706705, acc: 0.511719]  [A loss: 0.713783, acc: 0.453125]\n",
      "3351: [D loss: 0.719457, acc: 0.503906]  [A loss: 0.908156, acc: 0.113281]\n",
      "3352: [D loss: 0.699539, acc: 0.531250]  [A loss: 0.728706, acc: 0.453125]\n",
      "3353: [D loss: 0.729504, acc: 0.492188]  [A loss: 0.921334, acc: 0.132812]\n",
      "3354: [D loss: 0.699547, acc: 0.505859]  [A loss: 0.704732, acc: 0.484375]\n",
      "3355: [D loss: 0.702532, acc: 0.515625]  [A loss: 0.874735, acc: 0.191406]\n",
      "3356: [D loss: 0.699615, acc: 0.529297]  [A loss: 0.740598, acc: 0.417969]\n",
      "3357: [D loss: 0.709729, acc: 0.513672]  [A loss: 0.891106, acc: 0.175781]\n",
      "3358: [D loss: 0.691384, acc: 0.529297]  [A loss: 0.731096, acc: 0.414062]\n",
      "3359: [D loss: 0.712884, acc: 0.523438]  [A loss: 0.858216, acc: 0.207031]\n",
      "3360: [D loss: 0.703235, acc: 0.529297]  [A loss: 0.735995, acc: 0.414062]\n",
      "3361: [D loss: 0.691341, acc: 0.546875]  [A loss: 0.812016, acc: 0.292969]\n",
      "3362: [D loss: 0.713150, acc: 0.519531]  [A loss: 0.882253, acc: 0.175781]\n",
      "3363: [D loss: 0.695110, acc: 0.509766]  [A loss: 0.793038, acc: 0.285156]\n",
      "3364: [D loss: 0.690592, acc: 0.556641]  [A loss: 0.943860, acc: 0.109375]\n",
      "3365: [D loss: 0.699611, acc: 0.546875]  [A loss: 0.717862, acc: 0.472656]\n",
      "3366: [D loss: 0.720803, acc: 0.503906]  [A loss: 0.981673, acc: 0.062500]\n",
      "3367: [D loss: 0.696306, acc: 0.525391]  [A loss: 0.659121, acc: 0.570312]\n",
      "3368: [D loss: 0.746315, acc: 0.488281]  [A loss: 1.048249, acc: 0.042969]\n",
      "3369: [D loss: 0.718477, acc: 0.492188]  [A loss: 0.777043, acc: 0.304688]\n",
      "3370: [D loss: 0.712563, acc: 0.517578]  [A loss: 0.901204, acc: 0.136719]\n",
      "3371: [D loss: 0.703984, acc: 0.501953]  [A loss: 0.765485, acc: 0.359375]\n",
      "3372: [D loss: 0.719850, acc: 0.494141]  [A loss: 0.921386, acc: 0.140625]\n",
      "3373: [D loss: 0.700015, acc: 0.531250]  [A loss: 0.767097, acc: 0.335938]\n",
      "3374: [D loss: 0.726085, acc: 0.482422]  [A loss: 0.884351, acc: 0.136719]\n",
      "3375: [D loss: 0.694877, acc: 0.521484]  [A loss: 0.737751, acc: 0.402344]\n",
      "3376: [D loss: 0.724626, acc: 0.484375]  [A loss: 0.926379, acc: 0.136719]\n",
      "3377: [D loss: 0.705769, acc: 0.496094]  [A loss: 0.748237, acc: 0.410156]\n",
      "3378: [D loss: 0.711532, acc: 0.541016]  [A loss: 0.875992, acc: 0.160156]\n",
      "3379: [D loss: 0.703058, acc: 0.517578]  [A loss: 0.753832, acc: 0.332031]\n",
      "3380: [D loss: 0.712288, acc: 0.519531]  [A loss: 0.892516, acc: 0.167969]\n",
      "3381: [D loss: 0.695063, acc: 0.513672]  [A loss: 0.702780, acc: 0.500000]\n",
      "3382: [D loss: 0.718642, acc: 0.519531]  [A loss: 0.968839, acc: 0.089844]\n",
      "3383: [D loss: 0.702722, acc: 0.519531]  [A loss: 0.765845, acc: 0.339844]\n",
      "3384: [D loss: 0.709046, acc: 0.507812]  [A loss: 0.868952, acc: 0.156250]\n",
      "3385: [D loss: 0.704300, acc: 0.505859]  [A loss: 0.772520, acc: 0.351562]\n",
      "3386: [D loss: 0.704309, acc: 0.515625]  [A loss: 0.867999, acc: 0.171875]\n",
      "3387: [D loss: 0.698901, acc: 0.505859]  [A loss: 0.772309, acc: 0.324219]\n",
      "3388: [D loss: 0.705238, acc: 0.511719]  [A loss: 0.881356, acc: 0.140625]\n",
      "3389: [D loss: 0.695716, acc: 0.537109]  [A loss: 0.740542, acc: 0.398438]\n",
      "3390: [D loss: 0.698996, acc: 0.531250]  [A loss: 0.874424, acc: 0.175781]\n",
      "3391: [D loss: 0.689787, acc: 0.566406]  [A loss: 0.739064, acc: 0.406250]\n",
      "3392: [D loss: 0.713482, acc: 0.494141]  [A loss: 0.968051, acc: 0.085938]\n",
      "3393: [D loss: 0.719643, acc: 0.490234]  [A loss: 0.692925, acc: 0.511719]\n",
      "3394: [D loss: 0.710184, acc: 0.515625]  [A loss: 0.911961, acc: 0.121094]\n",
      "3395: [D loss: 0.700725, acc: 0.525391]  [A loss: 0.705763, acc: 0.527344]\n",
      "3396: [D loss: 0.708065, acc: 0.521484]  [A loss: 0.841689, acc: 0.199219]\n",
      "3397: [D loss: 0.687496, acc: 0.529297]  [A loss: 0.714078, acc: 0.453125]\n",
      "3398: [D loss: 0.709715, acc: 0.511719]  [A loss: 0.929307, acc: 0.082031]\n",
      "3399: [D loss: 0.698991, acc: 0.529297]  [A loss: 0.672046, acc: 0.566406]\n",
      "3400: [D loss: 0.719607, acc: 0.542969]  [A loss: 0.992896, acc: 0.082031]\n",
      "3401: [D loss: 0.707119, acc: 0.498047]  [A loss: 0.752523, acc: 0.378906]\n",
      "3402: [D loss: 0.716947, acc: 0.470703]  [A loss: 0.817029, acc: 0.257812]\n",
      "3403: [D loss: 0.691395, acc: 0.572266]  [A loss: 0.804633, acc: 0.253906]\n",
      "3404: [D loss: 0.705092, acc: 0.519531]  [A loss: 0.880013, acc: 0.183594]\n",
      "3405: [D loss: 0.690359, acc: 0.544922]  [A loss: 0.753857, acc: 0.406250]\n",
      "3406: [D loss: 0.713851, acc: 0.519531]  [A loss: 0.863584, acc: 0.171875]\n",
      "3407: [D loss: 0.720634, acc: 0.480469]  [A loss: 0.810266, acc: 0.265625]\n",
      "3408: [D loss: 0.699376, acc: 0.529297]  [A loss: 0.813151, acc: 0.234375]\n",
      "3409: [D loss: 0.715929, acc: 0.498047]  [A loss: 0.926971, acc: 0.085938]\n",
      "3410: [D loss: 0.696222, acc: 0.527344]  [A loss: 0.716400, acc: 0.449219]\n",
      "3411: [D loss: 0.710629, acc: 0.515625]  [A loss: 1.013221, acc: 0.078125]\n",
      "3412: [D loss: 0.710620, acc: 0.503906]  [A loss: 0.671572, acc: 0.601562]\n",
      "3413: [D loss: 0.716781, acc: 0.523438]  [A loss: 0.863047, acc: 0.171875]\n",
      "3414: [D loss: 0.712855, acc: 0.503906]  [A loss: 0.797901, acc: 0.250000]\n",
      "3415: [D loss: 0.707019, acc: 0.517578]  [A loss: 0.836520, acc: 0.210938]\n",
      "3416: [D loss: 0.704716, acc: 0.492188]  [A loss: 0.806082, acc: 0.273438]\n",
      "3417: [D loss: 0.699047, acc: 0.523438]  [A loss: 0.851395, acc: 0.226562]\n",
      "3418: [D loss: 0.704299, acc: 0.505859]  [A loss: 0.785061, acc: 0.324219]\n",
      "3419: [D loss: 0.709524, acc: 0.507812]  [A loss: 0.897162, acc: 0.132812]\n",
      "3420: [D loss: 0.702968, acc: 0.490234]  [A loss: 0.797687, acc: 0.312500]\n",
      "3421: [D loss: 0.704389, acc: 0.521484]  [A loss: 0.895502, acc: 0.144531]\n",
      "3422: [D loss: 0.700079, acc: 0.509766]  [A loss: 0.690354, acc: 0.511719]\n",
      "3423: [D loss: 0.707359, acc: 0.523438]  [A loss: 0.951476, acc: 0.125000]\n",
      "3424: [D loss: 0.686095, acc: 0.541016]  [A loss: 0.713786, acc: 0.457031]\n",
      "3425: [D loss: 0.727877, acc: 0.501953]  [A loss: 0.972019, acc: 0.089844]\n",
      "3426: [D loss: 0.708282, acc: 0.482422]  [A loss: 0.705385, acc: 0.496094]\n",
      "3427: [D loss: 0.700733, acc: 0.535156]  [A loss: 0.921088, acc: 0.128906]\n",
      "3428: [D loss: 0.689124, acc: 0.533203]  [A loss: 0.757518, acc: 0.375000]\n",
      "3429: [D loss: 0.701444, acc: 0.542969]  [A loss: 0.896311, acc: 0.171875]\n",
      "3430: [D loss: 0.688245, acc: 0.550781]  [A loss: 0.764383, acc: 0.371094]\n",
      "3431: [D loss: 0.710893, acc: 0.496094]  [A loss: 0.921970, acc: 0.113281]\n",
      "3432: [D loss: 0.705640, acc: 0.474609]  [A loss: 0.675716, acc: 0.542969]\n",
      "3433: [D loss: 0.709349, acc: 0.521484]  [A loss: 1.019050, acc: 0.050781]\n",
      "3434: [D loss: 0.709816, acc: 0.515625]  [A loss: 0.670305, acc: 0.578125]\n",
      "3435: [D loss: 0.722176, acc: 0.515625]  [A loss: 0.901677, acc: 0.140625]\n",
      "3436: [D loss: 0.695827, acc: 0.503906]  [A loss: 0.757345, acc: 0.363281]\n",
      "3437: [D loss: 0.712858, acc: 0.527344]  [A loss: 0.867038, acc: 0.179688]\n",
      "3438: [D loss: 0.699677, acc: 0.525391]  [A loss: 0.749333, acc: 0.429688]\n",
      "3439: [D loss: 0.712355, acc: 0.498047]  [A loss: 0.855231, acc: 0.230469]\n",
      "3440: [D loss: 0.695558, acc: 0.523438]  [A loss: 0.800913, acc: 0.273438]\n",
      "3441: [D loss: 0.686232, acc: 0.546875]  [A loss: 0.840957, acc: 0.234375]\n",
      "3442: [D loss: 0.699478, acc: 0.511719]  [A loss: 0.749400, acc: 0.429688]\n",
      "3443: [D loss: 0.709451, acc: 0.500000]  [A loss: 0.955943, acc: 0.078125]\n",
      "3444: [D loss: 0.696703, acc: 0.519531]  [A loss: 0.669961, acc: 0.566406]\n",
      "3445: [D loss: 0.716130, acc: 0.515625]  [A loss: 0.958784, acc: 0.082031]\n",
      "3446: [D loss: 0.701638, acc: 0.496094]  [A loss: 0.740942, acc: 0.410156]\n",
      "3447: [D loss: 0.704182, acc: 0.527344]  [A loss: 0.872185, acc: 0.191406]\n",
      "3448: [D loss: 0.701438, acc: 0.519531]  [A loss: 0.815389, acc: 0.246094]\n",
      "3449: [D loss: 0.704954, acc: 0.490234]  [A loss: 0.817251, acc: 0.246094]\n",
      "3450: [D loss: 0.702504, acc: 0.537109]  [A loss: 0.882373, acc: 0.144531]\n",
      "3451: [D loss: 0.691039, acc: 0.535156]  [A loss: 0.677945, acc: 0.597656]\n",
      "3452: [D loss: 0.726689, acc: 0.519531]  [A loss: 1.016607, acc: 0.066406]\n",
      "3453: [D loss: 0.723297, acc: 0.476562]  [A loss: 0.777069, acc: 0.312500]\n",
      "3454: [D loss: 0.699862, acc: 0.533203]  [A loss: 0.844036, acc: 0.203125]\n",
      "3455: [D loss: 0.709923, acc: 0.503906]  [A loss: 0.799804, acc: 0.257812]\n",
      "3456: [D loss: 0.707184, acc: 0.501953]  [A loss: 0.841539, acc: 0.195312]\n",
      "3457: [D loss: 0.691107, acc: 0.544922]  [A loss: 0.804800, acc: 0.269531]\n",
      "3458: [D loss: 0.710225, acc: 0.505859]  [A loss: 0.788302, acc: 0.320312]\n",
      "3459: [D loss: 0.700657, acc: 0.527344]  [A loss: 0.973807, acc: 0.082031]\n",
      "3460: [D loss: 0.700790, acc: 0.521484]  [A loss: 0.682308, acc: 0.539062]\n",
      "3461: [D loss: 0.723730, acc: 0.515625]  [A loss: 1.126713, acc: 0.035156]\n",
      "3462: [D loss: 0.705379, acc: 0.519531]  [A loss: 0.647403, acc: 0.617188]\n",
      "3463: [D loss: 0.746233, acc: 0.496094]  [A loss: 0.915929, acc: 0.085938]\n",
      "3464: [D loss: 0.686639, acc: 0.541016]  [A loss: 0.729980, acc: 0.414062]\n",
      "3465: [D loss: 0.727290, acc: 0.498047]  [A loss: 0.981584, acc: 0.074219]\n",
      "3466: [D loss: 0.696677, acc: 0.539062]  [A loss: 0.672754, acc: 0.582031]\n",
      "3467: [D loss: 0.719956, acc: 0.494141]  [A loss: 0.879789, acc: 0.160156]\n",
      "3468: [D loss: 0.685176, acc: 0.562500]  [A loss: 0.742370, acc: 0.437500]\n",
      "3469: [D loss: 0.701382, acc: 0.527344]  [A loss: 0.761078, acc: 0.355469]\n",
      "3470: [D loss: 0.712059, acc: 0.525391]  [A loss: 0.787934, acc: 0.296875]\n",
      "3471: [D loss: 0.710274, acc: 0.507812]  [A loss: 0.776781, acc: 0.316406]\n",
      "3472: [D loss: 0.691891, acc: 0.525391]  [A loss: 0.822411, acc: 0.253906]\n",
      "3473: [D loss: 0.708903, acc: 0.484375]  [A loss: 0.793614, acc: 0.304688]\n",
      "3474: [D loss: 0.695197, acc: 0.550781]  [A loss: 0.801413, acc: 0.300781]\n",
      "3475: [D loss: 0.707172, acc: 0.494141]  [A loss: 0.810219, acc: 0.210938]\n",
      "3476: [D loss: 0.703866, acc: 0.505859]  [A loss: 0.892163, acc: 0.140625]\n",
      "3477: [D loss: 0.699274, acc: 0.503906]  [A loss: 0.767194, acc: 0.339844]\n",
      "3478: [D loss: 0.716364, acc: 0.535156]  [A loss: 0.929980, acc: 0.125000]\n",
      "3479: [D loss: 0.700860, acc: 0.541016]  [A loss: 0.835674, acc: 0.238281]\n",
      "3480: [D loss: 0.712384, acc: 0.488281]  [A loss: 0.919938, acc: 0.144531]\n",
      "3481: [D loss: 0.703536, acc: 0.509766]  [A loss: 0.760363, acc: 0.347656]\n",
      "3482: [D loss: 0.710918, acc: 0.521484]  [A loss: 0.916311, acc: 0.109375]\n",
      "3483: [D loss: 0.692363, acc: 0.535156]  [A loss: 0.812459, acc: 0.289062]\n",
      "3484: [D loss: 0.707160, acc: 0.515625]  [A loss: 0.879259, acc: 0.199219]\n",
      "3485: [D loss: 0.702358, acc: 0.498047]  [A loss: 0.740507, acc: 0.410156]\n",
      "3486: [D loss: 0.716447, acc: 0.509766]  [A loss: 0.981534, acc: 0.070312]\n",
      "3487: [D loss: 0.701304, acc: 0.517578]  [A loss: 0.680466, acc: 0.578125]\n",
      "3488: [D loss: 0.721766, acc: 0.501953]  [A loss: 0.921414, acc: 0.078125]\n",
      "3489: [D loss: 0.708987, acc: 0.486328]  [A loss: 0.662057, acc: 0.617188]\n",
      "3490: [D loss: 0.710132, acc: 0.529297]  [A loss: 0.950251, acc: 0.125000]\n",
      "3491: [D loss: 0.715562, acc: 0.511719]  [A loss: 0.775560, acc: 0.328125]\n",
      "3492: [D loss: 0.708765, acc: 0.484375]  [A loss: 0.811878, acc: 0.257812]\n",
      "3493: [D loss: 0.699269, acc: 0.505859]  [A loss: 0.744222, acc: 0.335938]\n",
      "3494: [D loss: 0.716021, acc: 0.521484]  [A loss: 0.960797, acc: 0.097656]\n",
      "3495: [D loss: 0.700365, acc: 0.501953]  [A loss: 0.672713, acc: 0.562500]\n",
      "3496: [D loss: 0.741525, acc: 0.484375]  [A loss: 1.026321, acc: 0.046875]\n",
      "3497: [D loss: 0.701361, acc: 0.525391]  [A loss: 0.682922, acc: 0.550781]\n",
      "3498: [D loss: 0.721453, acc: 0.511719]  [A loss: 0.894861, acc: 0.117188]\n",
      "3499: [D loss: 0.696815, acc: 0.500000]  [A loss: 0.729594, acc: 0.429688]\n",
      "3500: [D loss: 0.715131, acc: 0.503906]  [A loss: 0.901567, acc: 0.128906]\n",
      "3501: [D loss: 0.699800, acc: 0.517578]  [A loss: 0.712609, acc: 0.503906]\n",
      "3502: [D loss: 0.715838, acc: 0.511719]  [A loss: 0.871981, acc: 0.164062]\n",
      "3503: [D loss: 0.707080, acc: 0.505859]  [A loss: 0.775690, acc: 0.351562]\n",
      "3504: [D loss: 0.699346, acc: 0.535156]  [A loss: 0.838884, acc: 0.222656]\n",
      "3505: [D loss: 0.701462, acc: 0.527344]  [A loss: 0.749040, acc: 0.410156]\n",
      "3506: [D loss: 0.720476, acc: 0.501953]  [A loss: 0.917648, acc: 0.093750]\n",
      "3507: [D loss: 0.702295, acc: 0.525391]  [A loss: 0.697216, acc: 0.507812]\n",
      "3508: [D loss: 0.721694, acc: 0.494141]  [A loss: 0.890329, acc: 0.156250]\n",
      "3509: [D loss: 0.712673, acc: 0.474609]  [A loss: 0.764244, acc: 0.332031]\n",
      "3510: [D loss: 0.701350, acc: 0.546875]  [A loss: 0.826556, acc: 0.246094]\n",
      "3511: [D loss: 0.701769, acc: 0.509766]  [A loss: 0.820263, acc: 0.265625]\n",
      "3512: [D loss: 0.703985, acc: 0.505859]  [A loss: 0.816333, acc: 0.234375]\n",
      "3513: [D loss: 0.711439, acc: 0.494141]  [A loss: 0.861530, acc: 0.144531]\n",
      "3514: [D loss: 0.696574, acc: 0.505859]  [A loss: 0.788671, acc: 0.292969]\n",
      "3515: [D loss: 0.700165, acc: 0.521484]  [A loss: 0.926798, acc: 0.125000]\n",
      "3516: [D loss: 0.707014, acc: 0.513672]  [A loss: 0.690420, acc: 0.527344]\n",
      "3517: [D loss: 0.717407, acc: 0.531250]  [A loss: 0.983819, acc: 0.117188]\n",
      "3518: [D loss: 0.701656, acc: 0.523438]  [A loss: 0.726845, acc: 0.472656]\n",
      "3519: [D loss: 0.719008, acc: 0.501953]  [A loss: 0.871369, acc: 0.207031]\n",
      "3520: [D loss: 0.723700, acc: 0.492188]  [A loss: 0.828004, acc: 0.222656]\n",
      "3521: [D loss: 0.710633, acc: 0.537109]  [A loss: 0.835412, acc: 0.199219]\n",
      "3522: [D loss: 0.705917, acc: 0.515625]  [A loss: 0.821057, acc: 0.234375]\n",
      "3523: [D loss: 0.708062, acc: 0.488281]  [A loss: 0.855906, acc: 0.171875]\n",
      "3524: [D loss: 0.702543, acc: 0.498047]  [A loss: 0.902792, acc: 0.148438]\n",
      "3525: [D loss: 0.692070, acc: 0.527344]  [A loss: 0.752324, acc: 0.375000]\n",
      "3526: [D loss: 0.700924, acc: 0.527344]  [A loss: 0.938272, acc: 0.113281]\n",
      "3527: [D loss: 0.704068, acc: 0.509766]  [A loss: 0.705519, acc: 0.507812]\n",
      "3528: [D loss: 0.714590, acc: 0.519531]  [A loss: 1.021625, acc: 0.031250]\n",
      "3529: [D loss: 0.696596, acc: 0.519531]  [A loss: 0.696451, acc: 0.550781]\n",
      "3530: [D loss: 0.737915, acc: 0.498047]  [A loss: 0.949807, acc: 0.046875]\n",
      "3531: [D loss: 0.693860, acc: 0.529297]  [A loss: 0.660576, acc: 0.609375]\n",
      "3532: [D loss: 0.739602, acc: 0.517578]  [A loss: 1.021839, acc: 0.066406]\n",
      "3533: [D loss: 0.698846, acc: 0.519531]  [A loss: 0.702058, acc: 0.472656]\n",
      "3534: [D loss: 0.728651, acc: 0.513672]  [A loss: 0.877332, acc: 0.183594]\n",
      "3535: [D loss: 0.698020, acc: 0.501953]  [A loss: 0.749251, acc: 0.382812]\n",
      "3536: [D loss: 0.711339, acc: 0.513672]  [A loss: 0.887506, acc: 0.156250]\n",
      "3537: [D loss: 0.703091, acc: 0.505859]  [A loss: 0.757493, acc: 0.382812]\n",
      "3538: [D loss: 0.688266, acc: 0.556641]  [A loss: 0.827997, acc: 0.261719]\n",
      "3539: [D loss: 0.704037, acc: 0.501953]  [A loss: 0.775923, acc: 0.320312]\n",
      "3540: [D loss: 0.706323, acc: 0.509766]  [A loss: 0.835399, acc: 0.222656]\n",
      "3541: [D loss: 0.705422, acc: 0.498047]  [A loss: 0.838071, acc: 0.238281]\n",
      "3542: [D loss: 0.689460, acc: 0.550781]  [A loss: 0.828191, acc: 0.199219]\n",
      "3543: [D loss: 0.690986, acc: 0.544922]  [A loss: 0.834370, acc: 0.226562]\n",
      "3544: [D loss: 0.704661, acc: 0.509766]  [A loss: 0.849215, acc: 0.191406]\n",
      "3545: [D loss: 0.700711, acc: 0.523438]  [A loss: 0.797296, acc: 0.312500]\n",
      "3546: [D loss: 0.706109, acc: 0.531250]  [A loss: 0.906453, acc: 0.128906]\n",
      "3547: [D loss: 0.705972, acc: 0.515625]  [A loss: 0.758344, acc: 0.367188]\n",
      "3548: [D loss: 0.689331, acc: 0.564453]  [A loss: 0.917274, acc: 0.140625]\n",
      "3549: [D loss: 0.708270, acc: 0.486328]  [A loss: 0.765722, acc: 0.386719]\n",
      "3550: [D loss: 0.714886, acc: 0.492188]  [A loss: 0.953733, acc: 0.082031]\n",
      "3551: [D loss: 0.706313, acc: 0.515625]  [A loss: 0.745633, acc: 0.394531]\n",
      "3552: [D loss: 0.698205, acc: 0.503906]  [A loss: 1.014318, acc: 0.054688]\n",
      "3553: [D loss: 0.706215, acc: 0.498047]  [A loss: 0.629819, acc: 0.683594]\n",
      "3554: [D loss: 0.753362, acc: 0.500000]  [A loss: 0.989801, acc: 0.082031]\n",
      "3555: [D loss: 0.719409, acc: 0.496094]  [A loss: 0.789813, acc: 0.281250]\n",
      "3556: [D loss: 0.711889, acc: 0.505859]  [A loss: 0.818503, acc: 0.187500]\n",
      "3557: [D loss: 0.709591, acc: 0.507812]  [A loss: 0.809773, acc: 0.261719]\n",
      "3558: [D loss: 0.694525, acc: 0.552734]  [A loss: 0.809725, acc: 0.285156]\n",
      "3559: [D loss: 0.692759, acc: 0.533203]  [A loss: 0.755739, acc: 0.371094]\n",
      "3560: [D loss: 0.698282, acc: 0.529297]  [A loss: 0.894743, acc: 0.140625]\n",
      "3561: [D loss: 0.682178, acc: 0.570312]  [A loss: 0.766437, acc: 0.339844]\n",
      "3562: [D loss: 0.712337, acc: 0.501953]  [A loss: 0.961759, acc: 0.082031]\n",
      "3563: [D loss: 0.702739, acc: 0.515625]  [A loss: 0.709072, acc: 0.476562]\n",
      "3564: [D loss: 0.710759, acc: 0.519531]  [A loss: 0.962093, acc: 0.089844]\n",
      "3565: [D loss: 0.709214, acc: 0.517578]  [A loss: 0.677662, acc: 0.554688]\n",
      "3566: [D loss: 0.725527, acc: 0.501953]  [A loss: 0.907252, acc: 0.140625]\n",
      "3567: [D loss: 0.686111, acc: 0.542969]  [A loss: 0.710507, acc: 0.500000]\n",
      "3568: [D loss: 0.721698, acc: 0.515625]  [A loss: 0.889243, acc: 0.156250]\n",
      "3569: [D loss: 0.699559, acc: 0.533203]  [A loss: 0.772751, acc: 0.347656]\n",
      "3570: [D loss: 0.698533, acc: 0.529297]  [A loss: 0.848445, acc: 0.226562]\n",
      "3571: [D loss: 0.702646, acc: 0.535156]  [A loss: 0.762558, acc: 0.367188]\n",
      "3572: [D loss: 0.706043, acc: 0.519531]  [A loss: 0.789237, acc: 0.300781]\n",
      "3573: [D loss: 0.707068, acc: 0.535156]  [A loss: 0.852292, acc: 0.234375]\n",
      "3574: [D loss: 0.697278, acc: 0.537109]  [A loss: 0.738851, acc: 0.414062]\n",
      "3575: [D loss: 0.707337, acc: 0.498047]  [A loss: 0.848813, acc: 0.207031]\n",
      "3576: [D loss: 0.698713, acc: 0.515625]  [A loss: 0.784558, acc: 0.351562]\n",
      "3577: [D loss: 0.715236, acc: 0.503906]  [A loss: 0.927840, acc: 0.136719]\n",
      "3578: [D loss: 0.693227, acc: 0.556641]  [A loss: 0.725110, acc: 0.441406]\n",
      "3579: [D loss: 0.714127, acc: 0.519531]  [A loss: 1.011668, acc: 0.070312]\n",
      "3580: [D loss: 0.706333, acc: 0.501953]  [A loss: 0.648249, acc: 0.675781]\n",
      "3581: [D loss: 0.737672, acc: 0.500000]  [A loss: 0.890219, acc: 0.179688]\n",
      "3582: [D loss: 0.704797, acc: 0.500000]  [A loss: 0.748354, acc: 0.402344]\n",
      "3583: [D loss: 0.705448, acc: 0.523438]  [A loss: 0.832828, acc: 0.238281]\n",
      "3584: [D loss: 0.706919, acc: 0.490234]  [A loss: 0.735647, acc: 0.406250]\n",
      "3585: [D loss: 0.714777, acc: 0.515625]  [A loss: 0.914889, acc: 0.140625]\n",
      "3586: [D loss: 0.696734, acc: 0.503906]  [A loss: 0.699832, acc: 0.519531]\n",
      "3587: [D loss: 0.738545, acc: 0.498047]  [A loss: 0.936022, acc: 0.117188]\n",
      "3588: [D loss: 0.701173, acc: 0.527344]  [A loss: 0.748045, acc: 0.414062]\n",
      "3589: [D loss: 0.704460, acc: 0.527344]  [A loss: 0.824819, acc: 0.234375]\n",
      "3590: [D loss: 0.702325, acc: 0.537109]  [A loss: 0.813775, acc: 0.277344]\n",
      "3591: [D loss: 0.689502, acc: 0.537109]  [A loss: 0.741312, acc: 0.375000]\n",
      "3592: [D loss: 0.715708, acc: 0.537109]  [A loss: 0.961858, acc: 0.074219]\n",
      "3593: [D loss: 0.703349, acc: 0.509766]  [A loss: 0.673844, acc: 0.570312]\n",
      "3594: [D loss: 0.713173, acc: 0.511719]  [A loss: 0.956432, acc: 0.066406]\n",
      "3595: [D loss: 0.705638, acc: 0.503906]  [A loss: 0.694616, acc: 0.546875]\n",
      "3596: [D loss: 0.726291, acc: 0.496094]  [A loss: 0.951375, acc: 0.101562]\n",
      "3597: [D loss: 0.689425, acc: 0.541016]  [A loss: 0.715292, acc: 0.472656]\n",
      "3598: [D loss: 0.724373, acc: 0.492188]  [A loss: 0.931615, acc: 0.117188]\n",
      "3599: [D loss: 0.693637, acc: 0.535156]  [A loss: 0.677707, acc: 0.562500]\n",
      "3600: [D loss: 0.716017, acc: 0.505859]  [A loss: 0.873610, acc: 0.167969]\n",
      "3601: [D loss: 0.693653, acc: 0.546875]  [A loss: 0.786938, acc: 0.304688]\n",
      "3602: [D loss: 0.712278, acc: 0.492188]  [A loss: 0.831355, acc: 0.238281]\n",
      "3603: [D loss: 0.697485, acc: 0.509766]  [A loss: 0.738829, acc: 0.398438]\n",
      "3604: [D loss: 0.713182, acc: 0.511719]  [A loss: 0.883598, acc: 0.144531]\n",
      "3605: [D loss: 0.702604, acc: 0.501953]  [A loss: 0.767774, acc: 0.347656]\n",
      "3606: [D loss: 0.700913, acc: 0.521484]  [A loss: 0.829483, acc: 0.230469]\n",
      "3607: [D loss: 0.695143, acc: 0.523438]  [A loss: 0.782645, acc: 0.316406]\n",
      "3608: [D loss: 0.711930, acc: 0.494141]  [A loss: 0.895180, acc: 0.152344]\n",
      "3609: [D loss: 0.685715, acc: 0.574219]  [A loss: 0.747286, acc: 0.390625]\n",
      "3610: [D loss: 0.715415, acc: 0.476562]  [A loss: 0.913269, acc: 0.101562]\n",
      "3611: [D loss: 0.694131, acc: 0.546875]  [A loss: 0.849344, acc: 0.203125]\n",
      "3612: [D loss: 0.704401, acc: 0.501953]  [A loss: 0.871460, acc: 0.199219]\n",
      "3613: [D loss: 0.701696, acc: 0.509766]  [A loss: 0.835361, acc: 0.203125]\n",
      "3614: [D loss: 0.688203, acc: 0.558594]  [A loss: 0.850750, acc: 0.222656]\n",
      "3615: [D loss: 0.709055, acc: 0.505859]  [A loss: 0.767759, acc: 0.335938]\n",
      "3616: [D loss: 0.700248, acc: 0.533203]  [A loss: 0.877570, acc: 0.191406]\n",
      "3617: [D loss: 0.682629, acc: 0.548828]  [A loss: 0.775738, acc: 0.382812]\n",
      "3618: [D loss: 0.709709, acc: 0.507812]  [A loss: 0.898532, acc: 0.152344]\n",
      "3619: [D loss: 0.711453, acc: 0.513672]  [A loss: 0.826364, acc: 0.207031]\n",
      "3620: [D loss: 0.702598, acc: 0.527344]  [A loss: 0.919468, acc: 0.113281]\n",
      "3621: [D loss: 0.688781, acc: 0.542969]  [A loss: 0.743375, acc: 0.386719]\n",
      "3622: [D loss: 0.730832, acc: 0.494141]  [A loss: 1.044088, acc: 0.070312]\n",
      "3623: [D loss: 0.697523, acc: 0.554688]  [A loss: 0.662685, acc: 0.570312]\n",
      "3624: [D loss: 0.718502, acc: 0.521484]  [A loss: 1.006156, acc: 0.062500]\n",
      "3625: [D loss: 0.683691, acc: 0.546875]  [A loss: 0.675495, acc: 0.562500]\n",
      "3626: [D loss: 0.728433, acc: 0.482422]  [A loss: 0.945244, acc: 0.105469]\n",
      "3627: [D loss: 0.713290, acc: 0.515625]  [A loss: 0.773628, acc: 0.355469]\n",
      "3628: [D loss: 0.696572, acc: 0.519531]  [A loss: 0.850628, acc: 0.195312]\n",
      "3629: [D loss: 0.704660, acc: 0.484375]  [A loss: 0.808694, acc: 0.269531]\n",
      "3630: [D loss: 0.695251, acc: 0.527344]  [A loss: 0.805680, acc: 0.273438]\n",
      "3631: [D loss: 0.705246, acc: 0.494141]  [A loss: 0.851833, acc: 0.246094]\n",
      "3632: [D loss: 0.697776, acc: 0.560547]  [A loss: 0.824361, acc: 0.246094]\n",
      "3633: [D loss: 0.702603, acc: 0.513672]  [A loss: 0.861276, acc: 0.222656]\n",
      "3634: [D loss: 0.711770, acc: 0.482422]  [A loss: 0.809573, acc: 0.273438]\n",
      "3635: [D loss: 0.718811, acc: 0.488281]  [A loss: 0.918560, acc: 0.148438]\n",
      "3636: [D loss: 0.705075, acc: 0.505859]  [A loss: 0.733512, acc: 0.468750]\n",
      "3637: [D loss: 0.709503, acc: 0.494141]  [A loss: 0.847325, acc: 0.238281]\n",
      "3638: [D loss: 0.707964, acc: 0.490234]  [A loss: 0.832193, acc: 0.257812]\n",
      "3639: [D loss: 0.690193, acc: 0.535156]  [A loss: 0.906454, acc: 0.148438]\n",
      "3640: [D loss: 0.702794, acc: 0.539062]  [A loss: 0.740171, acc: 0.386719]\n",
      "3641: [D loss: 0.713703, acc: 0.478516]  [A loss: 0.888708, acc: 0.195312]\n",
      "3642: [D loss: 0.695013, acc: 0.535156]  [A loss: 0.668159, acc: 0.593750]\n",
      "3643: [D loss: 0.729018, acc: 0.500000]  [A loss: 0.999668, acc: 0.089844]\n",
      "3644: [D loss: 0.703572, acc: 0.533203]  [A loss: 0.753416, acc: 0.406250]\n",
      "3645: [D loss: 0.724293, acc: 0.513672]  [A loss: 0.906165, acc: 0.160156]\n",
      "3646: [D loss: 0.691580, acc: 0.527344]  [A loss: 0.676871, acc: 0.558594]\n",
      "3647: [D loss: 0.729924, acc: 0.505859]  [A loss: 0.997476, acc: 0.074219]\n",
      "3648: [D loss: 0.689432, acc: 0.550781]  [A loss: 0.722191, acc: 0.457031]\n",
      "3649: [D loss: 0.709052, acc: 0.533203]  [A loss: 0.840901, acc: 0.218750]\n",
      "3650: [D loss: 0.699188, acc: 0.529297]  [A loss: 0.798081, acc: 0.277344]\n",
      "3651: [D loss: 0.709706, acc: 0.527344]  [A loss: 0.875172, acc: 0.167969]\n",
      "3652: [D loss: 0.711046, acc: 0.490234]  [A loss: 0.819979, acc: 0.261719]\n",
      "3653: [D loss: 0.686222, acc: 0.527344]  [A loss: 0.801588, acc: 0.273438]\n",
      "3654: [D loss: 0.705481, acc: 0.513672]  [A loss: 0.887154, acc: 0.195312]\n",
      "3655: [D loss: 0.714077, acc: 0.496094]  [A loss: 0.837847, acc: 0.203125]\n",
      "3656: [D loss: 0.701548, acc: 0.503906]  [A loss: 0.886011, acc: 0.156250]\n",
      "3657: [D loss: 0.687357, acc: 0.564453]  [A loss: 0.781591, acc: 0.312500]\n",
      "3658: [D loss: 0.710308, acc: 0.496094]  [A loss: 0.980696, acc: 0.082031]\n",
      "3659: [D loss: 0.695113, acc: 0.537109]  [A loss: 0.692514, acc: 0.507812]\n",
      "3660: [D loss: 0.722066, acc: 0.527344]  [A loss: 0.975089, acc: 0.085938]\n",
      "3661: [D loss: 0.695415, acc: 0.519531]  [A loss: 0.711382, acc: 0.457031]\n",
      "3662: [D loss: 0.705037, acc: 0.505859]  [A loss: 0.926586, acc: 0.144531]\n",
      "3663: [D loss: 0.694779, acc: 0.527344]  [A loss: 0.715597, acc: 0.453125]\n",
      "3664: [D loss: 0.726033, acc: 0.513672]  [A loss: 0.966777, acc: 0.105469]\n",
      "3665: [D loss: 0.709972, acc: 0.503906]  [A loss: 0.712835, acc: 0.500000]\n",
      "3666: [D loss: 0.720537, acc: 0.486328]  [A loss: 0.863950, acc: 0.183594]\n",
      "3667: [D loss: 0.704614, acc: 0.511719]  [A loss: 0.764918, acc: 0.394531]\n",
      "3668: [D loss: 0.711947, acc: 0.519531]  [A loss: 0.890734, acc: 0.164062]\n",
      "3669: [D loss: 0.700128, acc: 0.505859]  [A loss: 0.756222, acc: 0.375000]\n",
      "3670: [D loss: 0.713556, acc: 0.546875]  [A loss: 0.949535, acc: 0.093750]\n",
      "3671: [D loss: 0.695707, acc: 0.533203]  [A loss: 0.718823, acc: 0.472656]\n",
      "3672: [D loss: 0.722578, acc: 0.511719]  [A loss: 0.994154, acc: 0.093750]\n",
      "3673: [D loss: 0.705780, acc: 0.529297]  [A loss: 0.723484, acc: 0.464844]\n",
      "3674: [D loss: 0.720290, acc: 0.515625]  [A loss: 0.891176, acc: 0.125000]\n",
      "3675: [D loss: 0.701479, acc: 0.511719]  [A loss: 0.792938, acc: 0.304688]\n",
      "3676: [D loss: 0.710218, acc: 0.505859]  [A loss: 0.857606, acc: 0.207031]\n",
      "3677: [D loss: 0.708099, acc: 0.484375]  [A loss: 0.764033, acc: 0.343750]\n",
      "3678: [D loss: 0.701767, acc: 0.556641]  [A loss: 0.858737, acc: 0.253906]\n",
      "3679: [D loss: 0.709967, acc: 0.490234]  [A loss: 0.831151, acc: 0.214844]\n",
      "3680: [D loss: 0.706183, acc: 0.501953]  [A loss: 0.938024, acc: 0.128906]\n",
      "3681: [D loss: 0.710217, acc: 0.505859]  [A loss: 0.740035, acc: 0.410156]\n",
      "3682: [D loss: 0.719454, acc: 0.500000]  [A loss: 1.000345, acc: 0.078125]\n",
      "3683: [D loss: 0.706311, acc: 0.496094]  [A loss: 0.649066, acc: 0.617188]\n",
      "3684: [D loss: 0.723099, acc: 0.515625]  [A loss: 0.993651, acc: 0.070312]\n",
      "3685: [D loss: 0.707846, acc: 0.500000]  [A loss: 0.690268, acc: 0.531250]\n",
      "3686: [D loss: 0.735395, acc: 0.496094]  [A loss: 1.019135, acc: 0.062500]\n",
      "3687: [D loss: 0.700415, acc: 0.537109]  [A loss: 0.789859, acc: 0.339844]\n",
      "3688: [D loss: 0.709246, acc: 0.507812]  [A loss: 0.903260, acc: 0.136719]\n",
      "3689: [D loss: 0.695360, acc: 0.525391]  [A loss: 0.730276, acc: 0.472656]\n",
      "3690: [D loss: 0.715575, acc: 0.498047]  [A loss: 0.829510, acc: 0.230469]\n",
      "3691: [D loss: 0.705193, acc: 0.501953]  [A loss: 0.810145, acc: 0.304688]\n",
      "3692: [D loss: 0.703244, acc: 0.511719]  [A loss: 0.840985, acc: 0.199219]\n",
      "3693: [D loss: 0.696025, acc: 0.511719]  [A loss: 0.821565, acc: 0.277344]\n",
      "3694: [D loss: 0.705246, acc: 0.523438]  [A loss: 0.820859, acc: 0.273438]\n",
      "3695: [D loss: 0.712918, acc: 0.507812]  [A loss: 0.853930, acc: 0.218750]\n",
      "3696: [D loss: 0.701690, acc: 0.509766]  [A loss: 0.832528, acc: 0.265625]\n",
      "3697: [D loss: 0.705713, acc: 0.511719]  [A loss: 0.892828, acc: 0.128906]\n",
      "3698: [D loss: 0.695452, acc: 0.531250]  [A loss: 0.822670, acc: 0.226562]\n",
      "3699: [D loss: 0.699262, acc: 0.511719]  [A loss: 0.963611, acc: 0.125000]\n",
      "3700: [D loss: 0.684214, acc: 0.570312]  [A loss: 0.659970, acc: 0.621094]\n",
      "3701: [D loss: 0.729218, acc: 0.492188]  [A loss: 0.987035, acc: 0.097656]\n",
      "3702: [D loss: 0.701544, acc: 0.505859]  [A loss: 0.669633, acc: 0.578125]\n",
      "3703: [D loss: 0.736068, acc: 0.519531]  [A loss: 0.998124, acc: 0.085938]\n",
      "3704: [D loss: 0.712926, acc: 0.484375]  [A loss: 0.767475, acc: 0.359375]\n",
      "3705: [D loss: 0.711513, acc: 0.527344]  [A loss: 0.899853, acc: 0.171875]\n",
      "3706: [D loss: 0.696731, acc: 0.533203]  [A loss: 0.715330, acc: 0.449219]\n",
      "3707: [D loss: 0.706866, acc: 0.554688]  [A loss: 0.925617, acc: 0.128906]\n",
      "3708: [D loss: 0.703364, acc: 0.505859]  [A loss: 0.698383, acc: 0.503906]\n",
      "3709: [D loss: 0.708098, acc: 0.541016]  [A loss: 0.932113, acc: 0.109375]\n",
      "3710: [D loss: 0.689170, acc: 0.568359]  [A loss: 0.703808, acc: 0.507812]\n",
      "3711: [D loss: 0.727492, acc: 0.488281]  [A loss: 0.925072, acc: 0.125000]\n",
      "3712: [D loss: 0.713494, acc: 0.490234]  [A loss: 0.724546, acc: 0.457031]\n",
      "3713: [D loss: 0.714571, acc: 0.500000]  [A loss: 0.906372, acc: 0.128906]\n",
      "3714: [D loss: 0.695556, acc: 0.509766]  [A loss: 0.791206, acc: 0.308594]\n",
      "3715: [D loss: 0.709613, acc: 0.517578]  [A loss: 0.856997, acc: 0.179688]\n",
      "3716: [D loss: 0.697315, acc: 0.511719]  [A loss: 0.769716, acc: 0.351562]\n",
      "3717: [D loss: 0.715380, acc: 0.490234]  [A loss: 0.898996, acc: 0.187500]\n",
      "3718: [D loss: 0.697644, acc: 0.517578]  [A loss: 0.731308, acc: 0.441406]\n",
      "3719: [D loss: 0.733174, acc: 0.503906]  [A loss: 1.017629, acc: 0.093750]\n",
      "3720: [D loss: 0.710511, acc: 0.505859]  [A loss: 0.680841, acc: 0.562500]\n",
      "3721: [D loss: 0.729624, acc: 0.509766]  [A loss: 0.864435, acc: 0.214844]\n",
      "3722: [D loss: 0.703610, acc: 0.525391]  [A loss: 0.779980, acc: 0.304688]\n",
      "3723: [D loss: 0.720534, acc: 0.500000]  [A loss: 0.870701, acc: 0.183594]\n",
      "3724: [D loss: 0.693012, acc: 0.531250]  [A loss: 0.763542, acc: 0.363281]\n",
      "3725: [D loss: 0.697868, acc: 0.523438]  [A loss: 0.899123, acc: 0.191406]\n",
      "3726: [D loss: 0.689148, acc: 0.521484]  [A loss: 0.827325, acc: 0.257812]\n",
      "3727: [D loss: 0.710054, acc: 0.505859]  [A loss: 0.928819, acc: 0.109375]\n",
      "3728: [D loss: 0.706235, acc: 0.498047]  [A loss: 0.689479, acc: 0.542969]\n",
      "3729: [D loss: 0.734631, acc: 0.515625]  [A loss: 1.069213, acc: 0.050781]\n",
      "3730: [D loss: 0.707050, acc: 0.513672]  [A loss: 0.629836, acc: 0.675781]\n",
      "3731: [D loss: 0.735461, acc: 0.496094]  [A loss: 0.909161, acc: 0.117188]\n",
      "3732: [D loss: 0.698024, acc: 0.500000]  [A loss: 0.691075, acc: 0.535156]\n",
      "3733: [D loss: 0.729258, acc: 0.509766]  [A loss: 0.907676, acc: 0.144531]\n",
      "3734: [D loss: 0.694547, acc: 0.539062]  [A loss: 0.719363, acc: 0.441406]\n",
      "3735: [D loss: 0.705777, acc: 0.539062]  [A loss: 0.821032, acc: 0.230469]\n",
      "3736: [D loss: 0.710103, acc: 0.484375]  [A loss: 0.801309, acc: 0.253906]\n",
      "3737: [D loss: 0.714001, acc: 0.501953]  [A loss: 0.823201, acc: 0.257812]\n",
      "3738: [D loss: 0.696105, acc: 0.511719]  [A loss: 0.841399, acc: 0.218750]\n",
      "3739: [D loss: 0.707497, acc: 0.505859]  [A loss: 0.814265, acc: 0.296875]\n",
      "3740: [D loss: 0.720291, acc: 0.478516]  [A loss: 0.822438, acc: 0.261719]\n",
      "3741: [D loss: 0.697830, acc: 0.521484]  [A loss: 0.945896, acc: 0.085938]\n",
      "3742: [D loss: 0.706231, acc: 0.523438]  [A loss: 0.685718, acc: 0.539062]\n",
      "3743: [D loss: 0.727161, acc: 0.498047]  [A loss: 0.935501, acc: 0.113281]\n",
      "3744: [D loss: 0.693737, acc: 0.523438]  [A loss: 0.716640, acc: 0.468750]\n",
      "3745: [D loss: 0.711956, acc: 0.505859]  [A loss: 0.939714, acc: 0.109375]\n",
      "3746: [D loss: 0.696970, acc: 0.515625]  [A loss: 0.712677, acc: 0.492188]\n",
      "3747: [D loss: 0.722115, acc: 0.501953]  [A loss: 0.989677, acc: 0.085938]\n",
      "3748: [D loss: 0.696965, acc: 0.523438]  [A loss: 0.713384, acc: 0.503906]\n",
      "3749: [D loss: 0.726785, acc: 0.492188]  [A loss: 0.890767, acc: 0.136719]\n",
      "3750: [D loss: 0.708725, acc: 0.511719]  [A loss: 0.729635, acc: 0.390625]\n",
      "3751: [D loss: 0.725146, acc: 0.494141]  [A loss: 1.003150, acc: 0.050781]\n",
      "3752: [D loss: 0.700621, acc: 0.544922]  [A loss: 0.716957, acc: 0.488281]\n",
      "3753: [D loss: 0.719734, acc: 0.531250]  [A loss: 0.891158, acc: 0.136719]\n",
      "3754: [D loss: 0.700447, acc: 0.513672]  [A loss: 0.733254, acc: 0.429688]\n",
      "3755: [D loss: 0.710056, acc: 0.527344]  [A loss: 0.896646, acc: 0.175781]\n",
      "3756: [D loss: 0.722875, acc: 0.498047]  [A loss: 0.793204, acc: 0.289062]\n",
      "3757: [D loss: 0.713080, acc: 0.501953]  [A loss: 0.909160, acc: 0.144531]\n",
      "3758: [D loss: 0.707420, acc: 0.498047]  [A loss: 0.720912, acc: 0.449219]\n",
      "3759: [D loss: 0.717059, acc: 0.505859]  [A loss: 0.898692, acc: 0.175781]\n",
      "3760: [D loss: 0.705473, acc: 0.531250]  [A loss: 0.746645, acc: 0.390625]\n",
      "3761: [D loss: 0.704764, acc: 0.537109]  [A loss: 0.877123, acc: 0.187500]\n",
      "3762: [D loss: 0.683327, acc: 0.566406]  [A loss: 0.780771, acc: 0.343750]\n",
      "3763: [D loss: 0.708186, acc: 0.517578]  [A loss: 0.919531, acc: 0.152344]\n",
      "3764: [D loss: 0.702236, acc: 0.519531]  [A loss: 0.695497, acc: 0.519531]\n",
      "3765: [D loss: 0.721255, acc: 0.521484]  [A loss: 0.967429, acc: 0.097656]\n",
      "3766: [D loss: 0.696112, acc: 0.509766]  [A loss: 0.732290, acc: 0.433594]\n",
      "3767: [D loss: 0.720205, acc: 0.482422]  [A loss: 0.873112, acc: 0.207031]\n",
      "3768: [D loss: 0.704038, acc: 0.509766]  [A loss: 0.752540, acc: 0.363281]\n",
      "3769: [D loss: 0.723411, acc: 0.476562]  [A loss: 0.946331, acc: 0.054688]\n",
      "3770: [D loss: 0.699836, acc: 0.537109]  [A loss: 0.745516, acc: 0.398438]\n",
      "3771: [D loss: 0.716668, acc: 0.525391]  [A loss: 0.949540, acc: 0.054688]\n",
      "3772: [D loss: 0.705816, acc: 0.517578]  [A loss: 0.713792, acc: 0.492188]\n",
      "3773: [D loss: 0.711073, acc: 0.521484]  [A loss: 0.911927, acc: 0.144531]\n",
      "3774: [D loss: 0.702237, acc: 0.505859]  [A loss: 0.732527, acc: 0.437500]\n",
      "3775: [D loss: 0.699602, acc: 0.531250]  [A loss: 0.851792, acc: 0.203125]\n",
      "3776: [D loss: 0.695354, acc: 0.525391]  [A loss: 0.759097, acc: 0.378906]\n",
      "3777: [D loss: 0.703991, acc: 0.537109]  [A loss: 0.855519, acc: 0.230469]\n",
      "3778: [D loss: 0.701012, acc: 0.550781]  [A loss: 0.795781, acc: 0.296875]\n",
      "3779: [D loss: 0.701804, acc: 0.517578]  [A loss: 0.857082, acc: 0.222656]\n",
      "3780: [D loss: 0.691442, acc: 0.533203]  [A loss: 0.819550, acc: 0.238281]\n",
      "3781: [D loss: 0.709233, acc: 0.501953]  [A loss: 0.894685, acc: 0.160156]\n",
      "3782: [D loss: 0.710550, acc: 0.505859]  [A loss: 0.773897, acc: 0.324219]\n",
      "3783: [D loss: 0.687592, acc: 0.546875]  [A loss: 0.944226, acc: 0.144531]\n",
      "3784: [D loss: 0.708516, acc: 0.521484]  [A loss: 0.760081, acc: 0.355469]\n",
      "3785: [D loss: 0.715854, acc: 0.503906]  [A loss: 0.909263, acc: 0.144531]\n",
      "3786: [D loss: 0.703477, acc: 0.505859]  [A loss: 0.764169, acc: 0.375000]\n",
      "3787: [D loss: 0.724048, acc: 0.517578]  [A loss: 0.957979, acc: 0.125000]\n",
      "3788: [D loss: 0.697130, acc: 0.535156]  [A loss: 0.746697, acc: 0.398438]\n",
      "3789: [D loss: 0.719067, acc: 0.507812]  [A loss: 0.920416, acc: 0.136719]\n",
      "3790: [D loss: 0.714240, acc: 0.492188]  [A loss: 0.722988, acc: 0.437500]\n",
      "3791: [D loss: 0.720199, acc: 0.496094]  [A loss: 0.908606, acc: 0.148438]\n",
      "3792: [D loss: 0.695774, acc: 0.529297]  [A loss: 0.729787, acc: 0.437500]\n",
      "3793: [D loss: 0.721719, acc: 0.503906]  [A loss: 0.952011, acc: 0.078125]\n",
      "3794: [D loss: 0.706738, acc: 0.513672]  [A loss: 0.654406, acc: 0.625000]\n",
      "3795: [D loss: 0.739266, acc: 0.496094]  [A loss: 0.994994, acc: 0.089844]\n",
      "3796: [D loss: 0.703216, acc: 0.542969]  [A loss: 0.691268, acc: 0.550781]\n",
      "3797: [D loss: 0.726961, acc: 0.519531]  [A loss: 0.883420, acc: 0.136719]\n",
      "3798: [D loss: 0.683810, acc: 0.541016]  [A loss: 0.772266, acc: 0.332031]\n",
      "3799: [D loss: 0.710889, acc: 0.527344]  [A loss: 0.843607, acc: 0.238281]\n",
      "3800: [D loss: 0.699150, acc: 0.548828]  [A loss: 0.739226, acc: 0.425781]\n",
      "3801: [D loss: 0.719789, acc: 0.486328]  [A loss: 0.912577, acc: 0.160156]\n",
      "3802: [D loss: 0.696021, acc: 0.509766]  [A loss: 0.789563, acc: 0.300781]\n",
      "3803: [D loss: 0.703872, acc: 0.496094]  [A loss: 0.780462, acc: 0.339844]\n",
      "3804: [D loss: 0.708751, acc: 0.500000]  [A loss: 0.861893, acc: 0.222656]\n",
      "3805: [D loss: 0.701636, acc: 0.523438]  [A loss: 0.778230, acc: 0.324219]\n",
      "3806: [D loss: 0.705220, acc: 0.535156]  [A loss: 0.881406, acc: 0.210938]\n",
      "3807: [D loss: 0.705254, acc: 0.533203]  [A loss: 0.872225, acc: 0.207031]\n",
      "3808: [D loss: 0.694008, acc: 0.542969]  [A loss: 0.735267, acc: 0.414062]\n",
      "3809: [D loss: 0.714993, acc: 0.498047]  [A loss: 0.917809, acc: 0.152344]\n",
      "3810: [D loss: 0.707475, acc: 0.500000]  [A loss: 0.740666, acc: 0.417969]\n",
      "3811: [D loss: 0.732037, acc: 0.486328]  [A loss: 0.932932, acc: 0.105469]\n",
      "3812: [D loss: 0.694109, acc: 0.529297]  [A loss: 0.731343, acc: 0.402344]\n",
      "3813: [D loss: 0.730573, acc: 0.519531]  [A loss: 1.095470, acc: 0.039062]\n",
      "3814: [D loss: 0.719835, acc: 0.466797]  [A loss: 0.726000, acc: 0.441406]\n",
      "3815: [D loss: 0.717620, acc: 0.488281]  [A loss: 0.912478, acc: 0.125000]\n",
      "3816: [D loss: 0.697580, acc: 0.501953]  [A loss: 0.723911, acc: 0.449219]\n",
      "3817: [D loss: 0.712690, acc: 0.521484]  [A loss: 0.948726, acc: 0.085938]\n",
      "3818: [D loss: 0.713851, acc: 0.496094]  [A loss: 0.736259, acc: 0.421875]\n",
      "3819: [D loss: 0.701756, acc: 0.527344]  [A loss: 0.918490, acc: 0.148438]\n",
      "3820: [D loss: 0.699756, acc: 0.513672]  [A loss: 0.736206, acc: 0.453125]\n",
      "3821: [D loss: 0.715001, acc: 0.494141]  [A loss: 0.841049, acc: 0.253906]\n",
      "3822: [D loss: 0.706437, acc: 0.523438]  [A loss: 0.799020, acc: 0.328125]\n",
      "3823: [D loss: 0.695958, acc: 0.517578]  [A loss: 0.833761, acc: 0.265625]\n",
      "3824: [D loss: 0.717733, acc: 0.488281]  [A loss: 0.835409, acc: 0.261719]\n",
      "3825: [D loss: 0.698961, acc: 0.539062]  [A loss: 0.844529, acc: 0.234375]\n",
      "3826: [D loss: 0.699618, acc: 0.527344]  [A loss: 0.823780, acc: 0.257812]\n",
      "3827: [D loss: 0.704440, acc: 0.537109]  [A loss: 0.867776, acc: 0.199219]\n",
      "3828: [D loss: 0.696124, acc: 0.533203]  [A loss: 0.815366, acc: 0.261719]\n",
      "3829: [D loss: 0.714876, acc: 0.503906]  [A loss: 0.849795, acc: 0.226562]\n",
      "3830: [D loss: 0.692446, acc: 0.525391]  [A loss: 0.737258, acc: 0.437500]\n",
      "3831: [D loss: 0.713943, acc: 0.517578]  [A loss: 1.018941, acc: 0.097656]\n",
      "3832: [D loss: 0.698954, acc: 0.531250]  [A loss: 0.673201, acc: 0.546875]\n",
      "3833: [D loss: 0.744510, acc: 0.511719]  [A loss: 1.061130, acc: 0.066406]\n",
      "3834: [D loss: 0.710242, acc: 0.501953]  [A loss: 0.680670, acc: 0.554688]\n",
      "3835: [D loss: 0.753343, acc: 0.505859]  [A loss: 0.861239, acc: 0.238281]\n",
      "3836: [D loss: 0.707252, acc: 0.509766]  [A loss: 0.830899, acc: 0.285156]\n",
      "3837: [D loss: 0.714603, acc: 0.513672]  [A loss: 0.815444, acc: 0.261719]\n",
      "3838: [D loss: 0.710089, acc: 0.527344]  [A loss: 0.893262, acc: 0.167969]\n",
      "3839: [D loss: 0.703247, acc: 0.517578]  [A loss: 0.784303, acc: 0.332031]\n",
      "3840: [D loss: 0.705650, acc: 0.501953]  [A loss: 0.880064, acc: 0.164062]\n",
      "3841: [D loss: 0.704304, acc: 0.492188]  [A loss: 0.824834, acc: 0.269531]\n",
      "3842: [D loss: 0.720047, acc: 0.496094]  [A loss: 0.903624, acc: 0.160156]\n",
      "3843: [D loss: 0.714576, acc: 0.470703]  [A loss: 0.753931, acc: 0.359375]\n",
      "3844: [D loss: 0.721370, acc: 0.482422]  [A loss: 0.949697, acc: 0.113281]\n",
      "3845: [D loss: 0.710776, acc: 0.501953]  [A loss: 0.777835, acc: 0.363281]\n",
      "3846: [D loss: 0.712867, acc: 0.515625]  [A loss: 0.895017, acc: 0.152344]\n",
      "3847: [D loss: 0.716578, acc: 0.458984]  [A loss: 0.782623, acc: 0.378906]\n",
      "3848: [D loss: 0.704126, acc: 0.541016]  [A loss: 0.884317, acc: 0.187500]\n",
      "3849: [D loss: 0.712158, acc: 0.492188]  [A loss: 0.775216, acc: 0.355469]\n",
      "3850: [D loss: 0.705940, acc: 0.521484]  [A loss: 0.977062, acc: 0.097656]\n",
      "3851: [D loss: 0.711010, acc: 0.517578]  [A loss: 0.691784, acc: 0.542969]\n",
      "3852: [D loss: 0.733586, acc: 0.500000]  [A loss: 0.971605, acc: 0.082031]\n",
      "3853: [D loss: 0.699956, acc: 0.505859]  [A loss: 0.719496, acc: 0.449219]\n",
      "3854: [D loss: 0.711562, acc: 0.531250]  [A loss: 1.020674, acc: 0.062500]\n",
      "3855: [D loss: 0.702078, acc: 0.533203]  [A loss: 0.663055, acc: 0.640625]\n",
      "3856: [D loss: 0.745645, acc: 0.515625]  [A loss: 0.888655, acc: 0.156250]\n",
      "3857: [D loss: 0.697254, acc: 0.533203]  [A loss: 0.753647, acc: 0.351562]\n",
      "3858: [D loss: 0.713619, acc: 0.498047]  [A loss: 0.870399, acc: 0.195312]\n",
      "3859: [D loss: 0.711418, acc: 0.486328]  [A loss: 0.790626, acc: 0.320312]\n",
      "3860: [D loss: 0.725078, acc: 0.496094]  [A loss: 0.923688, acc: 0.101562]\n",
      "3861: [D loss: 0.698675, acc: 0.527344]  [A loss: 0.791706, acc: 0.324219]\n",
      "3862: [D loss: 0.713182, acc: 0.513672]  [A loss: 0.919670, acc: 0.125000]\n",
      "3863: [D loss: 0.689225, acc: 0.548828]  [A loss: 0.751156, acc: 0.402344]\n",
      "3864: [D loss: 0.706654, acc: 0.539062]  [A loss: 0.958489, acc: 0.066406]\n",
      "3865: [D loss: 0.710427, acc: 0.517578]  [A loss: 0.714249, acc: 0.460938]\n",
      "3866: [D loss: 0.725885, acc: 0.476562]  [A loss: 1.012936, acc: 0.070312]\n",
      "3867: [D loss: 0.693737, acc: 0.513672]  [A loss: 0.756683, acc: 0.371094]\n",
      "3868: [D loss: 0.718521, acc: 0.511719]  [A loss: 0.982531, acc: 0.128906]\n",
      "3869: [D loss: 0.695981, acc: 0.523438]  [A loss: 0.759076, acc: 0.359375]\n",
      "3870: [D loss: 0.728955, acc: 0.472656]  [A loss: 0.902484, acc: 0.179688]\n",
      "3871: [D loss: 0.699338, acc: 0.505859]  [A loss: 0.759404, acc: 0.335938]\n",
      "3872: [D loss: 0.706645, acc: 0.515625]  [A loss: 0.804670, acc: 0.308594]\n",
      "3873: [D loss: 0.709714, acc: 0.507812]  [A loss: 0.916411, acc: 0.160156]\n",
      "3874: [D loss: 0.713804, acc: 0.466797]  [A loss: 0.753455, acc: 0.402344]\n",
      "3875: [D loss: 0.714167, acc: 0.517578]  [A loss: 0.984904, acc: 0.082031]\n",
      "3876: [D loss: 0.713382, acc: 0.503906]  [A loss: 0.647645, acc: 0.625000]\n",
      "3877: [D loss: 0.736915, acc: 0.509766]  [A loss: 0.988368, acc: 0.117188]\n",
      "3878: [D loss: 0.713117, acc: 0.500000]  [A loss: 0.693334, acc: 0.554688]\n",
      "3879: [D loss: 0.703257, acc: 0.513672]  [A loss: 0.895426, acc: 0.183594]\n",
      "3880: [D loss: 0.705683, acc: 0.517578]  [A loss: 0.775969, acc: 0.308594]\n",
      "3881: [D loss: 0.716562, acc: 0.509766]  [A loss: 0.877359, acc: 0.195312]\n",
      "3882: [D loss: 0.692031, acc: 0.550781]  [A loss: 0.777623, acc: 0.343750]\n",
      "3883: [D loss: 0.715114, acc: 0.523438]  [A loss: 0.916700, acc: 0.156250]\n",
      "3884: [D loss: 0.692891, acc: 0.523438]  [A loss: 0.752332, acc: 0.390625]\n",
      "3885: [D loss: 0.699298, acc: 0.535156]  [A loss: 0.898142, acc: 0.199219]\n",
      "3886: [D loss: 0.698716, acc: 0.527344]  [A loss: 0.833810, acc: 0.230469]\n",
      "3887: [D loss: 0.696377, acc: 0.535156]  [A loss: 0.926774, acc: 0.125000]\n",
      "3888: [D loss: 0.705581, acc: 0.492188]  [A loss: 0.798202, acc: 0.285156]\n",
      "3889: [D loss: 0.708703, acc: 0.511719]  [A loss: 0.851876, acc: 0.222656]\n",
      "3890: [D loss: 0.701368, acc: 0.531250]  [A loss: 0.756526, acc: 0.398438]\n",
      "3891: [D loss: 0.712473, acc: 0.523438]  [A loss: 0.945573, acc: 0.125000]\n",
      "3892: [D loss: 0.703987, acc: 0.509766]  [A loss: 0.701003, acc: 0.488281]\n",
      "3893: [D loss: 0.730903, acc: 0.500000]  [A loss: 1.024538, acc: 0.093750]\n",
      "3894: [D loss: 0.704112, acc: 0.521484]  [A loss: 0.630100, acc: 0.644531]\n",
      "3895: [D loss: 0.764530, acc: 0.501953]  [A loss: 0.929576, acc: 0.140625]\n",
      "3896: [D loss: 0.702620, acc: 0.511719]  [A loss: 0.719769, acc: 0.453125]\n",
      "3897: [D loss: 0.730386, acc: 0.501953]  [A loss: 0.989070, acc: 0.062500]\n",
      "3898: [D loss: 0.688328, acc: 0.558594]  [A loss: 0.729727, acc: 0.425781]\n",
      "3899: [D loss: 0.710950, acc: 0.542969]  [A loss: 0.795432, acc: 0.242188]\n",
      "3900: [D loss: 0.715833, acc: 0.484375]  [A loss: 0.759732, acc: 0.378906]\n",
      "3901: [D loss: 0.716533, acc: 0.501953]  [A loss: 0.903476, acc: 0.164062]\n",
      "3902: [D loss: 0.704176, acc: 0.505859]  [A loss: 0.756342, acc: 0.394531]\n",
      "3903: [D loss: 0.712643, acc: 0.525391]  [A loss: 0.883164, acc: 0.187500]\n",
      "3904: [D loss: 0.699114, acc: 0.544922]  [A loss: 0.705331, acc: 0.468750]\n",
      "3905: [D loss: 0.726838, acc: 0.523438]  [A loss: 0.989682, acc: 0.105469]\n",
      "3906: [D loss: 0.699626, acc: 0.517578]  [A loss: 0.711939, acc: 0.464844]\n",
      "3907: [D loss: 0.728004, acc: 0.503906]  [A loss: 0.871927, acc: 0.199219]\n",
      "3908: [D loss: 0.724355, acc: 0.484375]  [A loss: 0.765553, acc: 0.410156]\n",
      "3909: [D loss: 0.701408, acc: 0.539062]  [A loss: 0.851715, acc: 0.238281]\n",
      "3910: [D loss: 0.700797, acc: 0.515625]  [A loss: 0.835745, acc: 0.250000]\n",
      "3911: [D loss: 0.708648, acc: 0.513672]  [A loss: 0.793232, acc: 0.320312]\n",
      "3912: [D loss: 0.704279, acc: 0.515625]  [A loss: 0.826864, acc: 0.308594]\n",
      "3913: [D loss: 0.697642, acc: 0.517578]  [A loss: 0.763186, acc: 0.382812]\n",
      "3914: [D loss: 0.704094, acc: 0.511719]  [A loss: 0.892539, acc: 0.171875]\n",
      "3915: [D loss: 0.708700, acc: 0.484375]  [A loss: 0.769545, acc: 0.367188]\n",
      "3916: [D loss: 0.702460, acc: 0.519531]  [A loss: 0.947109, acc: 0.121094]\n",
      "3917: [D loss: 0.711149, acc: 0.498047]  [A loss: 0.668823, acc: 0.574219]\n",
      "3918: [D loss: 0.739551, acc: 0.505859]  [A loss: 1.091071, acc: 0.031250]\n",
      "3919: [D loss: 0.702713, acc: 0.494141]  [A loss: 0.705483, acc: 0.500000]\n",
      "3920: [D loss: 0.710939, acc: 0.529297]  [A loss: 0.893338, acc: 0.160156]\n",
      "3921: [D loss: 0.704265, acc: 0.529297]  [A loss: 0.784036, acc: 0.332031]\n",
      "3922: [D loss: 0.701641, acc: 0.525391]  [A loss: 0.895486, acc: 0.152344]\n",
      "3923: [D loss: 0.702563, acc: 0.535156]  [A loss: 0.749617, acc: 0.394531]\n",
      "3924: [D loss: 0.705369, acc: 0.533203]  [A loss: 0.911525, acc: 0.144531]\n",
      "3925: [D loss: 0.696736, acc: 0.535156]  [A loss: 0.745377, acc: 0.417969]\n",
      "3926: [D loss: 0.710647, acc: 0.515625]  [A loss: 0.911884, acc: 0.144531]\n",
      "3927: [D loss: 0.711731, acc: 0.503906]  [A loss: 0.722361, acc: 0.453125]\n",
      "3928: [D loss: 0.713302, acc: 0.535156]  [A loss: 0.975581, acc: 0.105469]\n",
      "3929: [D loss: 0.715598, acc: 0.521484]  [A loss: 0.821706, acc: 0.265625]\n",
      "3930: [D loss: 0.706221, acc: 0.519531]  [A loss: 0.859096, acc: 0.203125]\n",
      "3931: [D loss: 0.710541, acc: 0.527344]  [A loss: 0.912567, acc: 0.148438]\n",
      "3932: [D loss: 0.699659, acc: 0.517578]  [A loss: 0.691550, acc: 0.492188]\n",
      "3933: [D loss: 0.718437, acc: 0.505859]  [A loss: 0.978438, acc: 0.097656]\n",
      "3934: [D loss: 0.698893, acc: 0.529297]  [A loss: 0.704535, acc: 0.500000]\n",
      "3935: [D loss: 0.718511, acc: 0.509766]  [A loss: 0.878214, acc: 0.167969]\n",
      "3936: [D loss: 0.706309, acc: 0.505859]  [A loss: 0.749112, acc: 0.402344]\n",
      "3937: [D loss: 0.712556, acc: 0.525391]  [A loss: 0.947307, acc: 0.109375]\n",
      "3938: [D loss: 0.694404, acc: 0.523438]  [A loss: 0.649657, acc: 0.640625]\n",
      "3939: [D loss: 0.723107, acc: 0.486328]  [A loss: 0.983726, acc: 0.089844]\n",
      "3940: [D loss: 0.699612, acc: 0.519531]  [A loss: 0.726486, acc: 0.468750]\n",
      "3941: [D loss: 0.727727, acc: 0.505859]  [A loss: 0.908814, acc: 0.156250]\n",
      "3942: [D loss: 0.695842, acc: 0.544922]  [A loss: 0.736572, acc: 0.421875]\n",
      "3943: [D loss: 0.715024, acc: 0.513672]  [A loss: 0.960626, acc: 0.113281]\n",
      "3944: [D loss: 0.701570, acc: 0.511719]  [A loss: 0.756776, acc: 0.382812]\n",
      "3945: [D loss: 0.727089, acc: 0.490234]  [A loss: 0.922095, acc: 0.152344]\n",
      "3946: [D loss: 0.701057, acc: 0.537109]  [A loss: 0.717411, acc: 0.496094]\n",
      "3947: [D loss: 0.719480, acc: 0.509766]  [A loss: 0.970787, acc: 0.117188]\n",
      "3948: [D loss: 0.696830, acc: 0.521484]  [A loss: 0.693818, acc: 0.523438]\n",
      "3949: [D loss: 0.732517, acc: 0.500000]  [A loss: 1.008516, acc: 0.074219]\n",
      "3950: [D loss: 0.694942, acc: 0.509766]  [A loss: 0.692511, acc: 0.511719]\n",
      "3951: [D loss: 0.723940, acc: 0.507812]  [A loss: 0.921313, acc: 0.167969]\n",
      "3952: [D loss: 0.721661, acc: 0.488281]  [A loss: 0.740507, acc: 0.402344]\n",
      "3953: [D loss: 0.719644, acc: 0.507812]  [A loss: 0.893363, acc: 0.140625]\n",
      "3954: [D loss: 0.710629, acc: 0.521484]  [A loss: 0.743772, acc: 0.433594]\n",
      "3955: [D loss: 0.722140, acc: 0.496094]  [A loss: 0.873646, acc: 0.191406]\n",
      "3956: [D loss: 0.705265, acc: 0.529297]  [A loss: 0.758596, acc: 0.390625]\n",
      "3957: [D loss: 0.692301, acc: 0.529297]  [A loss: 0.864509, acc: 0.199219]\n",
      "3958: [D loss: 0.709401, acc: 0.494141]  [A loss: 0.802571, acc: 0.285156]\n",
      "3959: [D loss: 0.713968, acc: 0.488281]  [A loss: 0.826801, acc: 0.246094]\n",
      "3960: [D loss: 0.708673, acc: 0.503906]  [A loss: 0.862366, acc: 0.222656]\n",
      "3961: [D loss: 0.692754, acc: 0.537109]  [A loss: 0.772227, acc: 0.371094]\n",
      "3962: [D loss: 0.717332, acc: 0.515625]  [A loss: 0.903988, acc: 0.175781]\n",
      "3963: [D loss: 0.704726, acc: 0.505859]  [A loss: 0.810652, acc: 0.281250]\n",
      "3964: [D loss: 0.694904, acc: 0.529297]  [A loss: 0.809145, acc: 0.285156]\n",
      "3965: [D loss: 0.724149, acc: 0.494141]  [A loss: 0.862607, acc: 0.210938]\n",
      "3966: [D loss: 0.716689, acc: 0.519531]  [A loss: 0.891227, acc: 0.164062]\n",
      "3967: [D loss: 0.691838, acc: 0.539062]  [A loss: 0.753379, acc: 0.406250]\n",
      "3968: [D loss: 0.718709, acc: 0.498047]  [A loss: 0.926434, acc: 0.164062]\n",
      "3969: [D loss: 0.697852, acc: 0.525391]  [A loss: 0.683893, acc: 0.535156]\n",
      "3970: [D loss: 0.713636, acc: 0.531250]  [A loss: 1.022534, acc: 0.062500]\n",
      "3971: [D loss: 0.702254, acc: 0.503906]  [A loss: 0.733249, acc: 0.464844]\n",
      "3972: [D loss: 0.723818, acc: 0.521484]  [A loss: 0.968741, acc: 0.101562]\n",
      "3973: [D loss: 0.694825, acc: 0.560547]  [A loss: 0.695233, acc: 0.507812]\n",
      "3974: [D loss: 0.711136, acc: 0.509766]  [A loss: 0.953305, acc: 0.121094]\n",
      "3975: [D loss: 0.709211, acc: 0.505859]  [A loss: 0.755965, acc: 0.378906]\n",
      "3976: [D loss: 0.717647, acc: 0.490234]  [A loss: 0.957163, acc: 0.105469]\n",
      "3977: [D loss: 0.705921, acc: 0.505859]  [A loss: 0.712733, acc: 0.492188]\n",
      "3978: [D loss: 0.722084, acc: 0.513672]  [A loss: 0.945821, acc: 0.078125]\n",
      "3979: [D loss: 0.689556, acc: 0.564453]  [A loss: 0.706073, acc: 0.515625]\n",
      "3980: [D loss: 0.721072, acc: 0.507812]  [A loss: 0.975494, acc: 0.109375]\n",
      "3981: [D loss: 0.708150, acc: 0.496094]  [A loss: 0.694042, acc: 0.496094]\n",
      "3982: [D loss: 0.733996, acc: 0.515625]  [A loss: 0.912188, acc: 0.125000]\n",
      "3983: [D loss: 0.706709, acc: 0.515625]  [A loss: 0.729968, acc: 0.468750]\n",
      "3984: [D loss: 0.705296, acc: 0.517578]  [A loss: 0.850800, acc: 0.218750]\n",
      "3985: [D loss: 0.704308, acc: 0.515625]  [A loss: 0.701739, acc: 0.550781]\n",
      "3986: [D loss: 0.716975, acc: 0.535156]  [A loss: 0.986369, acc: 0.101562]\n",
      "3987: [D loss: 0.711280, acc: 0.490234]  [A loss: 0.746714, acc: 0.382812]\n",
      "3988: [D loss: 0.725838, acc: 0.488281]  [A loss: 0.896429, acc: 0.152344]\n",
      "3989: [D loss: 0.697377, acc: 0.548828]  [A loss: 0.696863, acc: 0.488281]\n",
      "3990: [D loss: 0.708072, acc: 0.509766]  [A loss: 0.874273, acc: 0.191406]\n",
      "3991: [D loss: 0.694608, acc: 0.531250]  [A loss: 0.748924, acc: 0.406250]\n",
      "3992: [D loss: 0.705044, acc: 0.529297]  [A loss: 0.860738, acc: 0.242188]\n",
      "3993: [D loss: 0.704141, acc: 0.517578]  [A loss: 0.742277, acc: 0.421875]\n",
      "3994: [D loss: 0.719562, acc: 0.466797]  [A loss: 0.825406, acc: 0.304688]\n",
      "3995: [D loss: 0.703918, acc: 0.521484]  [A loss: 0.833062, acc: 0.261719]\n",
      "3996: [D loss: 0.703070, acc: 0.509766]  [A loss: 0.814725, acc: 0.250000]\n",
      "3997: [D loss: 0.709905, acc: 0.519531]  [A loss: 0.892442, acc: 0.207031]\n",
      "3998: [D loss: 0.707233, acc: 0.519531]  [A loss: 0.767515, acc: 0.355469]\n",
      "3999: [D loss: 0.705706, acc: 0.535156]  [A loss: 0.895454, acc: 0.167969]\n",
      "4000: [D loss: 0.707499, acc: 0.515625]  [A loss: 0.809112, acc: 0.281250]\n",
      "4001: [D loss: 0.703176, acc: 0.500000]  [A loss: 0.836409, acc: 0.222656]\n",
      "4002: [D loss: 0.704984, acc: 0.515625]  [A loss: 0.806162, acc: 0.292969]\n",
      "4003: [D loss: 0.706414, acc: 0.544922]  [A loss: 0.874357, acc: 0.179688]\n",
      "4004: [D loss: 0.699604, acc: 0.523438]  [A loss: 0.750690, acc: 0.394531]\n",
      "4005: [D loss: 0.706719, acc: 0.519531]  [A loss: 0.919845, acc: 0.148438]\n",
      "4006: [D loss: 0.719067, acc: 0.462891]  [A loss: 0.749576, acc: 0.421875]\n",
      "4007: [D loss: 0.704503, acc: 0.537109]  [A loss: 0.939429, acc: 0.128906]\n",
      "4008: [D loss: 0.705500, acc: 0.490234]  [A loss: 0.729397, acc: 0.464844]\n",
      "4009: [D loss: 0.721423, acc: 0.529297]  [A loss: 1.013497, acc: 0.066406]\n",
      "4010: [D loss: 0.705272, acc: 0.542969]  [A loss: 0.676327, acc: 0.550781]\n",
      "4011: [D loss: 0.726742, acc: 0.503906]  [A loss: 0.977342, acc: 0.089844]\n",
      "4012: [D loss: 0.718518, acc: 0.480469]  [A loss: 0.751214, acc: 0.390625]\n",
      "4013: [D loss: 0.723478, acc: 0.507812]  [A loss: 0.888208, acc: 0.152344]\n",
      "4014: [D loss: 0.705762, acc: 0.492188]  [A loss: 0.786325, acc: 0.308594]\n",
      "4015: [D loss: 0.714030, acc: 0.488281]  [A loss: 0.872540, acc: 0.148438]\n",
      "4016: [D loss: 0.697499, acc: 0.531250]  [A loss: 0.775046, acc: 0.367188]\n",
      "4017: [D loss: 0.722417, acc: 0.509766]  [A loss: 0.885004, acc: 0.179688]\n",
      "4018: [D loss: 0.702949, acc: 0.521484]  [A loss: 0.742517, acc: 0.410156]\n",
      "4019: [D loss: 0.725611, acc: 0.513672]  [A loss: 0.893190, acc: 0.167969]\n",
      "4020: [D loss: 0.705517, acc: 0.484375]  [A loss: 0.778753, acc: 0.316406]\n",
      "4021: [D loss: 0.701320, acc: 0.539062]  [A loss: 0.838520, acc: 0.246094]\n",
      "4022: [D loss: 0.695638, acc: 0.539062]  [A loss: 0.874815, acc: 0.195312]\n",
      "4023: [D loss: 0.716037, acc: 0.509766]  [A loss: 0.875907, acc: 0.191406]\n",
      "4024: [D loss: 0.699812, acc: 0.501953]  [A loss: 0.771981, acc: 0.406250]\n",
      "4025: [D loss: 0.721943, acc: 0.482422]  [A loss: 0.957744, acc: 0.125000]\n",
      "4026: [D loss: 0.693959, acc: 0.517578]  [A loss: 0.747181, acc: 0.410156]\n",
      "4027: [D loss: 0.710104, acc: 0.519531]  [A loss: 0.919417, acc: 0.140625]\n",
      "4028: [D loss: 0.701711, acc: 0.523438]  [A loss: 0.765415, acc: 0.375000]\n",
      "4029: [D loss: 0.712113, acc: 0.490234]  [A loss: 0.933853, acc: 0.101562]\n",
      "4030: [D loss: 0.683665, acc: 0.578125]  [A loss: 0.706313, acc: 0.496094]\n",
      "4031: [D loss: 0.716417, acc: 0.511719]  [A loss: 0.968508, acc: 0.097656]\n",
      "4032: [D loss: 0.711550, acc: 0.490234]  [A loss: 0.739577, acc: 0.453125]\n",
      "4033: [D loss: 0.717325, acc: 0.501953]  [A loss: 0.880045, acc: 0.175781]\n",
      "4034: [D loss: 0.708255, acc: 0.511719]  [A loss: 0.827416, acc: 0.230469]\n",
      "4035: [D loss: 0.709452, acc: 0.509766]  [A loss: 0.877973, acc: 0.214844]\n",
      "4036: [D loss: 0.700297, acc: 0.521484]  [A loss: 0.782692, acc: 0.378906]\n",
      "4037: [D loss: 0.705442, acc: 0.517578]  [A loss: 0.847638, acc: 0.234375]\n",
      "4038: [D loss: 0.706178, acc: 0.509766]  [A loss: 0.861384, acc: 0.199219]\n",
      "4039: [D loss: 0.722934, acc: 0.480469]  [A loss: 0.904484, acc: 0.128906]\n",
      "4040: [D loss: 0.683560, acc: 0.566406]  [A loss: 0.729458, acc: 0.425781]\n",
      "4041: [D loss: 0.735183, acc: 0.492188]  [A loss: 1.181612, acc: 0.011719]\n",
      "4042: [D loss: 0.708236, acc: 0.535156]  [A loss: 0.616168, acc: 0.660156]\n",
      "4043: [D loss: 0.735412, acc: 0.509766]  [A loss: 0.938755, acc: 0.152344]\n",
      "4044: [D loss: 0.690836, acc: 0.535156]  [A loss: 0.719230, acc: 0.492188]\n",
      "4045: [D loss: 0.710819, acc: 0.509766]  [A loss: 0.874620, acc: 0.183594]\n",
      "4046: [D loss: 0.687068, acc: 0.548828]  [A loss: 0.784438, acc: 0.339844]\n",
      "4047: [D loss: 0.704001, acc: 0.523438]  [A loss: 0.823089, acc: 0.281250]\n",
      "4048: [D loss: 0.688667, acc: 0.539062]  [A loss: 0.791712, acc: 0.343750]\n",
      "4049: [D loss: 0.729903, acc: 0.474609]  [A loss: 0.827575, acc: 0.238281]\n",
      "4050: [D loss: 0.694411, acc: 0.539062]  [A loss: 0.755317, acc: 0.390625]\n",
      "4051: [D loss: 0.708238, acc: 0.531250]  [A loss: 0.936714, acc: 0.117188]\n",
      "4052: [D loss: 0.703521, acc: 0.498047]  [A loss: 0.781159, acc: 0.339844]\n",
      "4053: [D loss: 0.705702, acc: 0.517578]  [A loss: 0.878244, acc: 0.164062]\n",
      "4054: [D loss: 0.697868, acc: 0.550781]  [A loss: 0.783919, acc: 0.355469]\n",
      "4055: [D loss: 0.695642, acc: 0.525391]  [A loss: 0.982520, acc: 0.097656]\n",
      "4056: [D loss: 0.696011, acc: 0.525391]  [A loss: 0.708938, acc: 0.496094]\n",
      "4057: [D loss: 0.717934, acc: 0.511719]  [A loss: 0.998097, acc: 0.093750]\n",
      "4058: [D loss: 0.701502, acc: 0.537109]  [A loss: 0.610661, acc: 0.730469]\n",
      "4059: [D loss: 0.748992, acc: 0.498047]  [A loss: 1.006594, acc: 0.085938]\n",
      "4060: [D loss: 0.695752, acc: 0.539062]  [A loss: 0.683217, acc: 0.511719]\n",
      "4061: [D loss: 0.746440, acc: 0.500000]  [A loss: 0.942879, acc: 0.140625]\n",
      "4062: [D loss: 0.699379, acc: 0.525391]  [A loss: 0.742514, acc: 0.414062]\n",
      "4063: [D loss: 0.722652, acc: 0.492188]  [A loss: 0.851203, acc: 0.214844]\n",
      "4064: [D loss: 0.702730, acc: 0.525391]  [A loss: 0.807323, acc: 0.277344]\n",
      "4065: [D loss: 0.701194, acc: 0.521484]  [A loss: 0.785987, acc: 0.347656]\n",
      "4066: [D loss: 0.704084, acc: 0.505859]  [A loss: 0.807739, acc: 0.253906]\n",
      "4067: [D loss: 0.703104, acc: 0.511719]  [A loss: 0.818862, acc: 0.250000]\n",
      "4068: [D loss: 0.697935, acc: 0.537109]  [A loss: 0.863931, acc: 0.226562]\n",
      "4069: [D loss: 0.713674, acc: 0.468750]  [A loss: 0.816075, acc: 0.300781]\n",
      "4070: [D loss: 0.705446, acc: 0.542969]  [A loss: 0.841660, acc: 0.203125]\n",
      "4071: [D loss: 0.697673, acc: 0.535156]  [A loss: 0.792970, acc: 0.328125]\n",
      "4072: [D loss: 0.711468, acc: 0.515625]  [A loss: 0.921019, acc: 0.121094]\n",
      "4073: [D loss: 0.700886, acc: 0.531250]  [A loss: 0.728493, acc: 0.441406]\n",
      "4074: [D loss: 0.718993, acc: 0.513672]  [A loss: 0.967107, acc: 0.109375]\n",
      "4075: [D loss: 0.704605, acc: 0.525391]  [A loss: 0.724075, acc: 0.476562]\n",
      "4076: [D loss: 0.717135, acc: 0.511719]  [A loss: 0.861438, acc: 0.214844]\n",
      "4077: [D loss: 0.717582, acc: 0.478516]  [A loss: 0.772353, acc: 0.355469]\n",
      "4078: [D loss: 0.707145, acc: 0.527344]  [A loss: 0.901426, acc: 0.164062]\n",
      "4079: [D loss: 0.700397, acc: 0.531250]  [A loss: 0.751677, acc: 0.417969]\n",
      "4080: [D loss: 0.695869, acc: 0.544922]  [A loss: 0.861014, acc: 0.226562]\n",
      "4081: [D loss: 0.710680, acc: 0.521484]  [A loss: 0.731207, acc: 0.457031]\n",
      "4082: [D loss: 0.719412, acc: 0.517578]  [A loss: 0.935460, acc: 0.093750]\n",
      "4083: [D loss: 0.696848, acc: 0.546875]  [A loss: 0.689564, acc: 0.554688]\n",
      "4084: [D loss: 0.723656, acc: 0.511719]  [A loss: 0.947632, acc: 0.101562]\n",
      "4085: [D loss: 0.703958, acc: 0.542969]  [A loss: 0.674319, acc: 0.558594]\n",
      "4086: [D loss: 0.732192, acc: 0.494141]  [A loss: 0.934253, acc: 0.132812]\n",
      "4087: [D loss: 0.705935, acc: 0.494141]  [A loss: 0.694604, acc: 0.531250]\n",
      "4088: [D loss: 0.734225, acc: 0.511719]  [A loss: 1.002493, acc: 0.105469]\n",
      "4089: [D loss: 0.711637, acc: 0.525391]  [A loss: 0.725896, acc: 0.437500]\n",
      "4090: [D loss: 0.714497, acc: 0.513672]  [A loss: 0.887737, acc: 0.160156]\n",
      "4091: [D loss: 0.682880, acc: 0.560547]  [A loss: 0.686198, acc: 0.519531]\n",
      "4092: [D loss: 0.715636, acc: 0.513672]  [A loss: 0.907764, acc: 0.171875]\n",
      "4093: [D loss: 0.705626, acc: 0.515625]  [A loss: 0.705648, acc: 0.539062]\n",
      "4094: [D loss: 0.711385, acc: 0.513672]  [A loss: 0.875656, acc: 0.171875]\n",
      "4095: [D loss: 0.701173, acc: 0.537109]  [A loss: 0.732242, acc: 0.449219]\n",
      "4096: [D loss: 0.710985, acc: 0.529297]  [A loss: 0.942651, acc: 0.109375]\n",
      "4097: [D loss: 0.678990, acc: 0.574219]  [A loss: 0.716963, acc: 0.476562]\n",
      "4098: [D loss: 0.728663, acc: 0.478516]  [A loss: 0.911291, acc: 0.140625]\n",
      "4099: [D loss: 0.690027, acc: 0.533203]  [A loss: 0.772156, acc: 0.367188]\n",
      "4100: [D loss: 0.699246, acc: 0.521484]  [A loss: 0.883296, acc: 0.195312]\n",
      "4101: [D loss: 0.686504, acc: 0.552734]  [A loss: 0.707313, acc: 0.503906]\n",
      "4102: [D loss: 0.724268, acc: 0.509766]  [A loss: 0.884213, acc: 0.160156]\n",
      "4103: [D loss: 0.713537, acc: 0.496094]  [A loss: 0.736278, acc: 0.425781]\n",
      "4104: [D loss: 0.719131, acc: 0.525391]  [A loss: 0.990662, acc: 0.074219]\n",
      "4105: [D loss: 0.694770, acc: 0.527344]  [A loss: 0.745893, acc: 0.433594]\n",
      "4106: [D loss: 0.717128, acc: 0.492188]  [A loss: 0.865808, acc: 0.226562]\n",
      "4107: [D loss: 0.718618, acc: 0.494141]  [A loss: 0.813568, acc: 0.304688]\n",
      "4108: [D loss: 0.708001, acc: 0.505859]  [A loss: 0.734251, acc: 0.414062]\n",
      "4109: [D loss: 0.709596, acc: 0.511719]  [A loss: 0.842853, acc: 0.277344]\n",
      "4110: [D loss: 0.710235, acc: 0.531250]  [A loss: 0.819430, acc: 0.257812]\n",
      "4111: [D loss: 0.697675, acc: 0.513672]  [A loss: 0.861017, acc: 0.179688]\n",
      "4112: [D loss: 0.694086, acc: 0.519531]  [A loss: 0.783029, acc: 0.324219]\n",
      "4113: [D loss: 0.714937, acc: 0.517578]  [A loss: 0.934210, acc: 0.136719]\n",
      "4114: [D loss: 0.694077, acc: 0.513672]  [A loss: 0.734186, acc: 0.410156]\n",
      "4115: [D loss: 0.716646, acc: 0.525391]  [A loss: 0.931499, acc: 0.101562]\n",
      "4116: [D loss: 0.691736, acc: 0.531250]  [A loss: 0.713372, acc: 0.484375]\n",
      "4117: [D loss: 0.718239, acc: 0.519531]  [A loss: 0.926498, acc: 0.191406]\n",
      "4118: [D loss: 0.716770, acc: 0.480469]  [A loss: 0.726301, acc: 0.453125]\n",
      "4119: [D loss: 0.708097, acc: 0.542969]  [A loss: 0.855286, acc: 0.210938]\n",
      "4120: [D loss: 0.708371, acc: 0.525391]  [A loss: 0.807241, acc: 0.257812]\n",
      "4121: [D loss: 0.700464, acc: 0.541016]  [A loss: 0.812436, acc: 0.308594]\n",
      "4122: [D loss: 0.719796, acc: 0.494141]  [A loss: 0.785424, acc: 0.339844]\n",
      "4123: [D loss: 0.692775, acc: 0.552734]  [A loss: 0.857378, acc: 0.214844]\n",
      "4124: [D loss: 0.702189, acc: 0.515625]  [A loss: 0.819417, acc: 0.257812]\n",
      "4125: [D loss: 0.706784, acc: 0.509766]  [A loss: 0.807539, acc: 0.289062]\n",
      "4126: [D loss: 0.711792, acc: 0.500000]  [A loss: 0.862037, acc: 0.175781]\n",
      "4127: [D loss: 0.702479, acc: 0.509766]  [A loss: 0.820708, acc: 0.261719]\n",
      "4128: [D loss: 0.700374, acc: 0.562500]  [A loss: 0.943382, acc: 0.121094]\n",
      "4129: [D loss: 0.706396, acc: 0.503906]  [A loss: 0.739498, acc: 0.421875]\n",
      "4130: [D loss: 0.711939, acc: 0.519531]  [A loss: 1.002083, acc: 0.046875]\n",
      "4131: [D loss: 0.697358, acc: 0.527344]  [A loss: 0.676567, acc: 0.527344]\n",
      "4132: [D loss: 0.730311, acc: 0.476562]  [A loss: 0.983151, acc: 0.113281]\n",
      "4133: [D loss: 0.696364, acc: 0.541016]  [A loss: 0.734163, acc: 0.414062]\n",
      "4134: [D loss: 0.717715, acc: 0.501953]  [A loss: 0.850655, acc: 0.226562]\n",
      "4135: [D loss: 0.705208, acc: 0.511719]  [A loss: 0.795460, acc: 0.296875]\n",
      "4136: [D loss: 0.701851, acc: 0.531250]  [A loss: 0.881074, acc: 0.179688]\n",
      "4137: [D loss: 0.691718, acc: 0.537109]  [A loss: 0.769667, acc: 0.355469]\n",
      "4138: [D loss: 0.708143, acc: 0.511719]  [A loss: 0.950011, acc: 0.148438]\n",
      "4139: [D loss: 0.705389, acc: 0.523438]  [A loss: 0.685212, acc: 0.519531]\n",
      "4140: [D loss: 0.734105, acc: 0.503906]  [A loss: 0.985589, acc: 0.089844]\n",
      "4141: [D loss: 0.709147, acc: 0.488281]  [A loss: 0.760870, acc: 0.386719]\n",
      "4142: [D loss: 0.703394, acc: 0.539062]  [A loss: 0.856815, acc: 0.226562]\n",
      "4143: [D loss: 0.702020, acc: 0.541016]  [A loss: 0.786033, acc: 0.320312]\n",
      "4144: [D loss: 0.713968, acc: 0.535156]  [A loss: 0.941754, acc: 0.148438]\n",
      "4145: [D loss: 0.712073, acc: 0.501953]  [A loss: 0.714445, acc: 0.460938]\n",
      "4146: [D loss: 0.715213, acc: 0.521484]  [A loss: 0.949523, acc: 0.113281]\n",
      "4147: [D loss: 0.689348, acc: 0.562500]  [A loss: 0.719522, acc: 0.457031]\n",
      "4148: [D loss: 0.720289, acc: 0.525391]  [A loss: 0.938660, acc: 0.097656]\n",
      "4149: [D loss: 0.697405, acc: 0.515625]  [A loss: 0.659137, acc: 0.589844]\n",
      "4150: [D loss: 0.718415, acc: 0.531250]  [A loss: 0.939213, acc: 0.101562]\n",
      "4151: [D loss: 0.702105, acc: 0.525391]  [A loss: 0.736267, acc: 0.417969]\n",
      "4152: [D loss: 0.709448, acc: 0.511719]  [A loss: 0.851769, acc: 0.218750]\n",
      "4153: [D loss: 0.704541, acc: 0.496094]  [A loss: 0.796123, acc: 0.316406]\n",
      "4154: [D loss: 0.709520, acc: 0.498047]  [A loss: 0.811897, acc: 0.230469]\n",
      "4155: [D loss: 0.705459, acc: 0.494141]  [A loss: 0.824264, acc: 0.257812]\n",
      "4156: [D loss: 0.694941, acc: 0.560547]  [A loss: 0.770206, acc: 0.351562]\n",
      "4157: [D loss: 0.725905, acc: 0.484375]  [A loss: 0.923301, acc: 0.125000]\n",
      "4158: [D loss: 0.701637, acc: 0.531250]  [A loss: 0.700224, acc: 0.531250]\n",
      "4159: [D loss: 0.721717, acc: 0.501953]  [A loss: 0.961520, acc: 0.101562]\n",
      "4160: [D loss: 0.709917, acc: 0.509766]  [A loss: 0.714421, acc: 0.476562]\n",
      "4161: [D loss: 0.724881, acc: 0.513672]  [A loss: 0.981588, acc: 0.054688]\n",
      "4162: [D loss: 0.700644, acc: 0.507812]  [A loss: 0.693883, acc: 0.500000]\n",
      "4163: [D loss: 0.734164, acc: 0.476562]  [A loss: 1.013571, acc: 0.078125]\n",
      "4164: [D loss: 0.702746, acc: 0.531250]  [A loss: 0.709917, acc: 0.503906]\n",
      "4165: [D loss: 0.724203, acc: 0.492188]  [A loss: 0.842687, acc: 0.203125]\n",
      "4166: [D loss: 0.701815, acc: 0.511719]  [A loss: 0.807898, acc: 0.285156]\n",
      "4167: [D loss: 0.690920, acc: 0.562500]  [A loss: 0.776712, acc: 0.351562]\n",
      "4168: [D loss: 0.705146, acc: 0.542969]  [A loss: 0.800583, acc: 0.273438]\n",
      "4169: [D loss: 0.707259, acc: 0.503906]  [A loss: 0.831108, acc: 0.250000]\n",
      "4170: [D loss: 0.688220, acc: 0.550781]  [A loss: 0.835270, acc: 0.214844]\n",
      "4171: [D loss: 0.705877, acc: 0.494141]  [A loss: 0.884084, acc: 0.167969]\n",
      "4172: [D loss: 0.689768, acc: 0.548828]  [A loss: 0.731305, acc: 0.429688]\n",
      "4173: [D loss: 0.721806, acc: 0.478516]  [A loss: 0.925411, acc: 0.132812]\n",
      "4174: [D loss: 0.709838, acc: 0.478516]  [A loss: 0.706009, acc: 0.480469]\n",
      "4175: [D loss: 0.717369, acc: 0.503906]  [A loss: 1.024024, acc: 0.062500]\n",
      "4176: [D loss: 0.703584, acc: 0.523438]  [A loss: 0.605093, acc: 0.742188]\n",
      "4177: [D loss: 0.773081, acc: 0.507812]  [A loss: 1.067774, acc: 0.050781]\n",
      "4178: [D loss: 0.696780, acc: 0.531250]  [A loss: 0.699230, acc: 0.531250]\n",
      "4179: [D loss: 0.722968, acc: 0.521484]  [A loss: 0.808158, acc: 0.292969]\n",
      "4180: [D loss: 0.702177, acc: 0.521484]  [A loss: 0.724553, acc: 0.460938]\n",
      "4181: [D loss: 0.709684, acc: 0.511719]  [A loss: 0.810571, acc: 0.277344]\n",
      "4182: [D loss: 0.696176, acc: 0.550781]  [A loss: 0.800618, acc: 0.289062]\n",
      "4183: [D loss: 0.703504, acc: 0.503906]  [A loss: 0.803382, acc: 0.324219]\n",
      "4184: [D loss: 0.706471, acc: 0.515625]  [A loss: 0.842392, acc: 0.238281]\n",
      "4185: [D loss: 0.708503, acc: 0.496094]  [A loss: 0.793499, acc: 0.289062]\n",
      "4186: [D loss: 0.717769, acc: 0.511719]  [A loss: 0.834709, acc: 0.242188]\n",
      "4187: [D loss: 0.704028, acc: 0.500000]  [A loss: 0.767486, acc: 0.363281]\n",
      "4188: [D loss: 0.720810, acc: 0.509766]  [A loss: 0.915563, acc: 0.136719]\n",
      "4189: [D loss: 0.700264, acc: 0.503906]  [A loss: 0.788438, acc: 0.308594]\n",
      "4190: [D loss: 0.718043, acc: 0.498047]  [A loss: 0.825965, acc: 0.242188]\n",
      "4191: [D loss: 0.703565, acc: 0.529297]  [A loss: 0.910589, acc: 0.132812]\n",
      "4192: [D loss: 0.700231, acc: 0.529297]  [A loss: 0.763979, acc: 0.351562]\n",
      "4193: [D loss: 0.718959, acc: 0.496094]  [A loss: 0.969144, acc: 0.089844]\n",
      "4194: [D loss: 0.699335, acc: 0.521484]  [A loss: 0.724773, acc: 0.449219]\n",
      "4195: [D loss: 0.715119, acc: 0.501953]  [A loss: 0.930652, acc: 0.152344]\n",
      "4196: [D loss: 0.697446, acc: 0.505859]  [A loss: 0.744055, acc: 0.410156]\n",
      "4197: [D loss: 0.706992, acc: 0.531250]  [A loss: 0.937420, acc: 0.160156]\n",
      "4198: [D loss: 0.697260, acc: 0.511719]  [A loss: 0.757910, acc: 0.382812]\n",
      "4199: [D loss: 0.703782, acc: 0.535156]  [A loss: 0.959713, acc: 0.136719]\n",
      "4200: [D loss: 0.701891, acc: 0.529297]  [A loss: 0.760459, acc: 0.386719]\n",
      "4201: [D loss: 0.721212, acc: 0.515625]  [A loss: 0.917466, acc: 0.125000]\n",
      "4202: [D loss: 0.698639, acc: 0.517578]  [A loss: 0.724653, acc: 0.441406]\n",
      "4203: [D loss: 0.711611, acc: 0.507812]  [A loss: 1.011561, acc: 0.078125]\n",
      "4204: [D loss: 0.702108, acc: 0.552734]  [A loss: 0.681478, acc: 0.554688]\n",
      "4205: [D loss: 0.735616, acc: 0.505859]  [A loss: 0.926640, acc: 0.144531]\n",
      "4206: [D loss: 0.700645, acc: 0.507812]  [A loss: 0.714014, acc: 0.472656]\n",
      "4207: [D loss: 0.725232, acc: 0.527344]  [A loss: 0.889849, acc: 0.187500]\n",
      "4208: [D loss: 0.707220, acc: 0.533203]  [A loss: 0.698582, acc: 0.535156]\n",
      "4209: [D loss: 0.706860, acc: 0.531250]  [A loss: 0.855892, acc: 0.183594]\n",
      "4210: [D loss: 0.701794, acc: 0.503906]  [A loss: 0.786884, acc: 0.304688]\n",
      "4211: [D loss: 0.711454, acc: 0.496094]  [A loss: 0.907355, acc: 0.160156]\n",
      "4212: [D loss: 0.698841, acc: 0.503906]  [A loss: 0.814365, acc: 0.261719]\n",
      "4213: [D loss: 0.701621, acc: 0.515625]  [A loss: 0.833387, acc: 0.285156]\n",
      "4214: [D loss: 0.700240, acc: 0.519531]  [A loss: 0.814389, acc: 0.300781]\n",
      "4215: [D loss: 0.711293, acc: 0.511719]  [A loss: 0.871702, acc: 0.203125]\n",
      "4216: [D loss: 0.702741, acc: 0.515625]  [A loss: 0.857318, acc: 0.179688]\n",
      "4217: [D loss: 0.702039, acc: 0.517578]  [A loss: 0.879362, acc: 0.191406]\n",
      "4218: [D loss: 0.698048, acc: 0.523438]  [A loss: 0.801268, acc: 0.289062]\n",
      "4219: [D loss: 0.709820, acc: 0.527344]  [A loss: 0.960759, acc: 0.058594]\n",
      "4220: [D loss: 0.708464, acc: 0.492188]  [A loss: 0.714871, acc: 0.437500]\n",
      "4221: [D loss: 0.723648, acc: 0.496094]  [A loss: 1.005955, acc: 0.046875]\n",
      "4222: [D loss: 0.696685, acc: 0.525391]  [A loss: 0.665384, acc: 0.585938]\n",
      "4223: [D loss: 0.742598, acc: 0.496094]  [A loss: 0.979215, acc: 0.078125]\n",
      "4224: [D loss: 0.702844, acc: 0.542969]  [A loss: 0.729461, acc: 0.449219]\n",
      "4225: [D loss: 0.726921, acc: 0.488281]  [A loss: 0.925842, acc: 0.125000]\n",
      "4226: [D loss: 0.725345, acc: 0.476562]  [A loss: 0.819290, acc: 0.265625]\n",
      "4227: [D loss: 0.714850, acc: 0.492188]  [A loss: 0.868852, acc: 0.164062]\n",
      "4228: [D loss: 0.701851, acc: 0.523438]  [A loss: 0.741140, acc: 0.441406]\n",
      "4229: [D loss: 0.724337, acc: 0.500000]  [A loss: 0.929983, acc: 0.136719]\n",
      "4230: [D loss: 0.687543, acc: 0.541016]  [A loss: 0.749472, acc: 0.402344]\n",
      "4231: [D loss: 0.721760, acc: 0.498047]  [A loss: 0.909923, acc: 0.125000]\n",
      "4232: [D loss: 0.699985, acc: 0.517578]  [A loss: 0.673972, acc: 0.593750]\n",
      "4233: [D loss: 0.740277, acc: 0.486328]  [A loss: 1.035318, acc: 0.062500]\n",
      "4234: [D loss: 0.706811, acc: 0.527344]  [A loss: 0.749978, acc: 0.386719]\n",
      "4235: [D loss: 0.718296, acc: 0.492188]  [A loss: 0.857099, acc: 0.183594]\n",
      "4236: [D loss: 0.711936, acc: 0.513672]  [A loss: 0.781605, acc: 0.359375]\n",
      "4237: [D loss: 0.697114, acc: 0.529297]  [A loss: 0.826735, acc: 0.285156]\n",
      "4238: [D loss: 0.708493, acc: 0.525391]  [A loss: 0.856142, acc: 0.214844]\n",
      "4239: [D loss: 0.719088, acc: 0.484375]  [A loss: 0.862063, acc: 0.230469]\n",
      "4240: [D loss: 0.691787, acc: 0.568359]  [A loss: 0.811387, acc: 0.292969]\n",
      "4241: [D loss: 0.698656, acc: 0.501953]  [A loss: 0.835244, acc: 0.246094]\n",
      "4242: [D loss: 0.723711, acc: 0.500000]  [A loss: 0.900869, acc: 0.117188]\n",
      "4243: [D loss: 0.699398, acc: 0.546875]  [A loss: 0.780298, acc: 0.335938]\n",
      "4244: [D loss: 0.713021, acc: 0.515625]  [A loss: 0.931718, acc: 0.105469]\n",
      "4245: [D loss: 0.707910, acc: 0.486328]  [A loss: 0.713725, acc: 0.476562]\n",
      "4246: [D loss: 0.719514, acc: 0.509766]  [A loss: 0.947839, acc: 0.113281]\n",
      "4247: [D loss: 0.698600, acc: 0.511719]  [A loss: 0.740834, acc: 0.406250]\n",
      "4248: [D loss: 0.720919, acc: 0.515625]  [A loss: 0.989778, acc: 0.093750]\n",
      "4249: [D loss: 0.708099, acc: 0.501953]  [A loss: 0.689971, acc: 0.542969]\n",
      "4250: [D loss: 0.725855, acc: 0.517578]  [A loss: 0.946531, acc: 0.085938]\n",
      "4251: [D loss: 0.696132, acc: 0.523438]  [A loss: 0.697942, acc: 0.546875]\n",
      "4252: [D loss: 0.705526, acc: 0.529297]  [A loss: 0.924635, acc: 0.093750]\n",
      "4253: [D loss: 0.699390, acc: 0.515625]  [A loss: 0.693851, acc: 0.503906]\n",
      "4254: [D loss: 0.713230, acc: 0.531250]  [A loss: 0.961401, acc: 0.109375]\n",
      "4255: [D loss: 0.711982, acc: 0.490234]  [A loss: 0.696326, acc: 0.511719]\n",
      "4256: [D loss: 0.730425, acc: 0.494141]  [A loss: 0.955724, acc: 0.078125]\n",
      "4257: [D loss: 0.686315, acc: 0.552734]  [A loss: 0.747857, acc: 0.378906]\n",
      "4258: [D loss: 0.712111, acc: 0.513672]  [A loss: 0.890205, acc: 0.164062]\n",
      "4259: [D loss: 0.699088, acc: 0.533203]  [A loss: 0.736452, acc: 0.464844]\n",
      "4260: [D loss: 0.703441, acc: 0.548828]  [A loss: 0.858203, acc: 0.210938]\n",
      "4261: [D loss: 0.707014, acc: 0.501953]  [A loss: 0.772459, acc: 0.324219]\n",
      "4262: [D loss: 0.710059, acc: 0.503906]  [A loss: 0.876737, acc: 0.199219]\n",
      "4263: [D loss: 0.714241, acc: 0.482422]  [A loss: 0.811221, acc: 0.289062]\n",
      "4264: [D loss: 0.701623, acc: 0.537109]  [A loss: 0.867015, acc: 0.187500]\n",
      "4265: [D loss: 0.702803, acc: 0.525391]  [A loss: 0.755541, acc: 0.355469]\n",
      "4266: [D loss: 0.705423, acc: 0.511719]  [A loss: 0.888086, acc: 0.164062]\n",
      "4267: [D loss: 0.687486, acc: 0.552734]  [A loss: 0.745658, acc: 0.398438]\n",
      "4268: [D loss: 0.716836, acc: 0.523438]  [A loss: 0.961250, acc: 0.085938]\n",
      "4269: [D loss: 0.702970, acc: 0.527344]  [A loss: 0.712918, acc: 0.488281]\n",
      "4270: [D loss: 0.723558, acc: 0.511719]  [A loss: 1.006130, acc: 0.070312]\n",
      "4271: [D loss: 0.696959, acc: 0.500000]  [A loss: 0.674075, acc: 0.593750]\n",
      "4272: [D loss: 0.713907, acc: 0.539062]  [A loss: 0.900850, acc: 0.144531]\n",
      "4273: [D loss: 0.702958, acc: 0.537109]  [A loss: 0.766373, acc: 0.371094]\n",
      "4274: [D loss: 0.730421, acc: 0.492188]  [A loss: 0.916391, acc: 0.164062]\n",
      "4275: [D loss: 0.699220, acc: 0.527344]  [A loss: 0.752466, acc: 0.375000]\n",
      "4276: [D loss: 0.704580, acc: 0.523438]  [A loss: 0.871387, acc: 0.207031]\n",
      "4277: [D loss: 0.708676, acc: 0.531250]  [A loss: 0.829464, acc: 0.246094]\n",
      "4278: [D loss: 0.702301, acc: 0.527344]  [A loss: 0.850084, acc: 0.191406]\n",
      "4279: [D loss: 0.699879, acc: 0.517578]  [A loss: 0.807537, acc: 0.312500]\n",
      "4280: [D loss: 0.708987, acc: 0.490234]  [A loss: 0.907524, acc: 0.121094]\n",
      "4281: [D loss: 0.689794, acc: 0.523438]  [A loss: 0.714646, acc: 0.472656]\n",
      "4282: [D loss: 0.730705, acc: 0.488281]  [A loss: 0.978148, acc: 0.074219]\n",
      "4283: [D loss: 0.720701, acc: 0.498047]  [A loss: 0.677694, acc: 0.566406]\n",
      "4284: [D loss: 0.725918, acc: 0.507812]  [A loss: 1.090278, acc: 0.050781]\n",
      "4285: [D loss: 0.701667, acc: 0.521484]  [A loss: 0.707189, acc: 0.539062]\n",
      "4286: [D loss: 0.753541, acc: 0.462891]  [A loss: 0.924641, acc: 0.132812]\n",
      "4287: [D loss: 0.704187, acc: 0.523438]  [A loss: 0.668262, acc: 0.574219]\n",
      "4288: [D loss: 0.750817, acc: 0.468750]  [A loss: 0.939213, acc: 0.113281]\n",
      "4289: [D loss: 0.697911, acc: 0.548828]  [A loss: 0.690780, acc: 0.527344]\n",
      "4290: [D loss: 0.724082, acc: 0.490234]  [A loss: 0.835582, acc: 0.250000]\n",
      "4291: [D loss: 0.704702, acc: 0.519531]  [A loss: 0.772453, acc: 0.378906]\n",
      "4292: [D loss: 0.733773, acc: 0.482422]  [A loss: 0.890921, acc: 0.156250]\n",
      "4293: [D loss: 0.711754, acc: 0.513672]  [A loss: 0.823799, acc: 0.289062]\n",
      "4294: [D loss: 0.708288, acc: 0.482422]  [A loss: 0.808262, acc: 0.289062]\n",
      "4295: [D loss: 0.686885, acc: 0.558594]  [A loss: 0.821117, acc: 0.265625]\n",
      "4296: [D loss: 0.704371, acc: 0.517578]  [A loss: 0.801876, acc: 0.320312]\n",
      "4297: [D loss: 0.702073, acc: 0.529297]  [A loss: 0.802008, acc: 0.238281]\n",
      "4298: [D loss: 0.708006, acc: 0.513672]  [A loss: 0.760444, acc: 0.386719]\n",
      "4299: [D loss: 0.716656, acc: 0.468750]  [A loss: 0.799222, acc: 0.292969]\n",
      "4300: [D loss: 0.706383, acc: 0.492188]  [A loss: 0.783183, acc: 0.335938]\n",
      "4301: [D loss: 0.701097, acc: 0.509766]  [A loss: 0.864951, acc: 0.222656]\n",
      "4302: [D loss: 0.696966, acc: 0.521484]  [A loss: 0.781086, acc: 0.328125]\n",
      "4303: [D loss: 0.696169, acc: 0.550781]  [A loss: 0.935017, acc: 0.121094]\n",
      "4304: [D loss: 0.706472, acc: 0.500000]  [A loss: 0.730620, acc: 0.453125]\n",
      "4305: [D loss: 0.722182, acc: 0.513672]  [A loss: 1.005004, acc: 0.089844]\n",
      "4306: [D loss: 0.702253, acc: 0.519531]  [A loss: 0.711916, acc: 0.460938]\n",
      "4307: [D loss: 0.721492, acc: 0.527344]  [A loss: 0.926513, acc: 0.113281]\n",
      "4308: [D loss: 0.697722, acc: 0.515625]  [A loss: 0.728225, acc: 0.453125]\n",
      "4309: [D loss: 0.712101, acc: 0.501953]  [A loss: 0.893665, acc: 0.152344]\n",
      "4310: [D loss: 0.699563, acc: 0.525391]  [A loss: 0.855340, acc: 0.277344]\n",
      "4311: [D loss: 0.701810, acc: 0.515625]  [A loss: 0.812346, acc: 0.269531]\n",
      "4312: [D loss: 0.719166, acc: 0.470703]  [A loss: 0.840232, acc: 0.242188]\n",
      "4313: [D loss: 0.696586, acc: 0.556641]  [A loss: 0.800659, acc: 0.320312]\n",
      "4314: [D loss: 0.713050, acc: 0.519531]  [A loss: 0.912266, acc: 0.156250]\n",
      "4315: [D loss: 0.697141, acc: 0.537109]  [A loss: 0.760392, acc: 0.402344]\n",
      "4316: [D loss: 0.725205, acc: 0.478516]  [A loss: 0.959793, acc: 0.093750]\n",
      "4317: [D loss: 0.691583, acc: 0.513672]  [A loss: 0.691222, acc: 0.523438]\n",
      "4318: [D loss: 0.728921, acc: 0.503906]  [A loss: 0.980054, acc: 0.082031]\n",
      "4319: [D loss: 0.683633, acc: 0.585938]  [A loss: 0.681198, acc: 0.542969]\n",
      "4320: [D loss: 0.736732, acc: 0.498047]  [A loss: 1.042971, acc: 0.074219]\n",
      "4321: [D loss: 0.702840, acc: 0.542969]  [A loss: 0.741079, acc: 0.414062]\n",
      "4322: [D loss: 0.713231, acc: 0.511719]  [A loss: 0.819085, acc: 0.250000]\n",
      "4323: [D loss: 0.718442, acc: 0.484375]  [A loss: 0.844053, acc: 0.214844]\n",
      "4324: [D loss: 0.698858, acc: 0.531250]  [A loss: 0.799949, acc: 0.308594]\n",
      "4325: [D loss: 0.707503, acc: 0.509766]  [A loss: 0.931878, acc: 0.128906]\n",
      "4326: [D loss: 0.696721, acc: 0.539062]  [A loss: 0.744976, acc: 0.437500]\n",
      "4327: [D loss: 0.705521, acc: 0.505859]  [A loss: 0.998971, acc: 0.113281]\n",
      "4328: [D loss: 0.709900, acc: 0.484375]  [A loss: 0.726077, acc: 0.425781]\n",
      "4329: [D loss: 0.737245, acc: 0.494141]  [A loss: 0.975514, acc: 0.085938]\n",
      "4330: [D loss: 0.697560, acc: 0.509766]  [A loss: 0.708549, acc: 0.500000]\n",
      "4331: [D loss: 0.716864, acc: 0.500000]  [A loss: 0.889574, acc: 0.160156]\n",
      "4332: [D loss: 0.704924, acc: 0.511719]  [A loss: 0.750085, acc: 0.367188]\n",
      "4333: [D loss: 0.713034, acc: 0.505859]  [A loss: 0.857330, acc: 0.195312]\n",
      "4334: [D loss: 0.701814, acc: 0.533203]  [A loss: 0.798116, acc: 0.300781]\n",
      "4335: [D loss: 0.706905, acc: 0.490234]  [A loss: 0.843112, acc: 0.238281]\n",
      "4336: [D loss: 0.705303, acc: 0.515625]  [A loss: 0.791771, acc: 0.324219]\n",
      "4337: [D loss: 0.700373, acc: 0.529297]  [A loss: 0.903249, acc: 0.171875]\n",
      "4338: [D loss: 0.689774, acc: 0.550781]  [A loss: 0.768135, acc: 0.359375]\n",
      "4339: [D loss: 0.715877, acc: 0.525391]  [A loss: 0.853022, acc: 0.203125]\n",
      "4340: [D loss: 0.708901, acc: 0.498047]  [A loss: 0.777268, acc: 0.343750]\n",
      "4341: [D loss: 0.712802, acc: 0.505859]  [A loss: 0.879488, acc: 0.187500]\n",
      "4342: [D loss: 0.710975, acc: 0.501953]  [A loss: 0.777236, acc: 0.328125]\n",
      "4343: [D loss: 0.713901, acc: 0.490234]  [A loss: 0.957951, acc: 0.101562]\n",
      "4344: [D loss: 0.697454, acc: 0.531250]  [A loss: 0.746674, acc: 0.390625]\n",
      "4345: [D loss: 0.715444, acc: 0.505859]  [A loss: 0.951765, acc: 0.136719]\n",
      "4346: [D loss: 0.714569, acc: 0.521484]  [A loss: 0.698760, acc: 0.511719]\n",
      "4347: [D loss: 0.728645, acc: 0.500000]  [A loss: 0.896852, acc: 0.140625]\n",
      "4348: [D loss: 0.701755, acc: 0.517578]  [A loss: 0.922347, acc: 0.140625]\n",
      "4349: [D loss: 0.692681, acc: 0.541016]  [A loss: 0.730786, acc: 0.449219]\n",
      "4350: [D loss: 0.706242, acc: 0.537109]  [A loss: 0.972330, acc: 0.093750]\n",
      "4351: [D loss: 0.697968, acc: 0.521484]  [A loss: 0.679602, acc: 0.527344]\n",
      "4352: [D loss: 0.727803, acc: 0.507812]  [A loss: 1.026752, acc: 0.062500]\n",
      "4353: [D loss: 0.716832, acc: 0.494141]  [A loss: 0.720064, acc: 0.453125]\n",
      "4354: [D loss: 0.720165, acc: 0.503906]  [A loss: 0.900409, acc: 0.171875]\n",
      "4355: [D loss: 0.699478, acc: 0.527344]  [A loss: 0.756494, acc: 0.371094]\n",
      "4356: [D loss: 0.710566, acc: 0.531250]  [A loss: 0.925427, acc: 0.085938]\n",
      "4357: [D loss: 0.703265, acc: 0.515625]  [A loss: 0.692039, acc: 0.542969]\n",
      "4358: [D loss: 0.717065, acc: 0.484375]  [A loss: 0.981001, acc: 0.074219]\n",
      "4359: [D loss: 0.701849, acc: 0.488281]  [A loss: 0.716528, acc: 0.464844]\n",
      "4360: [D loss: 0.717529, acc: 0.515625]  [A loss: 0.906985, acc: 0.132812]\n",
      "4361: [D loss: 0.713474, acc: 0.480469]  [A loss: 0.743439, acc: 0.417969]\n",
      "4362: [D loss: 0.717564, acc: 0.517578]  [A loss: 0.863835, acc: 0.179688]\n",
      "4363: [D loss: 0.696390, acc: 0.537109]  [A loss: 0.777474, acc: 0.328125]\n",
      "4364: [D loss: 0.721903, acc: 0.480469]  [A loss: 0.886884, acc: 0.136719]\n",
      "4365: [D loss: 0.697112, acc: 0.505859]  [A loss: 0.791934, acc: 0.292969]\n",
      "4366: [D loss: 0.700547, acc: 0.529297]  [A loss: 0.840155, acc: 0.253906]\n",
      "4367: [D loss: 0.694874, acc: 0.533203]  [A loss: 0.793469, acc: 0.308594]\n",
      "4368: [D loss: 0.705966, acc: 0.548828]  [A loss: 0.902967, acc: 0.164062]\n",
      "4369: [D loss: 0.689751, acc: 0.548828]  [A loss: 0.739214, acc: 0.429688]\n",
      "4370: [D loss: 0.702048, acc: 0.529297]  [A loss: 0.908968, acc: 0.113281]\n",
      "4371: [D loss: 0.699468, acc: 0.490234]  [A loss: 0.759911, acc: 0.367188]\n",
      "4372: [D loss: 0.705532, acc: 0.537109]  [A loss: 0.884606, acc: 0.164062]\n",
      "4373: [D loss: 0.705169, acc: 0.517578]  [A loss: 0.792176, acc: 0.300781]\n",
      "4374: [D loss: 0.704589, acc: 0.542969]  [A loss: 0.949084, acc: 0.089844]\n",
      "4375: [D loss: 0.700201, acc: 0.527344]  [A loss: 0.694370, acc: 0.523438]\n",
      "4376: [D loss: 0.729564, acc: 0.509766]  [A loss: 0.978249, acc: 0.117188]\n",
      "4377: [D loss: 0.701701, acc: 0.503906]  [A loss: 0.632870, acc: 0.683594]\n",
      "4378: [D loss: 0.767622, acc: 0.484375]  [A loss: 1.055033, acc: 0.039062]\n",
      "4379: [D loss: 0.707381, acc: 0.503906]  [A loss: 0.668525, acc: 0.597656]\n",
      "4380: [D loss: 0.719464, acc: 0.513672]  [A loss: 0.884438, acc: 0.144531]\n",
      "4381: [D loss: 0.694781, acc: 0.519531]  [A loss: 0.769741, acc: 0.378906]\n",
      "4382: [D loss: 0.704570, acc: 0.525391]  [A loss: 0.808591, acc: 0.300781]\n",
      "4383: [D loss: 0.705537, acc: 0.509766]  [A loss: 0.865941, acc: 0.199219]\n",
      "4384: [D loss: 0.692850, acc: 0.527344]  [A loss: 0.801724, acc: 0.289062]\n",
      "4385: [D loss: 0.713177, acc: 0.507812]  [A loss: 0.863011, acc: 0.164062]\n",
      "4386: [D loss: 0.708829, acc: 0.507812]  [A loss: 0.855620, acc: 0.203125]\n",
      "4387: [D loss: 0.712976, acc: 0.498047]  [A loss: 0.763470, acc: 0.382812]\n",
      "4388: [D loss: 0.717135, acc: 0.507812]  [A loss: 0.964703, acc: 0.093750]\n",
      "4389: [D loss: 0.693470, acc: 0.554688]  [A loss: 0.755055, acc: 0.417969]\n",
      "4390: [D loss: 0.706875, acc: 0.525391]  [A loss: 0.919251, acc: 0.117188]\n",
      "4391: [D loss: 0.712446, acc: 0.488281]  [A loss: 0.778129, acc: 0.312500]\n",
      "4392: [D loss: 0.700507, acc: 0.525391]  [A loss: 0.842895, acc: 0.199219]\n",
      "4393: [D loss: 0.724583, acc: 0.480469]  [A loss: 0.866259, acc: 0.156250]\n",
      "4394: [D loss: 0.695049, acc: 0.531250]  [A loss: 0.729609, acc: 0.445312]\n",
      "4395: [D loss: 0.739583, acc: 0.511719]  [A loss: 1.025360, acc: 0.082031]\n",
      "4396: [D loss: 0.701756, acc: 0.511719]  [A loss: 0.679561, acc: 0.535156]\n",
      "4397: [D loss: 0.742081, acc: 0.488281]  [A loss: 0.881826, acc: 0.179688]\n",
      "4398: [D loss: 0.701030, acc: 0.519531]  [A loss: 0.753837, acc: 0.402344]\n",
      "4399: [D loss: 0.711286, acc: 0.523438]  [A loss: 0.892063, acc: 0.128906]\n",
      "4400: [D loss: 0.708297, acc: 0.496094]  [A loss: 0.743537, acc: 0.414062]\n",
      "4401: [D loss: 0.716941, acc: 0.488281]  [A loss: 0.940284, acc: 0.105469]\n",
      "4402: [D loss: 0.699180, acc: 0.529297]  [A loss: 0.694561, acc: 0.500000]\n",
      "4403: [D loss: 0.710868, acc: 0.511719]  [A loss: 0.875281, acc: 0.183594]\n",
      "4404: [D loss: 0.707846, acc: 0.470703]  [A loss: 0.760173, acc: 0.316406]\n",
      "4405: [D loss: 0.716942, acc: 0.509766]  [A loss: 0.870268, acc: 0.195312]\n",
      "4406: [D loss: 0.693861, acc: 0.539062]  [A loss: 0.761974, acc: 0.359375]\n",
      "4407: [D loss: 0.718490, acc: 0.482422]  [A loss: 0.890021, acc: 0.179688]\n",
      "4408: [D loss: 0.702535, acc: 0.523438]  [A loss: 0.751176, acc: 0.378906]\n",
      "4409: [D loss: 0.700339, acc: 0.560547]  [A loss: 0.882878, acc: 0.160156]\n",
      "4410: [D loss: 0.710180, acc: 0.482422]  [A loss: 0.836382, acc: 0.238281]\n",
      "4411: [D loss: 0.694396, acc: 0.552734]  [A loss: 0.805143, acc: 0.300781]\n",
      "4412: [D loss: 0.706298, acc: 0.541016]  [A loss: 0.913869, acc: 0.148438]\n",
      "4413: [D loss: 0.717332, acc: 0.472656]  [A loss: 0.793319, acc: 0.296875]\n",
      "4414: [D loss: 0.704477, acc: 0.529297]  [A loss: 0.794639, acc: 0.320312]\n",
      "4415: [D loss: 0.698348, acc: 0.511719]  [A loss: 0.867318, acc: 0.132812]\n",
      "4416: [D loss: 0.719196, acc: 0.458984]  [A loss: 0.858375, acc: 0.199219]\n",
      "4417: [D loss: 0.698456, acc: 0.521484]  [A loss: 0.818777, acc: 0.234375]\n",
      "4418: [D loss: 0.720411, acc: 0.488281]  [A loss: 0.979308, acc: 0.066406]\n",
      "4419: [D loss: 0.697670, acc: 0.513672]  [A loss: 0.647704, acc: 0.625000]\n",
      "4420: [D loss: 0.739647, acc: 0.503906]  [A loss: 1.080510, acc: 0.039062]\n",
      "4421: [D loss: 0.710499, acc: 0.515625]  [A loss: 0.709170, acc: 0.468750]\n",
      "4422: [D loss: 0.755046, acc: 0.494141]  [A loss: 0.928260, acc: 0.089844]\n",
      "4423: [D loss: 0.705120, acc: 0.505859]  [A loss: 0.739141, acc: 0.425781]\n",
      "4424: [D loss: 0.706901, acc: 0.511719]  [A loss: 0.933672, acc: 0.132812]\n",
      "4425: [D loss: 0.706756, acc: 0.503906]  [A loss: 0.740512, acc: 0.394531]\n",
      "4426: [D loss: 0.715349, acc: 0.527344]  [A loss: 0.830678, acc: 0.203125]\n",
      "4427: [D loss: 0.707324, acc: 0.503906]  [A loss: 0.802945, acc: 0.281250]\n",
      "4428: [D loss: 0.701132, acc: 0.539062]  [A loss: 0.844112, acc: 0.191406]\n",
      "4429: [D loss: 0.698211, acc: 0.515625]  [A loss: 0.739948, acc: 0.410156]\n",
      "4430: [D loss: 0.708439, acc: 0.525391]  [A loss: 0.839984, acc: 0.226562]\n",
      "4431: [D loss: 0.696902, acc: 0.539062]  [A loss: 0.791691, acc: 0.316406]\n",
      "4432: [D loss: 0.714752, acc: 0.513672]  [A loss: 0.906828, acc: 0.132812]\n",
      "4433: [D loss: 0.702940, acc: 0.503906]  [A loss: 0.803843, acc: 0.269531]\n",
      "4434: [D loss: 0.712723, acc: 0.513672]  [A loss: 0.925778, acc: 0.144531]\n",
      "4435: [D loss: 0.705946, acc: 0.496094]  [A loss: 0.757188, acc: 0.382812]\n",
      "4436: [D loss: 0.710661, acc: 0.517578]  [A loss: 0.915850, acc: 0.109375]\n",
      "4437: [D loss: 0.706320, acc: 0.507812]  [A loss: 0.707572, acc: 0.445312]\n",
      "4438: [D loss: 0.727374, acc: 0.525391]  [A loss: 1.000971, acc: 0.050781]\n",
      "4439: [D loss: 0.700814, acc: 0.501953]  [A loss: 0.715530, acc: 0.503906]\n",
      "4440: [D loss: 0.735013, acc: 0.505859]  [A loss: 0.956059, acc: 0.058594]\n",
      "4441: [D loss: 0.705331, acc: 0.511719]  [A loss: 0.758367, acc: 0.386719]\n",
      "4442: [D loss: 0.725598, acc: 0.511719]  [A loss: 0.956484, acc: 0.074219]\n",
      "4443: [D loss: 0.689084, acc: 0.537109]  [A loss: 0.671055, acc: 0.562500]\n",
      "4444: [D loss: 0.731418, acc: 0.511719]  [A loss: 0.957818, acc: 0.121094]\n",
      "4445: [D loss: 0.715870, acc: 0.486328]  [A loss: 0.742271, acc: 0.386719]\n",
      "4446: [D loss: 0.709254, acc: 0.513672]  [A loss: 0.853837, acc: 0.210938]\n",
      "4447: [D loss: 0.698695, acc: 0.531250]  [A loss: 0.794568, acc: 0.296875]\n",
      "4448: [D loss: 0.706996, acc: 0.505859]  [A loss: 0.828724, acc: 0.234375]\n",
      "4449: [D loss: 0.701275, acc: 0.511719]  [A loss: 0.805518, acc: 0.285156]\n",
      "4450: [D loss: 0.714030, acc: 0.490234]  [A loss: 0.819235, acc: 0.230469]\n",
      "4451: [D loss: 0.684608, acc: 0.556641]  [A loss: 0.805902, acc: 0.292969]\n",
      "4452: [D loss: 0.699478, acc: 0.525391]  [A loss: 0.866058, acc: 0.179688]\n",
      "4453: [D loss: 0.701044, acc: 0.537109]  [A loss: 0.869395, acc: 0.164062]\n",
      "4454: [D loss: 0.711344, acc: 0.501953]  [A loss: 0.843922, acc: 0.207031]\n",
      "4455: [D loss: 0.709258, acc: 0.503906]  [A loss: 0.873791, acc: 0.167969]\n",
      "4456: [D loss: 0.694938, acc: 0.541016]  [A loss: 0.779891, acc: 0.375000]\n",
      "4457: [D loss: 0.709391, acc: 0.507812]  [A loss: 0.913882, acc: 0.117188]\n",
      "4458: [D loss: 0.695765, acc: 0.511719]  [A loss: 0.710252, acc: 0.468750]\n",
      "4459: [D loss: 0.733391, acc: 0.492188]  [A loss: 1.058667, acc: 0.050781]\n",
      "4460: [D loss: 0.709511, acc: 0.523438]  [A loss: 0.645184, acc: 0.648438]\n",
      "4461: [D loss: 0.736945, acc: 0.500000]  [A loss: 0.961215, acc: 0.113281]\n",
      "4462: [D loss: 0.696491, acc: 0.498047]  [A loss: 0.710928, acc: 0.492188]\n",
      "4463: [D loss: 0.713986, acc: 0.511719]  [A loss: 0.919665, acc: 0.128906]\n",
      "4464: [D loss: 0.696471, acc: 0.519531]  [A loss: 0.732673, acc: 0.382812]\n",
      "4465: [D loss: 0.716395, acc: 0.519531]  [A loss: 0.850825, acc: 0.199219]\n",
      "4466: [D loss: 0.699245, acc: 0.515625]  [A loss: 0.767080, acc: 0.324219]\n",
      "4467: [D loss: 0.696694, acc: 0.533203]  [A loss: 0.831678, acc: 0.253906]\n",
      "4468: [D loss: 0.710525, acc: 0.515625]  [A loss: 0.815845, acc: 0.242188]\n",
      "4469: [D loss: 0.697734, acc: 0.517578]  [A loss: 0.827646, acc: 0.226562]\n",
      "4470: [D loss: 0.700401, acc: 0.503906]  [A loss: 0.802805, acc: 0.273438]\n",
      "4471: [D loss: 0.710254, acc: 0.511719]  [A loss: 0.887993, acc: 0.140625]\n",
      "4472: [D loss: 0.696022, acc: 0.533203]  [A loss: 0.783989, acc: 0.292969]\n",
      "4473: [D loss: 0.704020, acc: 0.513672]  [A loss: 0.917424, acc: 0.160156]\n",
      "4474: [D loss: 0.693945, acc: 0.529297]  [A loss: 0.769335, acc: 0.371094]\n",
      "4475: [D loss: 0.704049, acc: 0.509766]  [A loss: 0.975241, acc: 0.113281]\n",
      "4476: [D loss: 0.702274, acc: 0.529297]  [A loss: 0.760130, acc: 0.378906]\n",
      "4477: [D loss: 0.717658, acc: 0.513672]  [A loss: 0.884286, acc: 0.136719]\n",
      "4478: [D loss: 0.701932, acc: 0.539062]  [A loss: 0.769176, acc: 0.328125]\n",
      "4479: [D loss: 0.716472, acc: 0.490234]  [A loss: 0.865446, acc: 0.210938]\n",
      "4480: [D loss: 0.699193, acc: 0.535156]  [A loss: 0.793374, acc: 0.308594]\n",
      "4481: [D loss: 0.721226, acc: 0.484375]  [A loss: 0.901706, acc: 0.132812]\n",
      "4482: [D loss: 0.691461, acc: 0.537109]  [A loss: 0.844445, acc: 0.226562]\n",
      "4483: [D loss: 0.723837, acc: 0.474609]  [A loss: 0.798201, acc: 0.332031]\n",
      "4484: [D loss: 0.701876, acc: 0.523438]  [A loss: 0.855564, acc: 0.179688]\n",
      "4485: [D loss: 0.699451, acc: 0.519531]  [A loss: 0.802432, acc: 0.292969]\n",
      "4486: [D loss: 0.689214, acc: 0.525391]  [A loss: 0.864611, acc: 0.187500]\n",
      "4487: [D loss: 0.694962, acc: 0.550781]  [A loss: 0.834401, acc: 0.261719]\n",
      "4488: [D loss: 0.704066, acc: 0.539062]  [A loss: 0.870603, acc: 0.175781]\n",
      "4489: [D loss: 0.695656, acc: 0.542969]  [A loss: 0.781099, acc: 0.343750]\n",
      "4490: [D loss: 0.711010, acc: 0.507812]  [A loss: 0.941857, acc: 0.105469]\n",
      "4491: [D loss: 0.694235, acc: 0.552734]  [A loss: 0.712548, acc: 0.464844]\n",
      "4492: [D loss: 0.727308, acc: 0.507812]  [A loss: 1.128006, acc: 0.027344]\n",
      "4493: [D loss: 0.732258, acc: 0.494141]  [A loss: 0.612324, acc: 0.683594]\n",
      "4494: [D loss: 0.790899, acc: 0.500000]  [A loss: 1.006316, acc: 0.082031]\n",
      "4495: [D loss: 0.712779, acc: 0.474609]  [A loss: 0.800396, acc: 0.265625]\n",
      "4496: [D loss: 0.715781, acc: 0.515625]  [A loss: 0.935068, acc: 0.109375]\n",
      "4497: [D loss: 0.701336, acc: 0.509766]  [A loss: 0.733741, acc: 0.414062]\n",
      "4498: [D loss: 0.706572, acc: 0.507812]  [A loss: 0.894079, acc: 0.140625]\n",
      "4499: [D loss: 0.710715, acc: 0.474609]  [A loss: 0.739754, acc: 0.414062]\n",
      "4500: [D loss: 0.716740, acc: 0.517578]  [A loss: 0.883452, acc: 0.179688]\n",
      "4501: [D loss: 0.703252, acc: 0.503906]  [A loss: 0.758113, acc: 0.382812]\n",
      "4502: [D loss: 0.712621, acc: 0.494141]  [A loss: 0.866362, acc: 0.164062]\n",
      "4503: [D loss: 0.693479, acc: 0.521484]  [A loss: 0.754020, acc: 0.359375]\n",
      "4504: [D loss: 0.734202, acc: 0.476562]  [A loss: 0.955830, acc: 0.089844]\n",
      "4505: [D loss: 0.695939, acc: 0.537109]  [A loss: 0.719443, acc: 0.488281]\n",
      "4506: [D loss: 0.726937, acc: 0.515625]  [A loss: 0.895624, acc: 0.156250]\n",
      "4507: [D loss: 0.704683, acc: 0.500000]  [A loss: 0.770585, acc: 0.398438]\n",
      "4508: [D loss: 0.720446, acc: 0.511719]  [A loss: 0.925874, acc: 0.152344]\n",
      "4509: [D loss: 0.706032, acc: 0.519531]  [A loss: 0.728223, acc: 0.437500]\n",
      "4510: [D loss: 0.713456, acc: 0.527344]  [A loss: 0.967018, acc: 0.093750]\n",
      "4511: [D loss: 0.705471, acc: 0.501953]  [A loss: 0.749501, acc: 0.398438]\n",
      "4512: [D loss: 0.706573, acc: 0.521484]  [A loss: 0.829500, acc: 0.218750]\n",
      "4513: [D loss: 0.697479, acc: 0.515625]  [A loss: 0.845134, acc: 0.253906]\n",
      "4514: [D loss: 0.697665, acc: 0.527344]  [A loss: 0.775352, acc: 0.359375]\n",
      "4515: [D loss: 0.711682, acc: 0.505859]  [A loss: 0.874421, acc: 0.183594]\n",
      "4516: [D loss: 0.696294, acc: 0.541016]  [A loss: 0.805951, acc: 0.289062]\n",
      "4517: [D loss: 0.704351, acc: 0.548828]  [A loss: 0.894726, acc: 0.164062]\n",
      "4518: [D loss: 0.711399, acc: 0.501953]  [A loss: 0.785904, acc: 0.339844]\n",
      "4519: [D loss: 0.722641, acc: 0.511719]  [A loss: 0.985877, acc: 0.082031]\n",
      "4520: [D loss: 0.707134, acc: 0.488281]  [A loss: 0.713975, acc: 0.507812]\n",
      "4521: [D loss: 0.717474, acc: 0.531250]  [A loss: 0.930437, acc: 0.140625]\n",
      "4522: [D loss: 0.707463, acc: 0.509766]  [A loss: 0.653800, acc: 0.578125]\n",
      "4523: [D loss: 0.737729, acc: 0.509766]  [A loss: 1.036022, acc: 0.062500]\n",
      "4524: [D loss: 0.704313, acc: 0.521484]  [A loss: 0.670618, acc: 0.562500]\n",
      "4525: [D loss: 0.723463, acc: 0.500000]  [A loss: 0.918448, acc: 0.144531]\n",
      "4526: [D loss: 0.699719, acc: 0.521484]  [A loss: 0.763708, acc: 0.343750]\n",
      "4527: [D loss: 0.718762, acc: 0.498047]  [A loss: 0.881612, acc: 0.140625]\n",
      "4528: [D loss: 0.707576, acc: 0.488281]  [A loss: 0.747868, acc: 0.394531]\n",
      "4529: [D loss: 0.721726, acc: 0.472656]  [A loss: 0.838723, acc: 0.230469]\n",
      "4530: [D loss: 0.714771, acc: 0.480469]  [A loss: 0.754978, acc: 0.367188]\n",
      "4531: [D loss: 0.705894, acc: 0.501953]  [A loss: 0.860344, acc: 0.199219]\n",
      "4532: [D loss: 0.704416, acc: 0.511719]  [A loss: 0.833530, acc: 0.250000]\n",
      "4533: [D loss: 0.718546, acc: 0.490234]  [A loss: 0.896950, acc: 0.164062]\n",
      "4534: [D loss: 0.708490, acc: 0.498047]  [A loss: 0.761825, acc: 0.359375]\n",
      "4535: [D loss: 0.704160, acc: 0.529297]  [A loss: 0.910351, acc: 0.109375]\n",
      "4536: [D loss: 0.701195, acc: 0.513672]  [A loss: 0.698954, acc: 0.503906]\n",
      "4537: [D loss: 0.739473, acc: 0.496094]  [A loss: 1.033188, acc: 0.031250]\n",
      "4538: [D loss: 0.715807, acc: 0.484375]  [A loss: 0.677181, acc: 0.582031]\n",
      "4539: [D loss: 0.725211, acc: 0.525391]  [A loss: 0.950426, acc: 0.109375]\n",
      "4540: [D loss: 0.698253, acc: 0.492188]  [A loss: 0.715038, acc: 0.496094]\n",
      "4541: [D loss: 0.699046, acc: 0.541016]  [A loss: 0.816094, acc: 0.253906]\n",
      "4542: [D loss: 0.688220, acc: 0.544922]  [A loss: 0.760723, acc: 0.343750]\n",
      "4543: [D loss: 0.718893, acc: 0.484375]  [A loss: 0.920254, acc: 0.152344]\n",
      "4544: [D loss: 0.705558, acc: 0.494141]  [A loss: 0.776051, acc: 0.343750]\n",
      "4545: [D loss: 0.703454, acc: 0.515625]  [A loss: 0.862361, acc: 0.218750]\n",
      "4546: [D loss: 0.691107, acc: 0.548828]  [A loss: 0.788627, acc: 0.312500]\n",
      "4547: [D loss: 0.704392, acc: 0.527344]  [A loss: 0.840533, acc: 0.238281]\n",
      "4548: [D loss: 0.723103, acc: 0.478516]  [A loss: 0.865535, acc: 0.226562]\n",
      "4549: [D loss: 0.693605, acc: 0.562500]  [A loss: 0.815145, acc: 0.285156]\n",
      "4550: [D loss: 0.706477, acc: 0.521484]  [A loss: 0.862455, acc: 0.199219]\n",
      "4551: [D loss: 0.716576, acc: 0.464844]  [A loss: 0.822559, acc: 0.269531]\n",
      "4552: [D loss: 0.713803, acc: 0.484375]  [A loss: 0.912952, acc: 0.125000]\n",
      "4553: [D loss: 0.702360, acc: 0.515625]  [A loss: 0.695528, acc: 0.476562]\n",
      "4554: [D loss: 0.720057, acc: 0.498047]  [A loss: 1.009296, acc: 0.085938]\n",
      "4555: [D loss: 0.698230, acc: 0.517578]  [A loss: 0.634128, acc: 0.664062]\n",
      "4556: [D loss: 0.748596, acc: 0.503906]  [A loss: 0.998727, acc: 0.066406]\n",
      "4557: [D loss: 0.722901, acc: 0.486328]  [A loss: 0.647661, acc: 0.628906]\n",
      "4558: [D loss: 0.754009, acc: 0.486328]  [A loss: 0.997417, acc: 0.070312]\n",
      "4559: [D loss: 0.716158, acc: 0.490234]  [A loss: 0.696790, acc: 0.531250]\n",
      "4560: [D loss: 0.716829, acc: 0.513672]  [A loss: 0.877689, acc: 0.144531]\n",
      "4561: [D loss: 0.695163, acc: 0.505859]  [A loss: 0.778009, acc: 0.312500]\n",
      "4562: [D loss: 0.716808, acc: 0.482422]  [A loss: 0.876775, acc: 0.167969]\n",
      "4563: [D loss: 0.701447, acc: 0.509766]  [A loss: 0.790467, acc: 0.304688]\n",
      "4564: [D loss: 0.711475, acc: 0.500000]  [A loss: 0.874800, acc: 0.152344]\n",
      "4565: [D loss: 0.702617, acc: 0.535156]  [A loss: 0.779606, acc: 0.300781]\n",
      "4566: [D loss: 0.703427, acc: 0.537109]  [A loss: 0.897320, acc: 0.167969]\n",
      "4567: [D loss: 0.695134, acc: 0.544922]  [A loss: 0.723322, acc: 0.492188]\n",
      "4568: [D loss: 0.711263, acc: 0.521484]  [A loss: 0.930365, acc: 0.085938]\n",
      "4569: [D loss: 0.691665, acc: 0.539062]  [A loss: 0.713193, acc: 0.468750]\n",
      "4570: [D loss: 0.718428, acc: 0.507812]  [A loss: 0.974642, acc: 0.085938]\n",
      "4571: [D loss: 0.694572, acc: 0.550781]  [A loss: 0.726165, acc: 0.406250]\n",
      "4572: [D loss: 0.727117, acc: 0.500000]  [A loss: 1.006336, acc: 0.062500]\n",
      "4573: [D loss: 0.695350, acc: 0.521484]  [A loss: 0.749829, acc: 0.406250]\n",
      "4574: [D loss: 0.708587, acc: 0.519531]  [A loss: 0.914205, acc: 0.128906]\n",
      "4575: [D loss: 0.717121, acc: 0.464844]  [A loss: 0.731091, acc: 0.437500]\n",
      "4576: [D loss: 0.720487, acc: 0.496094]  [A loss: 0.911556, acc: 0.132812]\n",
      "4577: [D loss: 0.690352, acc: 0.560547]  [A loss: 0.702735, acc: 0.511719]\n",
      "4578: [D loss: 0.731259, acc: 0.503906]  [A loss: 0.912921, acc: 0.128906]\n",
      "4579: [D loss: 0.712701, acc: 0.490234]  [A loss: 0.778922, acc: 0.339844]\n",
      "4580: [D loss: 0.721464, acc: 0.480469]  [A loss: 0.840185, acc: 0.242188]\n",
      "4581: [D loss: 0.711931, acc: 0.492188]  [A loss: 0.796880, acc: 0.339844]\n",
      "4582: [D loss: 0.716144, acc: 0.480469]  [A loss: 0.881055, acc: 0.160156]\n",
      "4583: [D loss: 0.695194, acc: 0.531250]  [A loss: 0.795713, acc: 0.324219]\n",
      "4584: [D loss: 0.711735, acc: 0.503906]  [A loss: 0.923789, acc: 0.144531]\n",
      "4585: [D loss: 0.693961, acc: 0.527344]  [A loss: 0.735899, acc: 0.417969]\n",
      "4586: [D loss: 0.718763, acc: 0.505859]  [A loss: 0.968760, acc: 0.082031]\n",
      "4587: [D loss: 0.717820, acc: 0.484375]  [A loss: 0.789107, acc: 0.320312]\n",
      "4588: [D loss: 0.712808, acc: 0.513672]  [A loss: 0.946596, acc: 0.113281]\n",
      "4589: [D loss: 0.703190, acc: 0.517578]  [A loss: 0.701947, acc: 0.496094]\n",
      "4590: [D loss: 0.731373, acc: 0.531250]  [A loss: 0.902678, acc: 0.148438]\n",
      "4591: [D loss: 0.701150, acc: 0.503906]  [A loss: 0.738793, acc: 0.445312]\n",
      "4592: [D loss: 0.717496, acc: 0.521484]  [A loss: 0.854910, acc: 0.222656]\n",
      "4593: [D loss: 0.716395, acc: 0.480469]  [A loss: 0.832964, acc: 0.281250]\n",
      "4594: [D loss: 0.710976, acc: 0.507812]  [A loss: 0.818352, acc: 0.273438]\n",
      "4595: [D loss: 0.715522, acc: 0.498047]  [A loss: 0.899726, acc: 0.128906]\n",
      "4596: [D loss: 0.699767, acc: 0.496094]  [A loss: 0.724187, acc: 0.468750]\n",
      "4597: [D loss: 0.720745, acc: 0.498047]  [A loss: 0.945212, acc: 0.093750]\n",
      "4598: [D loss: 0.688654, acc: 0.537109]  [A loss: 0.724409, acc: 0.472656]\n",
      "4599: [D loss: 0.742105, acc: 0.472656]  [A loss: 0.892127, acc: 0.164062]\n",
      "4600: [D loss: 0.704479, acc: 0.498047]  [A loss: 0.826297, acc: 0.234375]\n",
      "4601: [D loss: 0.702229, acc: 0.537109]  [A loss: 0.895529, acc: 0.152344]\n",
      "4602: [D loss: 0.702060, acc: 0.494141]  [A loss: 0.780074, acc: 0.324219]\n",
      "4603: [D loss: 0.702801, acc: 0.521484]  [A loss: 0.952057, acc: 0.097656]\n",
      "4604: [D loss: 0.705521, acc: 0.507812]  [A loss: 0.697034, acc: 0.511719]\n",
      "4605: [D loss: 0.709028, acc: 0.515625]  [A loss: 0.978011, acc: 0.097656]\n",
      "4606: [D loss: 0.700790, acc: 0.525391]  [A loss: 0.684517, acc: 0.550781]\n",
      "4607: [D loss: 0.712896, acc: 0.525391]  [A loss: 0.996650, acc: 0.066406]\n",
      "4608: [D loss: 0.704072, acc: 0.500000]  [A loss: 0.649024, acc: 0.660156]\n",
      "4609: [D loss: 0.721219, acc: 0.523438]  [A loss: 0.989519, acc: 0.062500]\n",
      "4610: [D loss: 0.701900, acc: 0.546875]  [A loss: 0.690280, acc: 0.535156]\n",
      "4611: [D loss: 0.728363, acc: 0.488281]  [A loss: 0.930183, acc: 0.105469]\n",
      "4612: [D loss: 0.699269, acc: 0.511719]  [A loss: 0.692545, acc: 0.539062]\n",
      "4613: [D loss: 0.717599, acc: 0.507812]  [A loss: 0.843822, acc: 0.195312]\n",
      "4614: [D loss: 0.696767, acc: 0.533203]  [A loss: 0.749517, acc: 0.433594]\n",
      "4615: [D loss: 0.711044, acc: 0.511719]  [A loss: 0.905882, acc: 0.132812]\n",
      "4616: [D loss: 0.705784, acc: 0.513672]  [A loss: 0.804096, acc: 0.277344]\n",
      "4617: [D loss: 0.721970, acc: 0.458984]  [A loss: 0.893703, acc: 0.148438]\n",
      "4618: [D loss: 0.688719, acc: 0.513672]  [A loss: 0.745861, acc: 0.378906]\n",
      "4619: [D loss: 0.710213, acc: 0.511719]  [A loss: 0.977940, acc: 0.097656]\n",
      "4620: [D loss: 0.692440, acc: 0.527344]  [A loss: 0.680046, acc: 0.570312]\n",
      "4621: [D loss: 0.709092, acc: 0.529297]  [A loss: 0.885910, acc: 0.140625]\n",
      "4622: [D loss: 0.703916, acc: 0.529297]  [A loss: 0.715058, acc: 0.507812]\n",
      "4623: [D loss: 0.716158, acc: 0.529297]  [A loss: 0.857146, acc: 0.207031]\n",
      "4624: [D loss: 0.711136, acc: 0.503906]  [A loss: 0.817012, acc: 0.250000]\n",
      "4625: [D loss: 0.707318, acc: 0.523438]  [A loss: 0.854364, acc: 0.226562]\n",
      "4626: [D loss: 0.707451, acc: 0.492188]  [A loss: 0.885883, acc: 0.175781]\n",
      "4627: [D loss: 0.698295, acc: 0.527344]  [A loss: 0.764254, acc: 0.378906]\n",
      "4628: [D loss: 0.705337, acc: 0.513672]  [A loss: 0.874941, acc: 0.140625]\n",
      "4629: [D loss: 0.708978, acc: 0.519531]  [A loss: 0.751094, acc: 0.363281]\n",
      "4630: [D loss: 0.715556, acc: 0.505859]  [A loss: 0.886334, acc: 0.144531]\n",
      "4631: [D loss: 0.718349, acc: 0.472656]  [A loss: 0.905046, acc: 0.121094]\n",
      "4632: [D loss: 0.712972, acc: 0.488281]  [A loss: 0.789504, acc: 0.347656]\n",
      "4633: [D loss: 0.715641, acc: 0.513672]  [A loss: 1.002660, acc: 0.097656]\n",
      "4634: [D loss: 0.711432, acc: 0.494141]  [A loss: 0.668239, acc: 0.593750]\n",
      "4635: [D loss: 0.748190, acc: 0.496094]  [A loss: 1.001342, acc: 0.046875]\n",
      "4636: [D loss: 0.707135, acc: 0.494141]  [A loss: 0.658044, acc: 0.644531]\n",
      "4637: [D loss: 0.725646, acc: 0.525391]  [A loss: 0.901864, acc: 0.117188]\n",
      "4638: [D loss: 0.699556, acc: 0.521484]  [A loss: 0.720037, acc: 0.496094]\n",
      "4639: [D loss: 0.715968, acc: 0.505859]  [A loss: 0.804323, acc: 0.339844]\n",
      "4640: [D loss: 0.695620, acc: 0.542969]  [A loss: 0.812163, acc: 0.265625]\n",
      "4641: [D loss: 0.702796, acc: 0.523438]  [A loss: 0.856044, acc: 0.175781]\n",
      "4642: [D loss: 0.710296, acc: 0.484375]  [A loss: 0.736325, acc: 0.437500]\n",
      "4643: [D loss: 0.722663, acc: 0.498047]  [A loss: 0.928266, acc: 0.089844]\n",
      "4644: [D loss: 0.716779, acc: 0.480469]  [A loss: 0.729731, acc: 0.457031]\n",
      "4645: [D loss: 0.715896, acc: 0.511719]  [A loss: 0.994464, acc: 0.054688]\n",
      "4646: [D loss: 0.703400, acc: 0.478516]  [A loss: 0.724146, acc: 0.437500]\n",
      "4647: [D loss: 0.719372, acc: 0.503906]  [A loss: 0.886522, acc: 0.113281]\n",
      "4648: [D loss: 0.703338, acc: 0.507812]  [A loss: 0.729862, acc: 0.449219]\n",
      "4649: [D loss: 0.699338, acc: 0.519531]  [A loss: 0.876322, acc: 0.171875]\n",
      "4650: [D loss: 0.696651, acc: 0.527344]  [A loss: 0.773027, acc: 0.382812]\n",
      "4651: [D loss: 0.706845, acc: 0.517578]  [A loss: 0.857849, acc: 0.226562]\n",
      "4652: [D loss: 0.708068, acc: 0.492188]  [A loss: 0.841300, acc: 0.253906]\n",
      "4653: [D loss: 0.693327, acc: 0.554688]  [A loss: 0.867837, acc: 0.222656]\n",
      "4654: [D loss: 0.690560, acc: 0.519531]  [A loss: 0.789793, acc: 0.320312]\n",
      "4655: [D loss: 0.715870, acc: 0.494141]  [A loss: 0.865488, acc: 0.179688]\n",
      "4656: [D loss: 0.705052, acc: 0.496094]  [A loss: 0.808518, acc: 0.269531]\n",
      "4657: [D loss: 0.694948, acc: 0.554688]  [A loss: 0.886625, acc: 0.160156]\n",
      "4658: [D loss: 0.703827, acc: 0.523438]  [A loss: 0.839983, acc: 0.210938]\n",
      "4659: [D loss: 0.711688, acc: 0.501953]  [A loss: 0.837841, acc: 0.222656]\n",
      "4660: [D loss: 0.709197, acc: 0.492188]  [A loss: 0.789040, acc: 0.343750]\n",
      "4661: [D loss: 0.705959, acc: 0.507812]  [A loss: 0.921354, acc: 0.093750]\n",
      "4662: [D loss: 0.696778, acc: 0.529297]  [A loss: 0.763684, acc: 0.382812]\n",
      "4663: [D loss: 0.716372, acc: 0.505859]  [A loss: 0.997566, acc: 0.050781]\n",
      "4664: [D loss: 0.692867, acc: 0.539062]  [A loss: 0.691263, acc: 0.574219]\n",
      "4665: [D loss: 0.731038, acc: 0.531250]  [A loss: 1.052227, acc: 0.027344]\n",
      "4666: [D loss: 0.698376, acc: 0.552734]  [A loss: 0.696470, acc: 0.527344]\n",
      "4667: [D loss: 0.762140, acc: 0.494141]  [A loss: 0.999804, acc: 0.058594]\n",
      "4668: [D loss: 0.696492, acc: 0.537109]  [A loss: 0.649305, acc: 0.636719]\n",
      "4669: [D loss: 0.742442, acc: 0.498047]  [A loss: 1.027076, acc: 0.070312]\n",
      "4670: [D loss: 0.719815, acc: 0.492188]  [A loss: 0.675237, acc: 0.562500]\n",
      "4671: [D loss: 0.719397, acc: 0.511719]  [A loss: 0.907857, acc: 0.128906]\n",
      "4672: [D loss: 0.689388, acc: 0.542969]  [A loss: 0.731284, acc: 0.484375]\n",
      "4673: [D loss: 0.708821, acc: 0.515625]  [A loss: 0.888726, acc: 0.203125]\n",
      "4674: [D loss: 0.701290, acc: 0.490234]  [A loss: 0.766483, acc: 0.367188]\n",
      "4675: [D loss: 0.702636, acc: 0.517578]  [A loss: 0.810597, acc: 0.218750]\n",
      "4676: [D loss: 0.724868, acc: 0.464844]  [A loss: 0.819187, acc: 0.269531]\n",
      "4677: [D loss: 0.716437, acc: 0.505859]  [A loss: 0.843895, acc: 0.226562]\n",
      "4678: [D loss: 0.711940, acc: 0.494141]  [A loss: 0.753998, acc: 0.375000]\n",
      "4679: [D loss: 0.712415, acc: 0.521484]  [A loss: 0.839280, acc: 0.242188]\n",
      "4680: [D loss: 0.706051, acc: 0.511719]  [A loss: 0.787442, acc: 0.281250]\n",
      "4681: [D loss: 0.712135, acc: 0.500000]  [A loss: 0.821219, acc: 0.234375]\n",
      "4682: [D loss: 0.710341, acc: 0.509766]  [A loss: 0.787608, acc: 0.324219]\n",
      "4683: [D loss: 0.709806, acc: 0.529297]  [A loss: 0.822652, acc: 0.250000]\n",
      "4684: [D loss: 0.692791, acc: 0.546875]  [A loss: 0.816816, acc: 0.285156]\n",
      "4685: [D loss: 0.715917, acc: 0.486328]  [A loss: 0.798879, acc: 0.324219]\n",
      "4686: [D loss: 0.709073, acc: 0.546875]  [A loss: 0.897020, acc: 0.140625]\n",
      "4687: [D loss: 0.703153, acc: 0.539062]  [A loss: 0.751685, acc: 0.414062]\n",
      "4688: [D loss: 0.710602, acc: 0.511719]  [A loss: 0.932967, acc: 0.105469]\n",
      "4689: [D loss: 0.710246, acc: 0.490234]  [A loss: 0.835266, acc: 0.238281]\n",
      "4690: [D loss: 0.706356, acc: 0.525391]  [A loss: 0.879192, acc: 0.167969]\n",
      "4691: [D loss: 0.695054, acc: 0.537109]  [A loss: 0.794357, acc: 0.265625]\n",
      "4692: [D loss: 0.719296, acc: 0.494141]  [A loss: 0.949526, acc: 0.132812]\n",
      "4693: [D loss: 0.709571, acc: 0.525391]  [A loss: 0.676424, acc: 0.519531]\n",
      "4694: [D loss: 0.736014, acc: 0.501953]  [A loss: 0.972696, acc: 0.078125]\n",
      "4695: [D loss: 0.695115, acc: 0.576172]  [A loss: 0.722413, acc: 0.441406]\n",
      "4696: [D loss: 0.728245, acc: 0.527344]  [A loss: 0.887192, acc: 0.136719]\n",
      "4697: [D loss: 0.701938, acc: 0.513672]  [A loss: 0.727451, acc: 0.453125]\n",
      "4698: [D loss: 0.714014, acc: 0.505859]  [A loss: 0.838631, acc: 0.242188]\n",
      "4699: [D loss: 0.697757, acc: 0.550781]  [A loss: 0.779428, acc: 0.355469]\n",
      "4700: [D loss: 0.697945, acc: 0.533203]  [A loss: 0.892199, acc: 0.136719]\n",
      "4701: [D loss: 0.708461, acc: 0.503906]  [A loss: 0.800018, acc: 0.289062]\n",
      "4702: [D loss: 0.704211, acc: 0.513672]  [A loss: 0.992368, acc: 0.089844]\n",
      "4703: [D loss: 0.688866, acc: 0.527344]  [A loss: 0.763102, acc: 0.425781]\n",
      "4704: [D loss: 0.723290, acc: 0.509766]  [A loss: 0.951773, acc: 0.085938]\n",
      "4705: [D loss: 0.700022, acc: 0.519531]  [A loss: 0.721531, acc: 0.441406]\n",
      "4706: [D loss: 0.730627, acc: 0.466797]  [A loss: 0.918482, acc: 0.097656]\n",
      "4707: [D loss: 0.703748, acc: 0.494141]  [A loss: 0.833369, acc: 0.269531]\n",
      "4708: [D loss: 0.713274, acc: 0.498047]  [A loss: 0.907504, acc: 0.144531]\n",
      "4709: [D loss: 0.709413, acc: 0.503906]  [A loss: 0.724850, acc: 0.453125]\n",
      "4710: [D loss: 0.720904, acc: 0.503906]  [A loss: 0.898333, acc: 0.117188]\n",
      "4711: [D loss: 0.707576, acc: 0.500000]  [A loss: 0.756182, acc: 0.410156]\n",
      "4712: [D loss: 0.705880, acc: 0.503906]  [A loss: 0.844907, acc: 0.199219]\n",
      "4713: [D loss: 0.692486, acc: 0.544922]  [A loss: 0.822417, acc: 0.281250]\n",
      "4714: [D loss: 0.694158, acc: 0.539062]  [A loss: 0.800700, acc: 0.304688]\n",
      "4715: [D loss: 0.706229, acc: 0.501953]  [A loss: 0.829593, acc: 0.253906]\n",
      "4716: [D loss: 0.711300, acc: 0.507812]  [A loss: 0.945970, acc: 0.140625]\n",
      "4717: [D loss: 0.688587, acc: 0.531250]  [A loss: 0.775429, acc: 0.312500]\n",
      "4718: [D loss: 0.692828, acc: 0.550781]  [A loss: 0.886788, acc: 0.152344]\n",
      "4719: [D loss: 0.683176, acc: 0.542969]  [A loss: 0.815638, acc: 0.324219]\n",
      "4720: [D loss: 0.702928, acc: 0.507812]  [A loss: 0.876442, acc: 0.164062]\n",
      "4721: [D loss: 0.702724, acc: 0.509766]  [A loss: 0.807868, acc: 0.281250]\n",
      "4722: [D loss: 0.720057, acc: 0.486328]  [A loss: 0.984983, acc: 0.078125]\n",
      "4723: [D loss: 0.706730, acc: 0.500000]  [A loss: 0.639347, acc: 0.656250]\n",
      "4724: [D loss: 0.744850, acc: 0.525391]  [A loss: 1.084852, acc: 0.046875]\n",
      "4725: [D loss: 0.719215, acc: 0.480469]  [A loss: 0.597011, acc: 0.734375]\n",
      "4726: [D loss: 0.786285, acc: 0.480469]  [A loss: 0.995823, acc: 0.066406]\n",
      "4727: [D loss: 0.700794, acc: 0.505859]  [A loss: 0.673258, acc: 0.574219]\n",
      "4728: [D loss: 0.726498, acc: 0.494141]  [A loss: 1.003731, acc: 0.070312]\n",
      "4729: [D loss: 0.695980, acc: 0.529297]  [A loss: 0.740614, acc: 0.394531]\n",
      "4730: [D loss: 0.700521, acc: 0.552734]  [A loss: 0.812713, acc: 0.230469]\n",
      "4731: [D loss: 0.711033, acc: 0.498047]  [A loss: 0.820816, acc: 0.226562]\n",
      "4732: [D loss: 0.711824, acc: 0.494141]  [A loss: 0.801634, acc: 0.277344]\n",
      "4733: [D loss: 0.720880, acc: 0.484375]  [A loss: 0.847500, acc: 0.242188]\n",
      "4734: [D loss: 0.690912, acc: 0.525391]  [A loss: 0.749258, acc: 0.375000]\n",
      "4735: [D loss: 0.706173, acc: 0.501953]  [A loss: 0.862653, acc: 0.222656]\n",
      "4736: [D loss: 0.720781, acc: 0.470703]  [A loss: 0.859290, acc: 0.218750]\n",
      "4737: [D loss: 0.706612, acc: 0.527344]  [A loss: 0.870523, acc: 0.207031]\n",
      "4738: [D loss: 0.707363, acc: 0.501953]  [A loss: 0.777506, acc: 0.308594]\n",
      "4739: [D loss: 0.707272, acc: 0.486328]  [A loss: 0.886818, acc: 0.144531]\n",
      "4740: [D loss: 0.706428, acc: 0.505859]  [A loss: 0.755051, acc: 0.375000]\n",
      "4741: [D loss: 0.699676, acc: 0.554688]  [A loss: 0.861916, acc: 0.226562]\n",
      "4742: [D loss: 0.713818, acc: 0.509766]  [A loss: 0.785435, acc: 0.351562]\n",
      "4743: [D loss: 0.724511, acc: 0.494141]  [A loss: 0.896118, acc: 0.136719]\n",
      "4744: [D loss: 0.703925, acc: 0.503906]  [A loss: 0.730229, acc: 0.406250]\n",
      "4745: [D loss: 0.710176, acc: 0.507812]  [A loss: 1.018489, acc: 0.062500]\n",
      "4746: [D loss: 0.708003, acc: 0.494141]  [A loss: 0.684456, acc: 0.539062]\n",
      "4747: [D loss: 0.727704, acc: 0.519531]  [A loss: 0.871897, acc: 0.160156]\n",
      "4748: [D loss: 0.706111, acc: 0.527344]  [A loss: 0.785702, acc: 0.332031]\n",
      "4749: [D loss: 0.704491, acc: 0.529297]  [A loss: 0.915074, acc: 0.136719]\n",
      "4750: [D loss: 0.693461, acc: 0.531250]  [A loss: 0.810700, acc: 0.261719]\n",
      "4751: [D loss: 0.693752, acc: 0.560547]  [A loss: 0.828730, acc: 0.210938]\n",
      "4752: [D loss: 0.694837, acc: 0.505859]  [A loss: 0.847465, acc: 0.191406]\n",
      "4753: [D loss: 0.701592, acc: 0.529297]  [A loss: 0.821963, acc: 0.269531]\n",
      "4754: [D loss: 0.696850, acc: 0.544922]  [A loss: 0.855986, acc: 0.238281]\n",
      "4755: [D loss: 0.698030, acc: 0.531250]  [A loss: 0.776871, acc: 0.367188]\n",
      "4756: [D loss: 0.700425, acc: 0.548828]  [A loss: 0.901399, acc: 0.160156]\n",
      "4757: [D loss: 0.688174, acc: 0.535156]  [A loss: 0.820851, acc: 0.316406]\n",
      "4758: [D loss: 0.724463, acc: 0.466797]  [A loss: 0.881085, acc: 0.144531]\n",
      "4759: [D loss: 0.712837, acc: 0.492188]  [A loss: 0.836371, acc: 0.273438]\n",
      "4760: [D loss: 0.690929, acc: 0.554688]  [A loss: 0.821517, acc: 0.269531]\n",
      "4761: [D loss: 0.707091, acc: 0.482422]  [A loss: 0.777964, acc: 0.343750]\n",
      "4762: [D loss: 0.709592, acc: 0.501953]  [A loss: 0.930608, acc: 0.148438]\n",
      "4763: [D loss: 0.714250, acc: 0.517578]  [A loss: 0.783676, acc: 0.375000]\n",
      "4764: [D loss: 0.712533, acc: 0.507812]  [A loss: 1.006941, acc: 0.074219]\n",
      "4765: [D loss: 0.699568, acc: 0.544922]  [A loss: 0.690504, acc: 0.531250]\n",
      "4766: [D loss: 0.727054, acc: 0.496094]  [A loss: 0.954486, acc: 0.097656]\n",
      "4767: [D loss: 0.709141, acc: 0.509766]  [A loss: 0.702031, acc: 0.496094]\n",
      "4768: [D loss: 0.718867, acc: 0.517578]  [A loss: 0.913972, acc: 0.105469]\n",
      "4769: [D loss: 0.701090, acc: 0.505859]  [A loss: 0.731821, acc: 0.480469]\n",
      "4770: [D loss: 0.708195, acc: 0.519531]  [A loss: 0.902016, acc: 0.171875]\n",
      "4771: [D loss: 0.702641, acc: 0.535156]  [A loss: 0.746203, acc: 0.406250]\n",
      "4772: [D loss: 0.720804, acc: 0.500000]  [A loss: 0.865243, acc: 0.179688]\n",
      "4773: [D loss: 0.702896, acc: 0.509766]  [A loss: 0.711722, acc: 0.492188]\n",
      "4774: [D loss: 0.716746, acc: 0.523438]  [A loss: 0.954156, acc: 0.128906]\n",
      "4775: [D loss: 0.702851, acc: 0.511719]  [A loss: 0.679512, acc: 0.527344]\n",
      "4776: [D loss: 0.717461, acc: 0.517578]  [A loss: 0.939377, acc: 0.125000]\n",
      "4777: [D loss: 0.712232, acc: 0.492188]  [A loss: 0.758632, acc: 0.363281]\n",
      "4778: [D loss: 0.712678, acc: 0.480469]  [A loss: 0.837821, acc: 0.238281]\n",
      "4779: [D loss: 0.703127, acc: 0.511719]  [A loss: 0.814145, acc: 0.277344]\n",
      "4780: [D loss: 0.717581, acc: 0.505859]  [A loss: 0.907716, acc: 0.121094]\n",
      "4781: [D loss: 0.698584, acc: 0.517578]  [A loss: 0.737636, acc: 0.441406]\n",
      "4782: [D loss: 0.719245, acc: 0.513672]  [A loss: 0.965176, acc: 0.113281]\n",
      "4783: [D loss: 0.697252, acc: 0.517578]  [A loss: 0.702058, acc: 0.539062]\n",
      "4784: [D loss: 0.729624, acc: 0.494141]  [A loss: 0.899939, acc: 0.132812]\n",
      "4785: [D loss: 0.702142, acc: 0.513672]  [A loss: 0.733014, acc: 0.441406]\n",
      "4786: [D loss: 0.723105, acc: 0.501953]  [A loss: 0.900420, acc: 0.140625]\n",
      "4787: [D loss: 0.702154, acc: 0.525391]  [A loss: 0.748862, acc: 0.402344]\n",
      "4788: [D loss: 0.703900, acc: 0.503906]  [A loss: 0.844972, acc: 0.218750]\n",
      "4789: [D loss: 0.705196, acc: 0.503906]  [A loss: 0.725658, acc: 0.457031]\n",
      "4790: [D loss: 0.711844, acc: 0.511719]  [A loss: 0.920197, acc: 0.140625]\n",
      "4791: [D loss: 0.699455, acc: 0.531250]  [A loss: 0.755781, acc: 0.414062]\n",
      "4792: [D loss: 0.711979, acc: 0.505859]  [A loss: 0.863701, acc: 0.203125]\n",
      "4793: [D loss: 0.716414, acc: 0.484375]  [A loss: 0.793103, acc: 0.285156]\n",
      "4794: [D loss: 0.713271, acc: 0.533203]  [A loss: 0.891736, acc: 0.109375]\n",
      "4795: [D loss: 0.698776, acc: 0.542969]  [A loss: 0.762507, acc: 0.363281]\n",
      "4796: [D loss: 0.706982, acc: 0.521484]  [A loss: 0.944608, acc: 0.125000]\n",
      "4797: [D loss: 0.696299, acc: 0.513672]  [A loss: 0.746040, acc: 0.433594]\n",
      "4798: [D loss: 0.722985, acc: 0.478516]  [A loss: 0.856380, acc: 0.187500]\n",
      "4799: [D loss: 0.699282, acc: 0.507812]  [A loss: 0.769966, acc: 0.332031]\n",
      "4800: [D loss: 0.706990, acc: 0.505859]  [A loss: 0.885855, acc: 0.148438]\n",
      "4801: [D loss: 0.700559, acc: 0.539062]  [A loss: 0.812327, acc: 0.281250]\n",
      "4802: [D loss: 0.710672, acc: 0.503906]  [A loss: 0.813564, acc: 0.296875]\n",
      "4803: [D loss: 0.704417, acc: 0.513672]  [A loss: 0.904526, acc: 0.164062]\n",
      "4804: [D loss: 0.699390, acc: 0.523438]  [A loss: 0.905176, acc: 0.195312]\n",
      "4805: [D loss: 0.710099, acc: 0.503906]  [A loss: 0.927442, acc: 0.140625]\n",
      "4806: [D loss: 0.691810, acc: 0.541016]  [A loss: 0.767477, acc: 0.332031]\n",
      "4807: [D loss: 0.713546, acc: 0.519531]  [A loss: 0.956703, acc: 0.109375]\n",
      "4808: [D loss: 0.703192, acc: 0.521484]  [A loss: 0.693418, acc: 0.531250]\n",
      "4809: [D loss: 0.710285, acc: 0.535156]  [A loss: 0.963785, acc: 0.109375]\n",
      "4810: [D loss: 0.714266, acc: 0.490234]  [A loss: 0.684461, acc: 0.589844]\n",
      "4811: [D loss: 0.718458, acc: 0.511719]  [A loss: 0.951958, acc: 0.085938]\n",
      "4812: [D loss: 0.704962, acc: 0.507812]  [A loss: 0.695096, acc: 0.511719]\n",
      "4813: [D loss: 0.721929, acc: 0.490234]  [A loss: 0.963344, acc: 0.113281]\n",
      "4814: [D loss: 0.706353, acc: 0.542969]  [A loss: 0.768280, acc: 0.375000]\n",
      "4815: [D loss: 0.701418, acc: 0.529297]  [A loss: 0.851980, acc: 0.234375]\n",
      "4816: [D loss: 0.693083, acc: 0.556641]  [A loss: 0.750295, acc: 0.394531]\n",
      "4817: [D loss: 0.715354, acc: 0.501953]  [A loss: 0.895015, acc: 0.152344]\n",
      "4818: [D loss: 0.701038, acc: 0.544922]  [A loss: 0.791774, acc: 0.328125]\n",
      "4819: [D loss: 0.709265, acc: 0.527344]  [A loss: 0.903121, acc: 0.125000]\n",
      "4820: [D loss: 0.703647, acc: 0.515625]  [A loss: 0.743055, acc: 0.394531]\n",
      "4821: [D loss: 0.699113, acc: 0.542969]  [A loss: 0.948598, acc: 0.132812]\n",
      "4822: [D loss: 0.709879, acc: 0.462891]  [A loss: 0.679321, acc: 0.578125]\n",
      "4823: [D loss: 0.721205, acc: 0.496094]  [A loss: 0.909549, acc: 0.117188]\n",
      "4824: [D loss: 0.714616, acc: 0.494141]  [A loss: 0.743329, acc: 0.375000]\n",
      "4825: [D loss: 0.731393, acc: 0.480469]  [A loss: 0.929561, acc: 0.093750]\n",
      "4826: [D loss: 0.692311, acc: 0.531250]  [A loss: 0.701962, acc: 0.500000]\n",
      "4827: [D loss: 0.711888, acc: 0.503906]  [A loss: 0.872876, acc: 0.203125]\n",
      "4828: [D loss: 0.695185, acc: 0.519531]  [A loss: 0.735984, acc: 0.406250]\n",
      "4829: [D loss: 0.717654, acc: 0.490234]  [A loss: 0.945472, acc: 0.085938]\n",
      "4830: [D loss: 0.703411, acc: 0.509766]  [A loss: 0.724671, acc: 0.421875]\n",
      "4831: [D loss: 0.713932, acc: 0.521484]  [A loss: 0.926647, acc: 0.160156]\n",
      "4832: [D loss: 0.711753, acc: 0.501953]  [A loss: 0.788208, acc: 0.328125]\n",
      "4833: [D loss: 0.699973, acc: 0.531250]  [A loss: 0.832205, acc: 0.179688]\n",
      "4834: [D loss: 0.699856, acc: 0.521484]  [A loss: 0.820392, acc: 0.296875]\n",
      "4835: [D loss: 0.687830, acc: 0.568359]  [A loss: 0.799380, acc: 0.328125]\n",
      "4836: [D loss: 0.715945, acc: 0.505859]  [A loss: 0.860782, acc: 0.218750]\n",
      "4837: [D loss: 0.697286, acc: 0.548828]  [A loss: 0.840640, acc: 0.222656]\n",
      "4838: [D loss: 0.713541, acc: 0.498047]  [A loss: 0.864671, acc: 0.191406]\n",
      "4839: [D loss: 0.686563, acc: 0.548828]  [A loss: 0.750821, acc: 0.406250]\n",
      "4840: [D loss: 0.727844, acc: 0.503906]  [A loss: 1.022723, acc: 0.042969]\n",
      "4841: [D loss: 0.704937, acc: 0.513672]  [A loss: 0.672753, acc: 0.554688]\n",
      "4842: [D loss: 0.714688, acc: 0.490234]  [A loss: 0.946728, acc: 0.117188]\n",
      "4843: [D loss: 0.697732, acc: 0.503906]  [A loss: 0.680351, acc: 0.550781]\n",
      "4844: [D loss: 0.722497, acc: 0.494141]  [A loss: 0.892866, acc: 0.160156]\n",
      "4845: [D loss: 0.710006, acc: 0.480469]  [A loss: 0.845438, acc: 0.218750]\n",
      "4846: [D loss: 0.693052, acc: 0.535156]  [A loss: 0.802493, acc: 0.250000]\n",
      "4847: [D loss: 0.696719, acc: 0.539062]  [A loss: 0.795536, acc: 0.312500]\n",
      "4848: [D loss: 0.716941, acc: 0.494141]  [A loss: 0.872735, acc: 0.222656]\n",
      "4849: [D loss: 0.702767, acc: 0.525391]  [A loss: 0.870348, acc: 0.171875]\n",
      "4850: [D loss: 0.698955, acc: 0.511719]  [A loss: 0.846752, acc: 0.195312]\n",
      "4851: [D loss: 0.696355, acc: 0.515625]  [A loss: 0.982496, acc: 0.109375]\n",
      "4852: [D loss: 0.706829, acc: 0.515625]  [A loss: 0.829080, acc: 0.242188]\n",
      "4853: [D loss: 0.688770, acc: 0.531250]  [A loss: 0.928274, acc: 0.144531]\n",
      "4854: [D loss: 0.717024, acc: 0.470703]  [A loss: 0.749672, acc: 0.390625]\n",
      "4855: [D loss: 0.730385, acc: 0.501953]  [A loss: 0.928114, acc: 0.132812]\n",
      "4856: [D loss: 0.696412, acc: 0.533203]  [A loss: 0.702876, acc: 0.480469]\n",
      "4857: [D loss: 0.720404, acc: 0.523438]  [A loss: 0.943973, acc: 0.132812]\n",
      "4858: [D loss: 0.720820, acc: 0.470703]  [A loss: 0.782405, acc: 0.316406]\n",
      "4859: [D loss: 0.708101, acc: 0.517578]  [A loss: 0.840339, acc: 0.230469]\n",
      "4860: [D loss: 0.707998, acc: 0.505859]  [A loss: 0.879879, acc: 0.203125]\n",
      "4861: [D loss: 0.705135, acc: 0.498047]  [A loss: 0.810501, acc: 0.277344]\n",
      "4862: [D loss: 0.705877, acc: 0.529297]  [A loss: 0.843408, acc: 0.199219]\n",
      "4863: [D loss: 0.709148, acc: 0.482422]  [A loss: 0.810822, acc: 0.300781]\n",
      "4864: [D loss: 0.716097, acc: 0.486328]  [A loss: 0.892497, acc: 0.136719]\n",
      "4865: [D loss: 0.708209, acc: 0.507812]  [A loss: 0.713157, acc: 0.472656]\n",
      "4866: [D loss: 0.730451, acc: 0.505859]  [A loss: 1.062263, acc: 0.082031]\n",
      "4867: [D loss: 0.708539, acc: 0.496094]  [A loss: 0.677131, acc: 0.554688]\n",
      "4868: [D loss: 0.732436, acc: 0.509766]  [A loss: 0.937655, acc: 0.117188]\n",
      "4869: [D loss: 0.691522, acc: 0.548828]  [A loss: 0.714563, acc: 0.492188]\n",
      "4870: [D loss: 0.716210, acc: 0.501953]  [A loss: 0.960114, acc: 0.113281]\n",
      "4871: [D loss: 0.689612, acc: 0.525391]  [A loss: 0.753821, acc: 0.390625]\n",
      "4872: [D loss: 0.703969, acc: 0.501953]  [A loss: 0.837827, acc: 0.250000]\n",
      "4873: [D loss: 0.703933, acc: 0.513672]  [A loss: 0.761555, acc: 0.382812]\n",
      "4874: [D loss: 0.715352, acc: 0.525391]  [A loss: 0.874060, acc: 0.183594]\n",
      "4875: [D loss: 0.699274, acc: 0.511719]  [A loss: 0.733855, acc: 0.417969]\n",
      "4876: [D loss: 0.718532, acc: 0.492188]  [A loss: 0.950252, acc: 0.113281]\n",
      "4877: [D loss: 0.703573, acc: 0.484375]  [A loss: 0.719733, acc: 0.445312]\n",
      "4878: [D loss: 0.701598, acc: 0.531250]  [A loss: 0.906801, acc: 0.152344]\n",
      "4879: [D loss: 0.694088, acc: 0.533203]  [A loss: 0.727819, acc: 0.460938]\n",
      "4880: [D loss: 0.716877, acc: 0.509766]  [A loss: 0.925972, acc: 0.097656]\n",
      "4881: [D loss: 0.712005, acc: 0.500000]  [A loss: 0.740633, acc: 0.414062]\n",
      "4882: [D loss: 0.707970, acc: 0.501953]  [A loss: 0.867351, acc: 0.238281]\n",
      "4883: [D loss: 0.705704, acc: 0.521484]  [A loss: 0.766765, acc: 0.343750]\n",
      "4884: [D loss: 0.714739, acc: 0.496094]  [A loss: 0.886335, acc: 0.167969]\n",
      "4885: [D loss: 0.707005, acc: 0.496094]  [A loss: 0.810783, acc: 0.281250]\n",
      "4886: [D loss: 0.715753, acc: 0.501953]  [A loss: 0.830064, acc: 0.246094]\n",
      "4887: [D loss: 0.706916, acc: 0.503906]  [A loss: 0.756954, acc: 0.406250]\n",
      "4888: [D loss: 0.717509, acc: 0.519531]  [A loss: 0.905960, acc: 0.132812]\n",
      "4889: [D loss: 0.706039, acc: 0.500000]  [A loss: 0.701888, acc: 0.539062]\n",
      "4890: [D loss: 0.703453, acc: 0.552734]  [A loss: 0.851030, acc: 0.210938]\n",
      "4891: [D loss: 0.700917, acc: 0.525391]  [A loss: 0.763216, acc: 0.339844]\n",
      "4892: [D loss: 0.713694, acc: 0.496094]  [A loss: 0.877033, acc: 0.148438]\n",
      "4893: [D loss: 0.701499, acc: 0.517578]  [A loss: 0.774409, acc: 0.343750]\n",
      "4894: [D loss: 0.720461, acc: 0.490234]  [A loss: 0.956270, acc: 0.078125]\n",
      "4895: [D loss: 0.705608, acc: 0.496094]  [A loss: 0.753423, acc: 0.363281]\n",
      "4896: [D loss: 0.716095, acc: 0.509766]  [A loss: 0.828417, acc: 0.250000]\n",
      "4897: [D loss: 0.705660, acc: 0.505859]  [A loss: 0.800864, acc: 0.292969]\n",
      "4898: [D loss: 0.705106, acc: 0.521484]  [A loss: 0.849167, acc: 0.238281]\n",
      "4899: [D loss: 0.696580, acc: 0.523438]  [A loss: 0.818118, acc: 0.242188]\n",
      "4900: [D loss: 0.703722, acc: 0.531250]  [A loss: 0.759251, acc: 0.351562]\n",
      "4901: [D loss: 0.709939, acc: 0.525391]  [A loss: 0.958331, acc: 0.097656]\n",
      "4902: [D loss: 0.692989, acc: 0.541016]  [A loss: 0.672068, acc: 0.570312]\n",
      "4903: [D loss: 0.722436, acc: 0.509766]  [A loss: 0.922768, acc: 0.117188]\n",
      "4904: [D loss: 0.697045, acc: 0.525391]  [A loss: 0.701577, acc: 0.519531]\n",
      "4905: [D loss: 0.727735, acc: 0.496094]  [A loss: 0.937443, acc: 0.105469]\n",
      "4906: [D loss: 0.700001, acc: 0.552734]  [A loss: 0.710524, acc: 0.480469]\n",
      "4907: [D loss: 0.725335, acc: 0.490234]  [A loss: 0.883630, acc: 0.144531]\n",
      "4908: [D loss: 0.706219, acc: 0.507812]  [A loss: 0.829913, acc: 0.261719]\n",
      "4909: [D loss: 0.697183, acc: 0.541016]  [A loss: 0.824214, acc: 0.257812]\n",
      "4910: [D loss: 0.710199, acc: 0.484375]  [A loss: 0.824250, acc: 0.246094]\n",
      "4911: [D loss: 0.707107, acc: 0.517578]  [A loss: 0.828269, acc: 0.218750]\n",
      "4912: [D loss: 0.688404, acc: 0.541016]  [A loss: 0.745662, acc: 0.425781]\n",
      "4913: [D loss: 0.703272, acc: 0.513672]  [A loss: 0.888392, acc: 0.171875]\n",
      "4914: [D loss: 0.708881, acc: 0.496094]  [A loss: 0.828796, acc: 0.316406]\n",
      "4915: [D loss: 0.707758, acc: 0.509766]  [A loss: 0.942792, acc: 0.132812]\n",
      "4916: [D loss: 0.687019, acc: 0.550781]  [A loss: 0.741750, acc: 0.394531]\n",
      "4917: [D loss: 0.707998, acc: 0.492188]  [A loss: 0.892032, acc: 0.144531]\n",
      "4918: [D loss: 0.686622, acc: 0.558594]  [A loss: 0.708333, acc: 0.519531]\n",
      "4919: [D loss: 0.710909, acc: 0.501953]  [A loss: 0.964923, acc: 0.089844]\n",
      "4920: [D loss: 0.706312, acc: 0.509766]  [A loss: 0.753333, acc: 0.390625]\n",
      "4921: [D loss: 0.718662, acc: 0.480469]  [A loss: 0.897179, acc: 0.144531]\n",
      "4922: [D loss: 0.692691, acc: 0.535156]  [A loss: 0.738357, acc: 0.425781]\n",
      "4923: [D loss: 0.741913, acc: 0.500000]  [A loss: 0.934319, acc: 0.128906]\n",
      "4924: [D loss: 0.684094, acc: 0.574219]  [A loss: 0.714988, acc: 0.503906]\n",
      "4925: [D loss: 0.726732, acc: 0.488281]  [A loss: 0.846715, acc: 0.234375]\n",
      "4926: [D loss: 0.714823, acc: 0.498047]  [A loss: 0.867629, acc: 0.175781]\n",
      "4927: [D loss: 0.693665, acc: 0.496094]  [A loss: 0.774798, acc: 0.355469]\n",
      "4928: [D loss: 0.702020, acc: 0.511719]  [A loss: 0.877882, acc: 0.187500]\n",
      "4929: [D loss: 0.704762, acc: 0.519531]  [A loss: 0.802325, acc: 0.289062]\n",
      "4930: [D loss: 0.713841, acc: 0.507812]  [A loss: 0.846014, acc: 0.183594]\n",
      "4931: [D loss: 0.714826, acc: 0.505859]  [A loss: 0.852093, acc: 0.179688]\n",
      "4932: [D loss: 0.697933, acc: 0.531250]  [A loss: 0.847009, acc: 0.183594]\n",
      "4933: [D loss: 0.696175, acc: 0.525391]  [A loss: 0.831175, acc: 0.250000]\n",
      "4934: [D loss: 0.692755, acc: 0.552734]  [A loss: 0.818987, acc: 0.281250]\n",
      "4935: [D loss: 0.706356, acc: 0.494141]  [A loss: 0.783849, acc: 0.320312]\n",
      "4936: [D loss: 0.692276, acc: 0.529297]  [A loss: 0.885117, acc: 0.214844]\n",
      "4937: [D loss: 0.719992, acc: 0.484375]  [A loss: 0.799988, acc: 0.304688]\n",
      "4938: [D loss: 0.710087, acc: 0.519531]  [A loss: 0.930765, acc: 0.140625]\n",
      "4939: [D loss: 0.687551, acc: 0.544922]  [A loss: 0.682229, acc: 0.550781]\n",
      "4940: [D loss: 0.727910, acc: 0.519531]  [A loss: 0.969252, acc: 0.125000]\n",
      "4941: [D loss: 0.711391, acc: 0.525391]  [A loss: 0.776345, acc: 0.335938]\n",
      "4942: [D loss: 0.723812, acc: 0.527344]  [A loss: 0.931231, acc: 0.093750]\n",
      "4943: [D loss: 0.716680, acc: 0.466797]  [A loss: 0.741868, acc: 0.421875]\n",
      "4944: [D loss: 0.734489, acc: 0.513672]  [A loss: 1.034114, acc: 0.042969]\n",
      "4945: [D loss: 0.713517, acc: 0.498047]  [A loss: 0.682944, acc: 0.593750]\n",
      "4946: [D loss: 0.730951, acc: 0.498047]  [A loss: 0.873913, acc: 0.175781]\n",
      "4947: [D loss: 0.700844, acc: 0.533203]  [A loss: 0.735523, acc: 0.421875]\n",
      "4948: [D loss: 0.699435, acc: 0.523438]  [A loss: 0.840320, acc: 0.207031]\n",
      "4949: [D loss: 0.707152, acc: 0.511719]  [A loss: 0.733801, acc: 0.425781]\n",
      "4950: [D loss: 0.708958, acc: 0.527344]  [A loss: 0.800146, acc: 0.281250]\n",
      "4951: [D loss: 0.702624, acc: 0.500000]  [A loss: 0.762327, acc: 0.398438]\n",
      "4952: [D loss: 0.695176, acc: 0.531250]  [A loss: 0.774028, acc: 0.378906]\n",
      "4953: [D loss: 0.700248, acc: 0.521484]  [A loss: 0.913828, acc: 0.132812]\n",
      "4954: [D loss: 0.715339, acc: 0.474609]  [A loss: 0.772038, acc: 0.316406]\n",
      "4955: [D loss: 0.712681, acc: 0.517578]  [A loss: 0.877137, acc: 0.183594]\n",
      "4956: [D loss: 0.709915, acc: 0.500000]  [A loss: 0.851355, acc: 0.218750]\n",
      "4957: [D loss: 0.698832, acc: 0.507812]  [A loss: 0.856950, acc: 0.238281]\n",
      "4958: [D loss: 0.704539, acc: 0.507812]  [A loss: 0.793426, acc: 0.285156]\n",
      "4959: [D loss: 0.715779, acc: 0.496094]  [A loss: 0.847195, acc: 0.222656]\n",
      "4960: [D loss: 0.705903, acc: 0.519531]  [A loss: 0.835108, acc: 0.199219]\n",
      "4961: [D loss: 0.707527, acc: 0.531250]  [A loss: 0.814498, acc: 0.281250]\n",
      "4962: [D loss: 0.701191, acc: 0.519531]  [A loss: 0.819032, acc: 0.281250]\n",
      "4963: [D loss: 0.702226, acc: 0.517578]  [A loss: 0.723830, acc: 0.496094]\n",
      "4964: [D loss: 0.705449, acc: 0.515625]  [A loss: 0.908637, acc: 0.117188]\n",
      "4965: [D loss: 0.699531, acc: 0.531250]  [A loss: 0.778602, acc: 0.355469]\n",
      "4966: [D loss: 0.710317, acc: 0.509766]  [A loss: 0.909998, acc: 0.117188]\n",
      "4967: [D loss: 0.694006, acc: 0.523438]  [A loss: 0.721096, acc: 0.488281]\n",
      "4968: [D loss: 0.719547, acc: 0.496094]  [A loss: 0.901013, acc: 0.144531]\n",
      "4969: [D loss: 0.697429, acc: 0.542969]  [A loss: 0.729927, acc: 0.390625]\n",
      "4970: [D loss: 0.724733, acc: 0.480469]  [A loss: 0.934398, acc: 0.121094]\n",
      "4971: [D loss: 0.694407, acc: 0.523438]  [A loss: 0.746326, acc: 0.414062]\n",
      "4972: [D loss: 0.724831, acc: 0.505859]  [A loss: 0.871302, acc: 0.183594]\n",
      "4973: [D loss: 0.718544, acc: 0.490234]  [A loss: 0.917588, acc: 0.152344]\n",
      "4974: [D loss: 0.687474, acc: 0.566406]  [A loss: 0.752717, acc: 0.375000]\n",
      "4975: [D loss: 0.722636, acc: 0.482422]  [A loss: 0.960958, acc: 0.097656]\n",
      "4976: [D loss: 0.702159, acc: 0.498047]  [A loss: 0.736611, acc: 0.429688]\n",
      "4977: [D loss: 0.707927, acc: 0.509766]  [A loss: 0.851839, acc: 0.250000]\n",
      "4978: [D loss: 0.712301, acc: 0.490234]  [A loss: 0.759470, acc: 0.402344]\n",
      "4979: [D loss: 0.699985, acc: 0.546875]  [A loss: 0.876738, acc: 0.187500]\n",
      "4980: [D loss: 0.675520, acc: 0.574219]  [A loss: 0.802205, acc: 0.281250]\n",
      "4981: [D loss: 0.730208, acc: 0.486328]  [A loss: 0.943250, acc: 0.089844]\n",
      "4982: [D loss: 0.692833, acc: 0.537109]  [A loss: 0.678658, acc: 0.562500]\n",
      "4983: [D loss: 0.730340, acc: 0.500000]  [A loss: 0.926861, acc: 0.113281]\n",
      "4984: [D loss: 0.713914, acc: 0.496094]  [A loss: 0.771038, acc: 0.371094]\n",
      "4985: [D loss: 0.714530, acc: 0.496094]  [A loss: 0.838132, acc: 0.246094]\n",
      "4986: [D loss: 0.708707, acc: 0.513672]  [A loss: 0.845506, acc: 0.230469]\n",
      "4987: [D loss: 0.715334, acc: 0.529297]  [A loss: 0.934815, acc: 0.125000]\n",
      "4988: [D loss: 0.707089, acc: 0.496094]  [A loss: 0.725368, acc: 0.429688]\n",
      "4989: [D loss: 0.725981, acc: 0.496094]  [A loss: 0.846919, acc: 0.242188]\n",
      "4990: [D loss: 0.705082, acc: 0.519531]  [A loss: 0.767737, acc: 0.347656]\n",
      "4991: [D loss: 0.712388, acc: 0.525391]  [A loss: 0.882219, acc: 0.164062]\n",
      "4992: [D loss: 0.696467, acc: 0.539062]  [A loss: 0.758001, acc: 0.359375]\n",
      "4993: [D loss: 0.704422, acc: 0.546875]  [A loss: 0.913644, acc: 0.121094]\n",
      "4994: [D loss: 0.712238, acc: 0.478516]  [A loss: 0.746491, acc: 0.433594]\n",
      "4995: [D loss: 0.731178, acc: 0.478516]  [A loss: 0.958008, acc: 0.132812]\n",
      "4996: [D loss: 0.696846, acc: 0.546875]  [A loss: 0.742684, acc: 0.390625]\n",
      "4997: [D loss: 0.715863, acc: 0.494141]  [A loss: 0.888735, acc: 0.187500]\n",
      "4998: [D loss: 0.698736, acc: 0.531250]  [A loss: 0.724042, acc: 0.460938]\n",
      "4999: [D loss: 0.710757, acc: 0.521484]  [A loss: 0.858969, acc: 0.199219]\n"
     ]
    }
   ],
   "source": [
    "# Initialize MNIST DCGAN and train\n",
    "mnist_dcgan = MNIST_DCGAN(X_train_keras)\n",
    "timer = ElapsedTimer()\n",
    "mnist_dcgan.train(train_steps=5000, batch_size=256, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1458
    },
    "colab_type": "code",
    "id": "UFnA61e_E65l",
    "outputId": "e2090100-353b-40f6-de7a-4957d1376578",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 42.0252006371816 min \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noifkIier/LcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate batch of synthetic MNIST images\n",
    "timer.elapsed_time()\n",
    "mnist_dcgan.plot_images(fake=True)\n",
    "mnist_dcgan.plot_images(fake=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12_GAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
