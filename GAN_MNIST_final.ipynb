{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwp8U_vSIz0s"
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7yQjE32oE645",
    "outputId": "b8ef3a37-f8b3-4a58-df27-01e37f16c66e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''Main'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re\n",
    "import pickle, gzip, datetime\n",
    "\n",
    "'''Data Viz'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import Grid\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''Data Prep and Model Evaluation'''\n",
    "from sklearn import preprocessing as pp\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, mean_squared_error\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "'''Algos'''\n",
    "import lightgbm as lgb\n",
    "\n",
    "'''TensorFlow and Keras'''\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import LeakyReLU, Reshape, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Input, Lambda\n",
    "from keras.layers import Embedding, Flatten, dot\n",
    "from keras import regularizers\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Kb5mJxBI5JW"
   },
   "source": [
    "# SET UP GOOGLE COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# this will prompt for authorization\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BIglkQnyIElN",
    "outputId": "3b4b822b-4bef-4930-c601-2da964ff2925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Colab Notebooks'\n"
     ]
    }
   ],
   "source": [
    "# after executing the cell above, Drive files available in \"/content/drive/My Drive\"\n",
    "! ls \"/content/drive/My Drive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U5MR_5CJI_OF"
   },
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aYO5HID2JLgy",
    "outputId": "e5a6cf51-7e7d-4521-a1a6-146e52c3764d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMGYe_XKE649"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "current_path = os.getcwd()\n",
    "file = '/drive/My Drive/Colab Notebooks'+'/datasets/mnist_data/mnist.pkl.gz'\n",
    "f = gzip.open(current_path+file, 'rb')\n",
    "train_set, validation_set, test_set = pickle.load(f, encoding='latin1')\n",
    "f.close()\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_validation, y_validation = validation_set[0], validation_set[1]\n",
    "X_test, y_test = test_set[0], test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yryy_zjCE65A"
   },
   "outputs": [],
   "source": [
    "X_train_keras = X_train.reshape(50000,28,28,1)\n",
    "X_validation_keras = X_validation.reshape(10000,28,28,1)\n",
    "X_test_keras = X_test.reshape(10000,28,28,1)\n",
    "\n",
    "y_train_keras = to_categorical(y_train)\n",
    "y_validation_keras = to_categorical(y_validation)\n",
    "y_test_keras = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xO4vT2ONE65C"
   },
   "outputs": [],
   "source": [
    "# Create Pandas DataFrames from the datasets\n",
    "train_index = range(0,len(X_train))\n",
    "validation_index = range(len(X_train),len(X_train)+len(X_validation))\n",
    "test_index = range(len(X_train)+len(X_validation),len(X_train)+ \\\n",
    "                   len(X_validation)+len(X_test))\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,index=train_index)\n",
    "y_train = pd.Series(data=y_train,index=train_index)\n",
    "\n",
    "X_validation = pd.DataFrame(data=X_validation,index=validation_index)\n",
    "y_validation = pd.Series(data=y_validation,index=validation_index)\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test,index=test_index)\n",
    "y_test = pd.Series(data=y_test,index=test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5wXfZmaTE65F"
   },
   "outputs": [],
   "source": [
    "def view_digit(X, y, example):\n",
    "    label = y.loc[example]\n",
    "    image = X.loc[example,:].values.reshape([28,28])\n",
    "    plt.title('Example: %d  Label: %d' % (example, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "hgQDEtnME65H",
    "outputId": "a66311e5-153d-4343-b9d6-9d84502c4e66"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEHCAYAAACHl1tOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXSQACrpIABR0kQAo6CIB+H9hqt0pGUrp/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the first digit\n",
    "view_digit(X_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "edhjAFk-qg_H",
    "outputId": "b2374191-5625-46f9-efa9-2511cd85d7d1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAEHCAYAAACHl1tOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_digit(X_train, y_train, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1B4tAnYKnSL"
   },
   "source": [
    "# CONFIRM USE OF GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AtUErICRE65K",
    "outputId": "2a56e213-033d-4c4c-de77-2b098cf6f45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Confirm use of GPU\n",
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else: print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqB-TfRKKvvq"
   },
   "source": [
    "# Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we first build a regular CNN to classify MNIST images. We use early stopping and save the best model to disk. \n",
    "- Feel free to skip if interested in the GAN problem only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5ydSEt2E65Q"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3505
    },
    "colab_type": "code",
    "id": "BqcAIbOCE65S",
    "outputId": "4c5eb21f-b9b7-45b7-c4c5-cdab9b1f3a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.1901 - acc: 0.9399 - val_loss: 0.0551 - val_acc: 0.9836\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0739 - acc: 0.9782 - val_loss: 0.0340 - val_acc: 0.9904\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0548 - acc: 0.9840 - val_loss: 0.0367 - val_acc: 0.9901\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0445 - acc: 0.9867 - val_loss: 0.0365 - val_acc: 0.9913\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0393 - acc: 0.9881 - val_loss: 0.0314 - val_acc: 0.9924\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0351 - acc: 0.9889 - val_loss: 0.0365 - val_acc: 0.9923\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0326 - acc: 0.9898 - val_loss: 0.0262 - val_acc: 0.9924\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0323 - acc: 0.9900 - val_loss: 0.0374 - val_acc: 0.9914\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0340 - val_acc: 0.9924\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0265 - acc: 0.9923 - val_loss: 0.0344 - val_acc: 0.9915\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0255 - acc: 0.9922 - val_loss: 0.0332 - val_acc: 0.9922\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0231 - acc: 0.9931 - val_loss: 0.0356 - val_acc: 0.9928\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.0307 - val_acc: 0.9927\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0233 - acc: 0.9923 - val_loss: 0.0374 - val_acc: 0.9923\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0231 - acc: 0.9930 - val_loss: 0.0393 - val_acc: 0.9919\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0208 - acc: 0.9937 - val_loss: 0.0341 - val_acc: 0.9928\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0337 - val_acc: 0.9937\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0315 - val_acc: 0.9929\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0213 - acc: 0.9939 - val_loss: 0.0353 - val_acc: 0.9930\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0315 - val_acc: 0.9935\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0323 - val_acc: 0.9937\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0182 - acc: 0.9946 - val_loss: 0.0405 - val_acc: 0.9931\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.0198 - acc: 0.9942 - val_loss: 0.0321 - val_acc: 0.9931\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 21s 410us/step - loss: 0.0175 - acc: 0.9951 - val_loss: 0.0330 - val_acc: 0.9935\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 20s 405us/step - loss: 0.0226 - acc: 0.9942 - val_loss: 0.0325 - val_acc: 0.9930\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 20s 402us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0528 - val_acc: 0.9910\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 20s 402us/step - loss: 0.0193 - acc: 0.9950 - val_loss: 0.0357 - val_acc: 0.9932\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 20s 402us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0345 - val_acc: 0.9931\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0412 - val_acc: 0.9929\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 20s 398us/step - loss: 0.0202 - acc: 0.9948 - val_loss: 0.0316 - val_acc: 0.9939\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0192 - acc: 0.9945 - val_loss: 0.0338 - val_acc: 0.9939\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0198 - acc: 0.9946 - val_loss: 0.0471 - val_acc: 0.9922\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0217 - acc: 0.9944 - val_loss: 0.0414 - val_acc: 0.9930\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0513 - val_acc: 0.9924\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0194 - acc: 0.9952 - val_loss: 0.0379 - val_acc: 0.9924\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0464 - val_acc: 0.9931\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 20s 395us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0462 - val_acc: 0.9931\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.0394 - val_acc: 0.9937\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 0.0205 - acc: 0.9946 - val_loss: 0.0411 - val_acc: 0.9929\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.0328 - val_acc: 0.9947\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0397 - val_acc: 0.9935\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0447 - val_acc: 0.9932\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 20s 394us/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.0435 - val_acc: 0.9943\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 20s 393us/step - loss: 0.0185 - acc: 0.9954 - val_loss: 0.0540 - val_acc: 0.9923\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0219 - acc: 0.9949 - val_loss: 0.0430 - val_acc: 0.9930\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0179 - acc: 0.9951 - val_loss: 0.0470 - val_acc: 0.9930\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0193 - acc: 0.9955 - val_loss: 0.0432 - val_acc: 0.9944\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0204 - acc: 0.9953 - val_loss: 0.0467 - val_acc: 0.9932\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0197 - acc: 0.9948 - val_loss: 0.0549 - val_acc: 0.9926\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0207 - acc: 0.9954 - val_loss: 0.0542 - val_acc: 0.9932\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0205 - acc: 0.9951 - val_loss: 0.0475 - val_acc: 0.9943\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 19s 380us/step - loss: 0.0227 - acc: 0.9949 - val_loss: 0.0499 - val_acc: 0.9930\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 24s 477us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0441 - val_acc: 0.9936\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 22s 436us/step - loss: 0.0197 - acc: 0.9950 - val_loss: 0.0474 - val_acc: 0.9928\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 22s 436us/step - loss: 0.0264 - acc: 0.9944 - val_loss: 0.0442 - val_acc: 0.9944\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0433 - val_acc: 0.9942\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 22s 432us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0489 - val_acc: 0.9935\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0240 - acc: 0.9948 - val_loss: 0.0464 - val_acc: 0.9936\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 21s 428us/step - loss: 0.0250 - acc: 0.9943 - val_loss: 0.0438 - val_acc: 0.9937\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0185 - acc: 0.9957 - val_loss: 0.0442 - val_acc: 0.9939\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0262 - acc: 0.9944 - val_loss: 0.0419 - val_acc: 0.9940\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0205 - acc: 0.9951 - val_loss: 0.0470 - val_acc: 0.9943\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0243 - acc: 0.9950 - val_loss: 0.0415 - val_acc: 0.9939\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0198 - acc: 0.9953 - val_loss: 0.0505 - val_acc: 0.9932\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.0257 - acc: 0.9943 - val_loss: 0.0499 - val_acc: 0.9936\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0276 - acc: 0.9944 - val_loss: 0.0508 - val_acc: 0.9923\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.0203 - acc: 0.9953 - val_loss: 0.0443 - val_acc: 0.9936\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.0235 - acc: 0.9951 - val_loss: 0.0510 - val_acc: 0.9945\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0248 - acc: 0.9948 - val_loss: 0.0565 - val_acc: 0.9937\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0268 - acc: 0.9945 - val_loss: 0.0492 - val_acc: 0.9937\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0261 - acc: 0.9948 - val_loss: 0.0455 - val_acc: 0.9938\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0279 - acc: 0.9950 - val_loss: 0.0538 - val_acc: 0.9935\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.0254 - acc: 0.9948 - val_loss: 0.0508 - val_acc: 0.9935\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.0220 - acc: 0.9954 - val_loss: 0.0620 - val_acc: 0.9932\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.0250 - acc: 0.9954 - val_loss: 0.0466 - val_acc: 0.9942\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.0244 - acc: 0.9950 - val_loss: 0.0451 - val_acc: 0.9942\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.0272 - acc: 0.9948 - val_loss: 0.0538 - val_acc: 0.9929\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.0236 - acc: 0.9954 - val_loss: 0.0565 - val_acc: 0.9935\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.0255 - acc: 0.9951 - val_loss: 0.0572 - val_acc: 0.9923\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 20s 409us/step - loss: 0.0229 - acc: 0.9951 - val_loss: 0.0516 - val_acc: 0.9930\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 21s 410us/step - loss: 0.0264 - acc: 0.9954 - val_loss: 0.0556 - val_acc: 0.9942\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 20s 410us/step - loss: 0.0263 - acc: 0.9948 - val_loss: 0.0736 - val_acc: 0.9920\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0307 - acc: 0.9944 - val_loss: 0.0534 - val_acc: 0.9933\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0252 - acc: 0.9948 - val_loss: 0.0584 - val_acc: 0.9932\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0230 - acc: 0.9951 - val_loss: 0.0500 - val_acc: 0.9939\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0272 - acc: 0.9948 - val_loss: 0.0572 - val_acc: 0.9931\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0247 - acc: 0.9953 - val_loss: 0.0453 - val_acc: 0.9947\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0270 - acc: 0.9946 - val_loss: 0.0511 - val_acc: 0.9945\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0283 - acc: 0.9946 - val_loss: 0.0692 - val_acc: 0.9924\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 20s 405us/step - loss: 0.0286 - acc: 0.9951 - val_loss: 0.0708 - val_acc: 0.9932\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0304 - acc: 0.9948 - val_loss: 0.0563 - val_acc: 0.9941\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0285 - acc: 0.9951 - val_loss: 0.0519 - val_acc: 0.9946\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 20s 406us/step - loss: 0.0273 - acc: 0.9954 - val_loss: 0.0733 - val_acc: 0.9926\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 20s 404us/step - loss: 0.0350 - acc: 0.9943 - val_loss: 0.0598 - val_acc: 0.9930\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.0250 - acc: 0.9954 - val_loss: 0.0718 - val_acc: 0.9934\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 20s 400us/step - loss: 0.0396 - acc: 0.9940 - val_loss: 0.0568 - val_acc: 0.9939\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 20s 396us/step - loss: 0.0285 - acc: 0.9951 - val_loss: 0.0509 - val_acc: 0.9939\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 28s 552us/step - loss: 0.0301 - acc: 0.9949 - val_loss: 0.0517 - val_acc: 0.9941\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 22s 433us/step - loss: 0.0283 - acc: 0.9954 - val_loss: 0.0691 - val_acc: 0.9930\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.0288 - acc: 0.9945 - val_loss: 0.0686 - val_acc: 0.9934\n"
     ]
    }
   ],
   "source": [
    "# Train CNN\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = model.fit(X_train_keras, y_train_keras, \n",
    "          validation_data=(X_validation_keras, y_validation_keras), \\\n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iaIwx_YeE65W",
    "outputId": "8d8e2dbe-a0fe-44cb-e95d-c67082ca2eb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(cnn_history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "KRmkdBr2E65Z",
    "outputId": "4c8cabf6-d88a-4c4b-f8d4-a20f92138537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.99454\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naUoKIiKSpqQgIiJp/x/tK5Pm/GsmUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracy of CNN\n",
    "print(\"CNN Final Accuracy\", cnn_history.history['acc'][-1])\n",
    "pd.Series(cnn_history.history['acc']).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFXGolCMQ_or"
   },
   "outputs": [],
   "source": [
    "# save the Keras CNN model\n",
    "model.save('cnn_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hd4bEYHoR4RI"
   },
   "outputs": [],
   "source": [
    "# can load the model without retraining\n",
    "from keras.models import load_model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model_loaded = load_model('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qrkFtE3TD3E"
   },
   "outputs": [],
   "source": [
    "def load_and_evaluate(model_name, return_model=False):\n",
    "    \"\"\"\n",
    "    Load in a trained model and evaluate with log loss and accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # model = load_model(f'{model_dir}{model_name}.h5')\n",
    "    model = load_model(model_name + '.h5')\n",
    "    r = model.evaluate(X_validation_keras, y_validation_keras, \n",
    "                       batch_size=2048, verbose=1)\n",
    "\n",
    "    valid_crossentropy = r[0]\n",
    "    valid_accuracy = r[1]\n",
    "\n",
    "    print(f'Cross Entropy: {round(valid_crossentropy, 4)}')\n",
    "    print(f'Accuracy: {round(100 * valid_accuracy, 2)}%')\n",
    "\n",
    "    if return_model:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "U_B2eHBpTEGq",
    "outputId": "8a572f81-726f-41f6-9946-441f41ccb17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 162us/step\n",
      "Cross Entropy: 0.0686\n",
      "Accuracy: 99.34%\n"
     ]
    }
   ],
   "source": [
    "model_loaded = load_and_evaluate('cnn_model', return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0rAXmGc8iyM"
   },
   "outputs": [],
   "source": [
    "# train with early stopping and save best model to disk\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def make_callbacks(model_name, save=True):\n",
    "    \"\"\"\n",
    "    Make list of callbacks for training\n",
    "    \"\"\"\n",
    "    \n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "    if save:\n",
    "        callbacks.append(\n",
    "            ModelCheckpoint(\n",
    "                f'{model_name}.h5',\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "callbacks = make_callbacks('cnn_model_early-stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "JkZyvelt9LYB",
    "outputId": "9b9a30d4-2436-4100-b9a9-52f0e7a51d1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 0.1820 - acc: 0.9442 - val_loss: 0.0466 - val_acc: 0.9861\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 20s 401us/step - loss: 0.0695 - acc: 0.9794 - val_loss: 0.0342 - val_acc: 0.9908\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 20s 397us/step - loss: 0.0520 - acc: 0.9846 - val_loss: 0.0340 - val_acc: 0.9895\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0460 - acc: 0.9858 - val_loss: 0.0290 - val_acc: 0.9923\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0382 - acc: 0.9879 - val_loss: 0.0278 - val_acc: 0.9916\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0357 - acc: 0.9893 - val_loss: 0.0374 - val_acc: 0.9923\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 20s 390us/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0330 - val_acc: 0.9912\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0297 - acc: 0.9913 - val_loss: 0.0381 - val_acc: 0.9907\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.0294 - acc: 0.9911 - val_loss: 0.0364 - val_acc: 0.9923\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 19s 389us/step - loss: 0.0268 - acc: 0.9924 - val_loss: 0.0275 - val_acc: 0.9941\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.0231 - acc: 0.9926 - val_loss: 0.0274 - val_acc: 0.9928\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0243 - acc: 0.9927 - val_loss: 0.0259 - val_acc: 0.9947\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0225 - acc: 0.9931 - val_loss: 0.0289 - val_acc: 0.9918\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0230 - acc: 0.9930 - val_loss: 0.0289 - val_acc: 0.9939\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0203 - acc: 0.9938 - val_loss: 0.0256 - val_acc: 0.9935\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0208 - acc: 0.9930 - val_loss: 0.0241 - val_acc: 0.9942\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0194 - acc: 0.9943 - val_loss: 0.0352 - val_acc: 0.9927\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 19s 385us/step - loss: 0.0204 - acc: 0.9940 - val_loss: 0.0275 - val_acc: 0.9935\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0377 - val_acc: 0.9920\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0198 - acc: 0.9944 - val_loss: 0.0293 - val_acc: 0.9942\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 19s 384us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0328 - val_acc: 0.9936\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_keras, y_train_keras, \n",
    "          validation_data=(X_validation_keras, y_validation_keras),\n",
    "          epochs=100,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "waiakvqrLZ6F",
    "outputId": "d84e8fd6-4997-41c5-a523-ec7e605ba37e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Final Accuracy 0.99428\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracy of CNN\n",
    "print(\"CNN Final Accuracy\", history.history['acc'][-1])\n",
    "pd.Series(history.history['acc']).plot(logy=False)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGZ3XXt_SJv5"
   },
   "source": [
    "# DEEP CONVOLUTIONAL GAN (DCGAN) NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we build and train a Deep Convolutional GAN, in order to create synthetic fake images that look similar to the original MNIST data. \n",
    "- The GAN network consists of a generator and discriminator that play against each other. \n",
    "- The discriminator is a referee that tries to tell the difference between real and fake images, whereas the generator tries to make the discriminator's life hard. In equilibrium the discrimnator cannot tell the difference between real and fake images, resulting in an accuracy of 50% (i.e., random guessing).\n",
    "- The training process is quite involved, with several comments trying to elucidate the idea behind this non-standard training process. \n",
    "- At the end we plot some fake images generated by our GAN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCjEWofkVP7o"
   },
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-j54NCCPqpVN"
   },
   "outputs": [],
   "source": [
    "class DCGAN_MNIST(object):\n",
    "    \n",
    "    def __init__(self, x_train):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = x_train\n",
    "        \n",
    "        # discriminator\n",
    "        self.D = self.discriminator()\n",
    "        # generator\n",
    "        self.G = self.generator()\n",
    "        # discriminator model\n",
    "        self.DM = self.discriminator_model()\n",
    "        # adversarial model (discrimnator stacked on top of generator)\n",
    "        self.AM = self.adversarial_model() \n",
    "    \n",
    "    \n",
    "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8,\n",
    "                  window=5, input_dim=100, output_depth=1): \n",
    "        gen = Sequential()\n",
    "        gen.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "        gen.add(Reshape((dim, dim, depth)))\n",
    "        gen.add(Dropout(dropout))\n",
    "        \n",
    "        gen.add(UpSampling2D())\n",
    "        gen.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "        gen.add(UpSampling2D())\n",
    "        gen.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "        gen.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "        gen.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
    "        gen.add(Activation('sigmoid'))\n",
    "        gen.summary()\n",
    "        return gen\n",
    "\n",
    "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3): \n",
    "        dis = Sequential()\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        dis.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Flatten())\n",
    "        dis.add(Dense(1))\n",
    "        dis.add(Activation('sigmoid'))\n",
    "        dis.summary()\n",
    "        return dis\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        DM = Sequential()\n",
    "        DM.add(self.D)\n",
    "        DM.compile(loss='binary_crossentropy',\n",
    "                   optimizer=optimizer, metrics=['accuracy'])\n",
    "        return DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        AM = Sequential()\n",
    "        AM.add(self.G)\n",
    "        # Set discriminator weights to non-trainable\n",
    "        # (will only apply to the `gan` model)\n",
    "        # self.D.trainable = False\n",
    "        AM.add(self.D)\n",
    "        AM.compile(loss='binary_crossentropy',\n",
    "                   optimizer=optimizer, metrics=['accuracy'])\n",
    "        return AM\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        \n",
    "        # loop over epochs\n",
    "        for i in range(train_steps):\n",
    "            # get random REAL training samples \n",
    "            images_train = self.x_train[np.random.randint(0, self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            # noise vector as input for generator\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            # generate fake images\n",
    "            images_fake = self.G.predict(noise)\n",
    "            # labeled sample contains real and fake images\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            # build label vector (real=1, fake=0)\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            # add random noise to the labels - important trick!\n",
    "            #y += 0.05 * np.random.random(y.shape)\n",
    "            \n",
    "            # train discriminator with real and fake images\n",
    "            # keras train_on_batch runs a single gradient update on a single batch of data\n",
    "            # returns loss and accuracy in list of scalars\n",
    "            d_loss = self.DM.train_on_batch(x, y)\n",
    "            \n",
    "            # train adversial network (generator+discriminator) with fake images\n",
    "            # NOTE: here fake images are labelled with y=1, since lose corresponds to fake image detected as fake\n",
    "            # generator does well if fake image declared as real in output of adversial network\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.AM.train_on_batch(noise, y)\n",
    "            \n",
    "            # track training progress in log message\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            \n",
    "            # print log messages\n",
    "            print(log_mesg)\n",
    "            \n",
    "            # plot and save fake sample during training every interval steps\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, \n",
    "                        samples=noise_input.shape[0],\n",
    "                        noise=noise_input, step=(i+1))\n",
    "    \n",
    "    # PLOT FAKE IMAGES\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16,\n",
    "                    noise=None, step=0):\n",
    "        \n",
    "        # location for storing fake images \n",
    "        current_path = os.getcwd()\n",
    "        file = '/drive/My Drive/Colab Notebooks/images/synthetic_mnist/'\n",
    "        filename = 'mnist.png'\n",
    "        \n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.G.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(current_path+file+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69377
    },
    "colab_type": "code",
    "id": "C3-sIg0J1y88",
    "outputId": "14815f03-7a1d-4964-ab06-540f2d3c5ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_13 (Conv2DT (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_14 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_15 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_16 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.694313, acc: 0.445312]  [A loss: 1.093180, acc: 0.000000]\n",
      "1: [D loss: 0.682395, acc: 0.500000]  [A loss: 1.003620, acc: 0.000000]\n",
      "2: [D loss: 0.625536, acc: 0.996094]  [A loss: 0.943510, acc: 0.000000]\n",
      "3: [D loss: 0.525030, acc: 0.697266]  [A loss: 1.437829, acc: 0.000000]\n",
      "4: [D loss: 0.571285, acc: 0.964844]  [A loss: 0.929844, acc: 0.000000]\n",
      "5: [D loss: 0.447801, acc: 0.767578]  [A loss: 1.532113, acc: 0.000000]\n",
      "6: [D loss: 0.504373, acc: 0.906250]  [A loss: 1.152758, acc: 0.000000]\n",
      "7: [D loss: 0.530190, acc: 0.515625]  [A loss: 1.805695, acc: 0.000000]\n",
      "8: [D loss: 0.601547, acc: 0.845703]  [A loss: 0.976706, acc: 0.000000]\n",
      "9: [D loss: 0.456732, acc: 0.677734]  [A loss: 1.020070, acc: 0.000000]\n",
      "10: [D loss: 0.431748, acc: 0.644531]  [A loss: 1.321097, acc: 0.000000]\n",
      "11: [D loss: 0.392052, acc: 0.998047]  [A loss: 1.068493, acc: 0.000000]\n",
      "12: [D loss: 0.573340, acc: 0.500000]  [A loss: 1.695986, acc: 0.000000]\n",
      "13: [D loss: 0.541270, acc: 0.892578]  [A loss: 1.044028, acc: 0.000000]\n",
      "14: [D loss: 0.389494, acc: 0.951172]  [A loss: 1.006855, acc: 0.000000]\n",
      "15: [D loss: 0.402749, acc: 0.691406]  [A loss: 1.272644, acc: 0.000000]\n",
      "16: [D loss: 0.367169, acc: 0.933594]  [A loss: 1.224933, acc: 0.000000]\n",
      "17: [D loss: 0.417439, acc: 0.693359]  [A loss: 1.494247, acc: 0.000000]\n",
      "18: [D loss: 0.371320, acc: 0.992188]  [A loss: 1.057064, acc: 0.011719]\n",
      "19: [D loss: 0.555531, acc: 0.521484]  [A loss: 1.802070, acc: 0.000000]\n",
      "20: [D loss: 0.492753, acc: 0.906250]  [A loss: 1.058781, acc: 0.003906]\n",
      "21: [D loss: 0.382679, acc: 0.791016]  [A loss: 1.185543, acc: 0.000000]\n",
      "22: [D loss: 0.335697, acc: 0.929688]  [A loss: 1.290374, acc: 0.000000]\n",
      "23: [D loss: 0.367415, acc: 0.816406]  [A loss: 1.507317, acc: 0.000000]\n",
      "24: [D loss: 0.312372, acc: 0.974609]  [A loss: 1.265975, acc: 0.000000]\n",
      "25: [D loss: 0.408803, acc: 0.726562]  [A loss: 1.979325, acc: 0.000000]\n",
      "26: [D loss: 0.410924, acc: 0.880859]  [A loss: 0.938319, acc: 0.152344]\n",
      "27: [D loss: 0.532709, acc: 0.583984]  [A loss: 1.695148, acc: 0.000000]\n",
      "28: [D loss: 0.339456, acc: 0.947266]  [A loss: 1.038353, acc: 0.062500]\n",
      "29: [D loss: 0.377474, acc: 0.757812]  [A loss: 1.458980, acc: 0.000000]\n",
      "30: [D loss: 0.265873, acc: 0.980469]  [A loss: 1.164300, acc: 0.031250]\n",
      "31: [D loss: 0.434651, acc: 0.710938]  [A loss: 1.906623, acc: 0.000000]\n",
      "32: [D loss: 0.335917, acc: 0.935547]  [A loss: 0.930933, acc: 0.179688]\n",
      "33: [D loss: 0.537990, acc: 0.626953]  [A loss: 1.830046, acc: 0.000000]\n",
      "34: [D loss: 0.353185, acc: 0.923828]  [A loss: 0.866584, acc: 0.269531]\n",
      "35: [D loss: 0.458771, acc: 0.683594]  [A loss: 1.502429, acc: 0.000000]\n",
      "36: [D loss: 0.321976, acc: 0.931641]  [A loss: 1.078684, acc: 0.070312]\n",
      "37: [D loss: 0.420958, acc: 0.722656]  [A loss: 1.691804, acc: 0.000000]\n",
      "38: [D loss: 0.347457, acc: 0.931641]  [A loss: 1.014199, acc: 0.156250]\n",
      "39: [D loss: 0.479578, acc: 0.677734]  [A loss: 1.871599, acc: 0.000000]\n",
      "40: [D loss: 0.384221, acc: 0.892578]  [A loss: 0.755466, acc: 0.429688]\n",
      "41: [D loss: 0.646589, acc: 0.554688]  [A loss: 1.795885, acc: 0.000000]\n",
      "42: [D loss: 0.449860, acc: 0.863281]  [A loss: 0.726328, acc: 0.468750]\n",
      "43: [D loss: 0.654970, acc: 0.525391]  [A loss: 1.460103, acc: 0.000000]\n",
      "44: [D loss: 0.396870, acc: 0.923828]  [A loss: 0.926355, acc: 0.128906]\n",
      "45: [D loss: 0.522036, acc: 0.593750]  [A loss: 1.406313, acc: 0.000000]\n",
      "46: [D loss: 0.429500, acc: 0.830078]  [A loss: 1.043787, acc: 0.050781]\n",
      "47: [D loss: 0.536567, acc: 0.587891]  [A loss: 1.520333, acc: 0.000000]\n",
      "48: [D loss: 0.460431, acc: 0.835938]  [A loss: 0.944903, acc: 0.082031]\n",
      "49: [D loss: 0.580466, acc: 0.546875]  [A loss: 1.627093, acc: 0.000000]\n",
      "50: [D loss: 0.488165, acc: 0.886719]  [A loss: 0.809277, acc: 0.273438]\n",
      "51: [D loss: 0.686904, acc: 0.505859]  [A loss: 1.654063, acc: 0.000000]\n",
      "52: [D loss: 0.490968, acc: 0.886719]  [A loss: 0.751584, acc: 0.363281]\n",
      "53: [D loss: 0.672560, acc: 0.505859]  [A loss: 1.474487, acc: 0.000000]\n",
      "54: [D loss: 0.484532, acc: 0.886719]  [A loss: 0.835272, acc: 0.160156]\n",
      "55: [D loss: 0.617165, acc: 0.513672]  [A loss: 1.460432, acc: 0.000000]\n",
      "56: [D loss: 0.482867, acc: 0.886719]  [A loss: 0.881620, acc: 0.101562]\n",
      "57: [D loss: 0.633900, acc: 0.505859]  [A loss: 1.543363, acc: 0.000000]\n",
      "58: [D loss: 0.500488, acc: 0.888672]  [A loss: 0.847081, acc: 0.171875]\n",
      "59: [D loss: 0.646881, acc: 0.501953]  [A loss: 1.587570, acc: 0.000000]\n",
      "60: [D loss: 0.516136, acc: 0.894531]  [A loss: 0.787465, acc: 0.234375]\n",
      "61: [D loss: 0.660332, acc: 0.501953]  [A loss: 1.521569, acc: 0.000000]\n",
      "62: [D loss: 0.519624, acc: 0.888672]  [A loss: 0.805190, acc: 0.171875]\n",
      "63: [D loss: 0.640783, acc: 0.501953]  [A loss: 1.543427, acc: 0.000000]\n",
      "64: [D loss: 0.503243, acc: 0.908203]  [A loss: 0.833988, acc: 0.121094]\n",
      "65: [D loss: 0.631335, acc: 0.503906]  [A loss: 1.597518, acc: 0.000000]\n",
      "66: [D loss: 0.498907, acc: 0.925781]  [A loss: 0.781677, acc: 0.238281]\n",
      "67: [D loss: 0.653413, acc: 0.501953]  [A loss: 1.639754, acc: 0.000000]\n",
      "68: [D loss: 0.511012, acc: 0.898438]  [A loss: 0.779327, acc: 0.250000]\n",
      "69: [D loss: 0.655493, acc: 0.500000]  [A loss: 1.538906, acc: 0.000000]\n",
      "70: [D loss: 0.506603, acc: 0.917969]  [A loss: 0.791817, acc: 0.218750]\n",
      "71: [D loss: 0.640242, acc: 0.500000]  [A loss: 1.621307, acc: 0.000000]\n",
      "72: [D loss: 0.490471, acc: 0.923828]  [A loss: 0.835370, acc: 0.164062]\n",
      "73: [D loss: 0.630369, acc: 0.507812]  [A loss: 1.627776, acc: 0.000000]\n",
      "74: [D loss: 0.506512, acc: 0.900391]  [A loss: 0.784504, acc: 0.273438]\n",
      "75: [D loss: 0.652278, acc: 0.500000]  [A loss: 1.624057, acc: 0.000000]\n",
      "76: [D loss: 0.491956, acc: 0.902344]  [A loss: 0.773600, acc: 0.277344]\n",
      "77: [D loss: 0.670945, acc: 0.501953]  [A loss: 1.649860, acc: 0.000000]\n",
      "78: [D loss: 0.517053, acc: 0.861328]  [A loss: 0.724083, acc: 0.421875]\n",
      "79: [D loss: 0.678251, acc: 0.500000]  [A loss: 1.597078, acc: 0.000000]\n",
      "80: [D loss: 0.514373, acc: 0.878906]  [A loss: 0.779590, acc: 0.234375]\n",
      "81: [D loss: 0.661176, acc: 0.500000]  [A loss: 1.579205, acc: 0.000000]\n",
      "82: [D loss: 0.494596, acc: 0.914062]  [A loss: 0.841672, acc: 0.148438]\n",
      "83: [D loss: 0.650293, acc: 0.507812]  [A loss: 1.665323, acc: 0.000000]\n",
      "84: [D loss: 0.521482, acc: 0.892578]  [A loss: 0.756397, acc: 0.316406]\n",
      "85: [D loss: 0.662951, acc: 0.500000]  [A loss: 1.634616, acc: 0.000000]\n",
      "86: [D loss: 0.517738, acc: 0.900391]  [A loss: 0.744557, acc: 0.363281]\n",
      "87: [D loss: 0.678403, acc: 0.503906]  [A loss: 1.702365, acc: 0.000000]\n",
      "88: [D loss: 0.527611, acc: 0.859375]  [A loss: 0.746256, acc: 0.367188]\n",
      "89: [D loss: 0.685029, acc: 0.500000]  [A loss: 1.574872, acc: 0.000000]\n",
      "90: [D loss: 0.522893, acc: 0.896484]  [A loss: 0.790055, acc: 0.257812]\n",
      "91: [D loss: 0.654441, acc: 0.501953]  [A loss: 1.656351, acc: 0.000000]\n",
      "92: [D loss: 0.502669, acc: 0.921875]  [A loss: 0.784376, acc: 0.304688]\n",
      "93: [D loss: 0.700442, acc: 0.505859]  [A loss: 1.780792, acc: 0.000000]\n",
      "94: [D loss: 0.562601, acc: 0.791016]  [A loss: 0.646994, acc: 0.617188]\n",
      "95: [D loss: 0.727638, acc: 0.498047]  [A loss: 1.420913, acc: 0.000000]\n",
      "96: [D loss: 0.541066, acc: 0.802734]  [A loss: 0.858537, acc: 0.156250]\n",
      "97: [D loss: 0.669770, acc: 0.501953]  [A loss: 1.639709, acc: 0.000000]\n",
      "98: [D loss: 0.537807, acc: 0.867188]  [A loss: 0.727706, acc: 0.394531]\n",
      "99: [D loss: 0.722568, acc: 0.503906]  [A loss: 1.749344, acc: 0.000000]\n",
      "100: [D loss: 0.562426, acc: 0.792969]  [A loss: 0.660432, acc: 0.605469]\n",
      "101: [D loss: 0.732997, acc: 0.505859]  [A loss: 1.411380, acc: 0.000000]\n",
      "102: [D loss: 0.551808, acc: 0.833984]  [A loss: 0.850621, acc: 0.140625]\n",
      "103: [D loss: 0.666308, acc: 0.503906]  [A loss: 1.642412, acc: 0.000000]\n",
      "104: [D loss: 0.543185, acc: 0.857422]  [A loss: 0.801395, acc: 0.250000]\n",
      "105: [D loss: 0.704778, acc: 0.507812]  [A loss: 1.758861, acc: 0.000000]\n",
      "106: [D loss: 0.576994, acc: 0.755859]  [A loss: 0.625654, acc: 0.679688]\n",
      "107: [D loss: 0.759491, acc: 0.500000]  [A loss: 1.550582, acc: 0.000000]\n",
      "108: [D loss: 0.556373, acc: 0.833984]  [A loss: 0.798005, acc: 0.253906]\n",
      "109: [D loss: 0.682095, acc: 0.507812]  [A loss: 1.691112, acc: 0.000000]\n",
      "110: [D loss: 0.550185, acc: 0.839844]  [A loss: 0.728615, acc: 0.414062]\n",
      "111: [D loss: 0.700213, acc: 0.501953]  [A loss: 1.739612, acc: 0.000000]\n",
      "112: [D loss: 0.588030, acc: 0.767578]  [A loss: 0.643904, acc: 0.683594]\n",
      "113: [D loss: 0.723613, acc: 0.500000]  [A loss: 1.477968, acc: 0.000000]\n",
      "114: [D loss: 0.562566, acc: 0.851562]  [A loss: 0.783513, acc: 0.250000]\n",
      "115: [D loss: 0.672627, acc: 0.503906]  [A loss: 1.795476, acc: 0.000000]\n",
      "116: [D loss: 0.589080, acc: 0.775391]  [A loss: 0.613770, acc: 0.707031]\n",
      "117: [D loss: 0.782219, acc: 0.500000]  [A loss: 1.542559, acc: 0.000000]\n",
      "118: [D loss: 0.620925, acc: 0.712891]  [A loss: 0.714007, acc: 0.457031]\n",
      "119: [D loss: 0.720028, acc: 0.501953]  [A loss: 1.369441, acc: 0.000000]\n",
      "120: [D loss: 0.577464, acc: 0.720703]  [A loss: 1.005514, acc: 0.023438]\n",
      "121: [D loss: 0.644558, acc: 0.525391]  [A loss: 1.568258, acc: 0.000000]\n",
      "122: [D loss: 0.578663, acc: 0.812500]  [A loss: 0.664621, acc: 0.593750]\n",
      "123: [D loss: 0.740520, acc: 0.500000]  [A loss: 1.678029, acc: 0.000000]\n",
      "124: [D loss: 0.612538, acc: 0.705078]  [A loss: 0.609449, acc: 0.773438]\n",
      "125: [D loss: 0.746683, acc: 0.500000]  [A loss: 1.403068, acc: 0.000000]\n",
      "126: [D loss: 0.586583, acc: 0.808594]  [A loss: 0.810729, acc: 0.222656]\n",
      "127: [D loss: 0.667366, acc: 0.513672]  [A loss: 1.522806, acc: 0.000000]\n",
      "128: [D loss: 0.584603, acc: 0.779297]  [A loss: 0.663443, acc: 0.589844]\n",
      "129: [D loss: 0.696617, acc: 0.500000]  [A loss: 1.485533, acc: 0.000000]\n",
      "130: [D loss: 0.584237, acc: 0.818359]  [A loss: 0.713717, acc: 0.445312]\n",
      "131: [D loss: 0.705361, acc: 0.501953]  [A loss: 1.547747, acc: 0.000000]\n",
      "132: [D loss: 0.603723, acc: 0.777344]  [A loss: 0.687106, acc: 0.574219]\n",
      "133: [D loss: 0.702651, acc: 0.500000]  [A loss: 1.378150, acc: 0.000000]\n",
      "134: [D loss: 0.578093, acc: 0.777344]  [A loss: 0.861629, acc: 0.132812]\n",
      "135: [D loss: 0.661847, acc: 0.513672]  [A loss: 1.520317, acc: 0.000000]\n",
      "136: [D loss: 0.599724, acc: 0.751953]  [A loss: 0.648342, acc: 0.632812]\n",
      "137: [D loss: 0.738876, acc: 0.500000]  [A loss: 1.381976, acc: 0.000000]\n",
      "138: [D loss: 0.594024, acc: 0.705078]  [A loss: 0.917955, acc: 0.066406]\n",
      "139: [D loss: 0.643244, acc: 0.535156]  [A loss: 1.360584, acc: 0.000000]\n",
      "140: [D loss: 0.605781, acc: 0.765625]  [A loss: 0.787093, acc: 0.218750]\n",
      "141: [D loss: 0.689013, acc: 0.505859]  [A loss: 1.736392, acc: 0.000000]\n",
      "142: [D loss: 0.610991, acc: 0.695312]  [A loss: 0.570193, acc: 0.839844]\n",
      "143: [D loss: 0.749249, acc: 0.498047]  [A loss: 1.296175, acc: 0.000000]\n",
      "144: [D loss: 0.631270, acc: 0.726562]  [A loss: 0.864561, acc: 0.125000]\n",
      "145: [D loss: 0.646404, acc: 0.509766]  [A loss: 1.315774, acc: 0.003906]\n",
      "146: [D loss: 0.650899, acc: 0.601562]  [A loss: 1.257100, acc: 0.000000]\n",
      "147: [D loss: 0.619367, acc: 0.734375]  [A loss: 0.797573, acc: 0.222656]\n",
      "148: [D loss: 0.676770, acc: 0.503906]  [A loss: 1.403669, acc: 0.000000]\n",
      "149: [D loss: 0.623472, acc: 0.619141]  [A loss: 1.151357, acc: 0.000000]\n",
      "150: [D loss: 0.600424, acc: 0.695312]  [A loss: 0.960130, acc: 0.046875]\n",
      "151: [D loss: 0.645205, acc: 0.539062]  [A loss: 1.668920, acc: 0.000000]\n",
      "152: [D loss: 0.595562, acc: 0.720703]  [A loss: 0.594134, acc: 0.761719]\n",
      "153: [D loss: 0.761538, acc: 0.500000]  [A loss: 1.672136, acc: 0.000000]\n",
      "154: [D loss: 0.621057, acc: 0.632812]  [A loss: 0.629145, acc: 0.683594]\n",
      "155: [D loss: 0.742323, acc: 0.501953]  [A loss: 1.365876, acc: 0.000000]\n",
      "156: [D loss: 0.597429, acc: 0.722656]  [A loss: 0.842719, acc: 0.191406]\n",
      "157: [D loss: 0.646176, acc: 0.527344]  [A loss: 1.450185, acc: 0.000000]\n",
      "158: [D loss: 0.629793, acc: 0.667969]  [A loss: 0.718529, acc: 0.464844]\n",
      "159: [D loss: 0.686012, acc: 0.507812]  [A loss: 1.454984, acc: 0.000000]\n",
      "160: [D loss: 0.597650, acc: 0.746094]  [A loss: 0.769873, acc: 0.324219]\n",
      "161: [D loss: 0.679944, acc: 0.515625]  [A loss: 1.384667, acc: 0.000000]\n",
      "162: [D loss: 0.643525, acc: 0.654297]  [A loss: 0.831717, acc: 0.164062]\n",
      "163: [D loss: 0.662902, acc: 0.517578]  [A loss: 1.329418, acc: 0.003906]\n",
      "164: [D loss: 0.689227, acc: 0.560547]  [A loss: 1.210294, acc: 0.000000]\n",
      "165: [D loss: 0.657517, acc: 0.587891]  [A loss: 0.881962, acc: 0.082031]\n",
      "166: [D loss: 0.699121, acc: 0.517578]  [A loss: 1.356583, acc: 0.000000]\n",
      "167: [D loss: 0.646824, acc: 0.607422]  [A loss: 1.357002, acc: 0.000000]\n",
      "168: [D loss: 0.601559, acc: 0.767578]  [A loss: 0.748604, acc: 0.351562]\n",
      "169: [D loss: 0.683880, acc: 0.511719]  [A loss: 1.543537, acc: 0.000000]\n",
      "170: [D loss: 0.606024, acc: 0.742188]  [A loss: 0.569867, acc: 0.828125]\n",
      "171: [D loss: 0.776095, acc: 0.501953]  [A loss: 1.654854, acc: 0.000000]\n",
      "172: [D loss: 0.658171, acc: 0.562500]  [A loss: 0.609361, acc: 0.734375]\n",
      "173: [D loss: 0.738651, acc: 0.500000]  [A loss: 1.170699, acc: 0.000000]\n",
      "174: [D loss: 0.611603, acc: 0.695312]  [A loss: 0.930931, acc: 0.054688]\n",
      "175: [D loss: 0.640666, acc: 0.570312]  [A loss: 1.173747, acc: 0.007812]\n",
      "176: [D loss: 0.615211, acc: 0.718750]  [A loss: 0.808405, acc: 0.218750]\n",
      "177: [D loss: 0.681790, acc: 0.521484]  [A loss: 1.377775, acc: 0.000000]\n",
      "178: [D loss: 0.608900, acc: 0.761719]  [A loss: 0.722876, acc: 0.453125]\n",
      "179: [D loss: 0.713451, acc: 0.492188]  [A loss: 1.515283, acc: 0.000000]\n",
      "180: [D loss: 0.638862, acc: 0.654297]  [A loss: 0.679496, acc: 0.597656]\n",
      "181: [D loss: 0.708070, acc: 0.501953]  [A loss: 1.402977, acc: 0.000000]\n",
      "182: [D loss: 0.646813, acc: 0.646484]  [A loss: 0.706411, acc: 0.496094]\n",
      "183: [D loss: 0.715693, acc: 0.501953]  [A loss: 1.356300, acc: 0.007812]\n",
      "184: [D loss: 0.675806, acc: 0.564453]  [A loss: 1.135835, acc: 0.000000]\n",
      "185: [D loss: 0.656254, acc: 0.626953]  [A loss: 0.842754, acc: 0.144531]\n",
      "186: [D loss: 0.676160, acc: 0.515625]  [A loss: 1.362373, acc: 0.000000]\n",
      "187: [D loss: 0.648269, acc: 0.640625]  [A loss: 0.936429, acc: 0.054688]\n",
      "188: [D loss: 0.657121, acc: 0.554688]  [A loss: 1.220423, acc: 0.003906]\n",
      "189: [D loss: 0.628119, acc: 0.669922]  [A loss: 0.889252, acc: 0.105469]\n",
      "190: [D loss: 0.691375, acc: 0.531250]  [A loss: 1.650125, acc: 0.000000]\n",
      "191: [D loss: 0.655341, acc: 0.572266]  [A loss: 0.517595, acc: 0.859375]\n",
      "192: [D loss: 0.824272, acc: 0.498047]  [A loss: 1.387645, acc: 0.000000]\n",
      "193: [D loss: 0.644794, acc: 0.652344]  [A loss: 0.671232, acc: 0.554688]\n",
      "194: [D loss: 0.731177, acc: 0.500000]  [A loss: 1.408465, acc: 0.000000]\n",
      "195: [D loss: 0.674820, acc: 0.570312]  [A loss: 0.732668, acc: 0.402344]\n",
      "196: [D loss: 0.728161, acc: 0.484375]  [A loss: 1.153646, acc: 0.054688]\n",
      "197: [D loss: 0.714396, acc: 0.546875]  [A loss: 1.203993, acc: 0.000000]\n",
      "198: [D loss: 0.643901, acc: 0.677734]  [A loss: 0.721944, acc: 0.417969]\n",
      "199: [D loss: 0.706973, acc: 0.501953]  [A loss: 1.225480, acc: 0.000000]\n",
      "200: [D loss: 0.641619, acc: 0.652344]  [A loss: 0.759919, acc: 0.335938]\n",
      "201: [D loss: 0.699534, acc: 0.517578]  [A loss: 1.324104, acc: 0.000000]\n",
      "202: [D loss: 0.643933, acc: 0.652344]  [A loss: 0.670373, acc: 0.613281]\n",
      "203: [D loss: 0.746722, acc: 0.500000]  [A loss: 1.289296, acc: 0.000000]\n",
      "204: [D loss: 0.660191, acc: 0.578125]  [A loss: 0.857985, acc: 0.128906]\n",
      "205: [D loss: 0.671068, acc: 0.537109]  [A loss: 1.112805, acc: 0.000000]\n",
      "206: [D loss: 0.647623, acc: 0.640625]  [A loss: 0.925195, acc: 0.062500]\n",
      "207: [D loss: 0.661917, acc: 0.574219]  [A loss: 1.196450, acc: 0.003906]\n",
      "208: [D loss: 0.670594, acc: 0.599609]  [A loss: 0.777817, acc: 0.269531]\n",
      "209: [D loss: 0.682924, acc: 0.513672]  [A loss: 1.570641, acc: 0.000000]\n",
      "210: [D loss: 0.650179, acc: 0.632812]  [A loss: 0.576819, acc: 0.839844]\n",
      "211: [D loss: 0.790327, acc: 0.500000]  [A loss: 1.388599, acc: 0.000000]\n",
      "212: [D loss: 0.660021, acc: 0.589844]  [A loss: 0.605842, acc: 0.757812]\n",
      "213: [D loss: 0.732811, acc: 0.498047]  [A loss: 1.295562, acc: 0.000000]\n",
      "214: [D loss: 0.647028, acc: 0.634766]  [A loss: 0.645734, acc: 0.648438]\n",
      "215: [D loss: 0.719258, acc: 0.496094]  [A loss: 1.222660, acc: 0.000000]\n",
      "216: [D loss: 0.668231, acc: 0.601562]  [A loss: 0.746212, acc: 0.320312]\n",
      "217: [D loss: 0.689876, acc: 0.521484]  [A loss: 1.132382, acc: 0.003906]\n",
      "218: [D loss: 0.664832, acc: 0.634766]  [A loss: 0.836892, acc: 0.171875]\n",
      "219: [D loss: 0.690433, acc: 0.515625]  [A loss: 1.072819, acc: 0.007812]\n",
      "220: [D loss: 0.642431, acc: 0.660156]  [A loss: 0.890461, acc: 0.062500]\n",
      "221: [D loss: 0.657460, acc: 0.566406]  [A loss: 1.142000, acc: 0.000000]\n",
      "222: [D loss: 0.651610, acc: 0.646484]  [A loss: 0.806833, acc: 0.164062]\n",
      "223: [D loss: 0.680251, acc: 0.519531]  [A loss: 1.421382, acc: 0.000000]\n",
      "224: [D loss: 0.649181, acc: 0.667969]  [A loss: 0.580620, acc: 0.808594]\n",
      "225: [D loss: 0.770959, acc: 0.501953]  [A loss: 1.412454, acc: 0.000000]\n",
      "226: [D loss: 0.661595, acc: 0.595703]  [A loss: 0.592838, acc: 0.824219]\n",
      "227: [D loss: 0.741958, acc: 0.494141]  [A loss: 1.329406, acc: 0.000000]\n",
      "228: [D loss: 0.658599, acc: 0.617188]  [A loss: 0.640357, acc: 0.710938]\n",
      "229: [D loss: 0.717380, acc: 0.501953]  [A loss: 1.149227, acc: 0.007812]\n",
      "230: [D loss: 0.652614, acc: 0.644531]  [A loss: 0.767929, acc: 0.332031]\n",
      "231: [D loss: 0.684264, acc: 0.544922]  [A loss: 1.093390, acc: 0.011719]\n",
      "232: [D loss: 0.665435, acc: 0.599609]  [A loss: 0.886480, acc: 0.078125]\n",
      "233: [D loss: 0.677332, acc: 0.537109]  [A loss: 1.094389, acc: 0.003906]\n",
      "234: [D loss: 0.653730, acc: 0.617188]  [A loss: 0.818639, acc: 0.187500]\n",
      "235: [D loss: 0.683800, acc: 0.519531]  [A loss: 1.301825, acc: 0.000000]\n",
      "236: [D loss: 0.661919, acc: 0.607422]  [A loss: 0.703375, acc: 0.500000]\n",
      "237: [D loss: 0.734189, acc: 0.494141]  [A loss: 1.301851, acc: 0.003906]\n",
      "238: [D loss: 0.665692, acc: 0.599609]  [A loss: 0.776170, acc: 0.261719]\n",
      "239: [D loss: 0.685831, acc: 0.525391]  [A loss: 1.308089, acc: 0.000000]\n",
      "240: [D loss: 0.677581, acc: 0.562500]  [A loss: 0.667383, acc: 0.601562]\n",
      "241: [D loss: 0.719461, acc: 0.501953]  [A loss: 1.297718, acc: 0.000000]\n",
      "242: [D loss: 0.679800, acc: 0.578125]  [A loss: 0.697201, acc: 0.519531]\n",
      "243: [D loss: 0.699250, acc: 0.505859]  [A loss: 1.128041, acc: 0.000000]\n",
      "244: [D loss: 0.672260, acc: 0.611328]  [A loss: 0.696773, acc: 0.484375]\n",
      "245: [D loss: 0.722576, acc: 0.501953]  [A loss: 1.227650, acc: 0.000000]\n",
      "246: [D loss: 0.694725, acc: 0.546875]  [A loss: 0.713353, acc: 0.421875]\n",
      "247: [D loss: 0.696962, acc: 0.501953]  [A loss: 1.098748, acc: 0.007812]\n",
      "248: [D loss: 0.672008, acc: 0.593750]  [A loss: 0.848059, acc: 0.117188]\n",
      "249: [D loss: 0.674438, acc: 0.535156]  [A loss: 0.924586, acc: 0.035156]\n",
      "250: [D loss: 0.669779, acc: 0.572266]  [A loss: 0.962526, acc: 0.011719]\n",
      "251: [D loss: 0.672949, acc: 0.570312]  [A loss: 1.000520, acc: 0.003906]\n",
      "252: [D loss: 0.655151, acc: 0.632812]  [A loss: 0.865149, acc: 0.117188]\n",
      "253: [D loss: 0.677822, acc: 0.542969]  [A loss: 1.092125, acc: 0.000000]\n",
      "254: [D loss: 0.672871, acc: 0.560547]  [A loss: 0.956660, acc: 0.039062]\n",
      "255: [D loss: 0.685306, acc: 0.517578]  [A loss: 0.995395, acc: 0.003906]\n",
      "256: [D loss: 0.677230, acc: 0.546875]  [A loss: 1.009410, acc: 0.027344]\n",
      "257: [D loss: 0.681675, acc: 0.550781]  [A loss: 1.211635, acc: 0.000000]\n",
      "258: [D loss: 0.688866, acc: 0.550781]  [A loss: 0.679231, acc: 0.582031]\n",
      "259: [D loss: 0.725963, acc: 0.503906]  [A loss: 1.439663, acc: 0.000000]\n",
      "260: [D loss: 0.678026, acc: 0.554688]  [A loss: 0.567374, acc: 0.894531]\n",
      "261: [D loss: 0.746304, acc: 0.500000]  [A loss: 1.142597, acc: 0.000000]\n",
      "262: [D loss: 0.697769, acc: 0.521484]  [A loss: 0.740285, acc: 0.343750]\n",
      "263: [D loss: 0.692917, acc: 0.503906]  [A loss: 0.966127, acc: 0.039062]\n",
      "264: [D loss: 0.677054, acc: 0.570312]  [A loss: 0.962046, acc: 0.000000]\n",
      "265: [D loss: 0.662782, acc: 0.636719]  [A loss: 0.778595, acc: 0.242188]\n",
      "266: [D loss: 0.687938, acc: 0.519531]  [A loss: 0.963094, acc: 0.031250]\n",
      "267: [D loss: 0.675788, acc: 0.539062]  [A loss: 0.948664, acc: 0.027344]\n",
      "268: [D loss: 0.667832, acc: 0.601562]  [A loss: 0.836058, acc: 0.078125]\n",
      "269: [D loss: 0.675384, acc: 0.546875]  [A loss: 1.047634, acc: 0.015625]\n",
      "270: [D loss: 0.663726, acc: 0.597656]  [A loss: 0.829333, acc: 0.160156]\n",
      "271: [D loss: 0.692138, acc: 0.521484]  [A loss: 1.048304, acc: 0.011719]\n",
      "272: [D loss: 0.667981, acc: 0.585938]  [A loss: 0.976892, acc: 0.027344]\n",
      "273: [D loss: 0.658407, acc: 0.630859]  [A loss: 0.961530, acc: 0.035156]\n",
      "274: [D loss: 0.681033, acc: 0.572266]  [A loss: 1.139667, acc: 0.000000]\n",
      "275: [D loss: 0.707143, acc: 0.496094]  [A loss: 0.836937, acc: 0.144531]\n",
      "276: [D loss: 0.680344, acc: 0.517578]  [A loss: 1.249594, acc: 0.000000]\n",
      "277: [D loss: 0.687462, acc: 0.513672]  [A loss: 0.721255, acc: 0.488281]\n",
      "278: [D loss: 0.717353, acc: 0.503906]  [A loss: 1.207904, acc: 0.000000]\n",
      "279: [D loss: 0.683447, acc: 0.580078]  [A loss: 0.616390, acc: 0.710938]\n",
      "280: [D loss: 0.729754, acc: 0.500000]  [A loss: 1.341265, acc: 0.000000]\n",
      "281: [D loss: 0.692205, acc: 0.525391]  [A loss: 0.569694, acc: 0.902344]\n",
      "282: [D loss: 0.740734, acc: 0.498047]  [A loss: 0.953266, acc: 0.027344]\n",
      "283: [D loss: 0.668005, acc: 0.589844]  [A loss: 0.775262, acc: 0.199219]\n",
      "284: [D loss: 0.679725, acc: 0.546875]  [A loss: 0.891193, acc: 0.019531]\n",
      "285: [D loss: 0.658099, acc: 0.632812]  [A loss: 0.805617, acc: 0.179688]\n",
      "286: [D loss: 0.672704, acc: 0.550781]  [A loss: 0.929872, acc: 0.027344]\n",
      "287: [D loss: 0.674860, acc: 0.576172]  [A loss: 0.763632, acc: 0.328125]\n",
      "288: [D loss: 0.689052, acc: 0.550781]  [A loss: 1.030001, acc: 0.003906]\n",
      "289: [D loss: 0.666965, acc: 0.621094]  [A loss: 0.758572, acc: 0.300781]\n",
      "290: [D loss: 0.703573, acc: 0.515625]  [A loss: 1.092527, acc: 0.000000]\n",
      "291: [D loss: 0.673137, acc: 0.607422]  [A loss: 0.680837, acc: 0.558594]\n",
      "292: [D loss: 0.703088, acc: 0.498047]  [A loss: 1.139041, acc: 0.007812]\n",
      "293: [D loss: 0.666044, acc: 0.617188]  [A loss: 0.658349, acc: 0.675781]\n",
      "294: [D loss: 0.713809, acc: 0.511719]  [A loss: 1.098106, acc: 0.000000]\n",
      "295: [D loss: 0.664628, acc: 0.623047]  [A loss: 0.688522, acc: 0.480469]\n",
      "296: [D loss: 0.748994, acc: 0.507812]  [A loss: 1.117584, acc: 0.000000]\n",
      "297: [D loss: 0.726027, acc: 0.416016]  [A loss: 0.911491, acc: 0.035156]\n",
      "298: [D loss: 0.679937, acc: 0.564453]  [A loss: 0.879515, acc: 0.085938]\n",
      "299: [D loss: 0.676681, acc: 0.550781]  [A loss: 0.865407, acc: 0.097656]\n",
      "300: [D loss: 0.692162, acc: 0.537109]  [A loss: 0.847458, acc: 0.109375]\n",
      "301: [D loss: 0.676206, acc: 0.574219]  [A loss: 0.875731, acc: 0.113281]\n",
      "302: [D loss: 0.681434, acc: 0.578125]  [A loss: 0.901798, acc: 0.042969]\n",
      "303: [D loss: 0.674729, acc: 0.609375]  [A loss: 0.849984, acc: 0.093750]\n",
      "304: [D loss: 0.679957, acc: 0.542969]  [A loss: 0.964562, acc: 0.031250]\n",
      "305: [D loss: 0.674797, acc: 0.587891]  [A loss: 0.888984, acc: 0.039062]\n",
      "306: [D loss: 0.676824, acc: 0.560547]  [A loss: 0.925045, acc: 0.070312]\n",
      "307: [D loss: 0.673693, acc: 0.560547]  [A loss: 0.907286, acc: 0.054688]\n",
      "308: [D loss: 0.682733, acc: 0.552734]  [A loss: 0.953507, acc: 0.066406]\n",
      "309: [D loss: 0.676376, acc: 0.587891]  [A loss: 0.948598, acc: 0.042969]\n",
      "310: [D loss: 0.677187, acc: 0.568359]  [A loss: 0.941832, acc: 0.027344]\n",
      "311: [D loss: 0.667175, acc: 0.615234]  [A loss: 0.859229, acc: 0.089844]\n",
      "312: [D loss: 0.695094, acc: 0.535156]  [A loss: 1.161583, acc: 0.023438]\n",
      "313: [D loss: 0.684279, acc: 0.548828]  [A loss: 0.783306, acc: 0.285156]\n",
      "314: [D loss: 0.700363, acc: 0.507812]  [A loss: 1.181220, acc: 0.000000]\n",
      "315: [D loss: 0.659000, acc: 0.617188]  [A loss: 0.586981, acc: 0.812500]\n",
      "316: [D loss: 0.754117, acc: 0.500000]  [A loss: 1.367832, acc: 0.000000]\n",
      "317: [D loss: 0.694569, acc: 0.511719]  [A loss: 0.585497, acc: 0.878906]\n",
      "318: [D loss: 0.753322, acc: 0.500000]  [A loss: 0.970531, acc: 0.023438]\n",
      "319: [D loss: 0.672974, acc: 0.578125]  [A loss: 0.769374, acc: 0.300781]\n",
      "320: [D loss: 0.699763, acc: 0.496094]  [A loss: 0.930168, acc: 0.039062]\n",
      "321: [D loss: 0.670596, acc: 0.587891]  [A loss: 0.755758, acc: 0.312500]\n",
      "322: [D loss: 0.698147, acc: 0.523438]  [A loss: 0.925997, acc: 0.003906]\n",
      "323: [D loss: 0.678804, acc: 0.554688]  [A loss: 0.817687, acc: 0.136719]\n",
      "324: [D loss: 0.676674, acc: 0.568359]  [A loss: 0.897168, acc: 0.042969]\n",
      "325: [D loss: 0.667976, acc: 0.603516]  [A loss: 0.838761, acc: 0.109375]\n",
      "326: [D loss: 0.676274, acc: 0.552734]  [A loss: 0.946781, acc: 0.027344]\n",
      "327: [D loss: 0.678183, acc: 0.583984]  [A loss: 0.927560, acc: 0.058594]\n",
      "328: [D loss: 0.686484, acc: 0.562500]  [A loss: 0.858034, acc: 0.136719]\n",
      "329: [D loss: 0.688556, acc: 0.572266]  [A loss: 0.981582, acc: 0.007812]\n",
      "330: [D loss: 0.682588, acc: 0.548828]  [A loss: 0.799605, acc: 0.183594]\n",
      "331: [D loss: 0.688319, acc: 0.529297]  [A loss: 1.010031, acc: 0.007812]\n",
      "332: [D loss: 0.676767, acc: 0.587891]  [A loss: 0.759953, acc: 0.308594]\n",
      "333: [D loss: 0.706017, acc: 0.507812]  [A loss: 1.122565, acc: 0.015625]\n",
      "334: [D loss: 0.688589, acc: 0.556641]  [A loss: 0.720182, acc: 0.429688]\n",
      "335: [D loss: 0.707343, acc: 0.505859]  [A loss: 1.098090, acc: 0.003906]\n",
      "336: [D loss: 0.674795, acc: 0.582031]  [A loss: 0.642916, acc: 0.703125]\n",
      "337: [D loss: 0.711132, acc: 0.501953]  [A loss: 1.154642, acc: 0.000000]\n",
      "338: [D loss: 0.691913, acc: 0.535156]  [A loss: 0.716817, acc: 0.429688]\n",
      "339: [D loss: 0.694271, acc: 0.521484]  [A loss: 0.989021, acc: 0.011719]\n",
      "340: [D loss: 0.669249, acc: 0.597656]  [A loss: 0.726486, acc: 0.375000]\n",
      "341: [D loss: 0.713900, acc: 0.511719]  [A loss: 1.047008, acc: 0.000000]\n",
      "342: [D loss: 0.675428, acc: 0.603516]  [A loss: 0.719353, acc: 0.429688]\n",
      "343: [D loss: 0.704247, acc: 0.511719]  [A loss: 0.991072, acc: 0.023438]\n",
      "344: [D loss: 0.682324, acc: 0.535156]  [A loss: 0.776360, acc: 0.230469]\n",
      "345: [D loss: 0.684864, acc: 0.525391]  [A loss: 0.923837, acc: 0.019531]\n",
      "346: [D loss: 0.686189, acc: 0.544922]  [A loss: 0.776856, acc: 0.253906]\n",
      "347: [D loss: 0.696685, acc: 0.525391]  [A loss: 1.033890, acc: 0.000000]\n",
      "348: [D loss: 0.674642, acc: 0.603516]  [A loss: 0.769060, acc: 0.265625]\n",
      "349: [D loss: 0.691679, acc: 0.515625]  [A loss: 0.984769, acc: 0.015625]\n",
      "350: [D loss: 0.679374, acc: 0.562500]  [A loss: 0.828929, acc: 0.136719]\n",
      "351: [D loss: 0.676422, acc: 0.550781]  [A loss: 0.925991, acc: 0.031250]\n",
      "352: [D loss: 0.668692, acc: 0.595703]  [A loss: 0.830147, acc: 0.117188]\n",
      "353: [D loss: 0.681708, acc: 0.539062]  [A loss: 0.980525, acc: 0.019531]\n",
      "354: [D loss: 0.673073, acc: 0.609375]  [A loss: 0.762478, acc: 0.281250]\n",
      "355: [D loss: 0.692029, acc: 0.529297]  [A loss: 1.106118, acc: 0.007812]\n",
      "356: [D loss: 0.665907, acc: 0.593750]  [A loss: 0.623094, acc: 0.714844]\n",
      "357: [D loss: 0.726520, acc: 0.503906]  [A loss: 1.186912, acc: 0.000000]\n",
      "358: [D loss: 0.682707, acc: 0.556641]  [A loss: 0.633472, acc: 0.695312]\n",
      "359: [D loss: 0.732362, acc: 0.501953]  [A loss: 1.011986, acc: 0.050781]\n",
      "360: [D loss: 0.680672, acc: 0.566406]  [A loss: 0.713014, acc: 0.414062]\n",
      "361: [D loss: 0.701317, acc: 0.519531]  [A loss: 0.937557, acc: 0.027344]\n",
      "362: [D loss: 0.675484, acc: 0.576172]  [A loss: 0.734047, acc: 0.367188]\n",
      "363: [D loss: 0.691052, acc: 0.531250]  [A loss: 0.966644, acc: 0.019531]\n",
      "364: [D loss: 0.672163, acc: 0.597656]  [A loss: 0.761917, acc: 0.234375]\n",
      "365: [D loss: 0.688850, acc: 0.509766]  [A loss: 0.921912, acc: 0.054688]\n",
      "366: [D loss: 0.678005, acc: 0.556641]  [A loss: 0.855656, acc: 0.082031]\n",
      "367: [D loss: 0.678673, acc: 0.544922]  [A loss: 0.828918, acc: 0.109375]\n",
      "368: [D loss: 0.678618, acc: 0.564453]  [A loss: 0.869806, acc: 0.093750]\n",
      "369: [D loss: 0.666784, acc: 0.599609]  [A loss: 0.855956, acc: 0.082031]\n",
      "370: [D loss: 0.671344, acc: 0.580078]  [A loss: 0.919559, acc: 0.035156]\n",
      "371: [D loss: 0.677569, acc: 0.583984]  [A loss: 0.874826, acc: 0.093750]\n",
      "372: [D loss: 0.671881, acc: 0.609375]  [A loss: 0.932260, acc: 0.027344]\n",
      "373: [D loss: 0.667319, acc: 0.615234]  [A loss: 0.782489, acc: 0.250000]\n",
      "374: [D loss: 0.690112, acc: 0.537109]  [A loss: 1.066292, acc: 0.003906]\n",
      "375: [D loss: 0.670666, acc: 0.605469]  [A loss: 0.641404, acc: 0.691406]\n",
      "376: [D loss: 0.721124, acc: 0.503906]  [A loss: 1.204812, acc: 0.000000]\n",
      "377: [D loss: 0.687018, acc: 0.527344]  [A loss: 0.629576, acc: 0.718750]\n",
      "378: [D loss: 0.735863, acc: 0.500000]  [A loss: 1.020321, acc: 0.011719]\n",
      "379: [D loss: 0.670955, acc: 0.589844]  [A loss: 0.697365, acc: 0.480469]\n",
      "380: [D loss: 0.698020, acc: 0.507812]  [A loss: 0.951564, acc: 0.011719]\n",
      "381: [D loss: 0.679404, acc: 0.560547]  [A loss: 0.753702, acc: 0.300781]\n",
      "382: [D loss: 0.682683, acc: 0.548828]  [A loss: 0.941111, acc: 0.007812]\n",
      "383: [D loss: 0.663998, acc: 0.625000]  [A loss: 0.721012, acc: 0.421875]\n",
      "384: [D loss: 0.689075, acc: 0.527344]  [A loss: 0.980839, acc: 0.027344]\n",
      "385: [D loss: 0.677019, acc: 0.572266]  [A loss: 0.775014, acc: 0.328125]\n",
      "386: [D loss: 0.703139, acc: 0.529297]  [A loss: 0.973300, acc: 0.035156]\n",
      "387: [D loss: 0.688066, acc: 0.576172]  [A loss: 0.842127, acc: 0.191406]\n",
      "388: [D loss: 0.699084, acc: 0.515625]  [A loss: 0.912490, acc: 0.054688]\n",
      "389: [D loss: 0.697172, acc: 0.537109]  [A loss: 0.929969, acc: 0.035156]\n",
      "390: [D loss: 0.665543, acc: 0.613281]  [A loss: 0.743923, acc: 0.371094]\n",
      "391: [D loss: 0.686568, acc: 0.537109]  [A loss: 0.940040, acc: 0.042969]\n",
      "392: [D loss: 0.682284, acc: 0.560547]  [A loss: 0.798285, acc: 0.195312]\n",
      "393: [D loss: 0.684970, acc: 0.546875]  [A loss: 0.919113, acc: 0.042969]\n",
      "394: [D loss: 0.670816, acc: 0.638672]  [A loss: 0.793187, acc: 0.195312]\n",
      "395: [D loss: 0.688869, acc: 0.529297]  [A loss: 0.951932, acc: 0.023438]\n",
      "396: [D loss: 0.664757, acc: 0.611328]  [A loss: 0.741446, acc: 0.363281]\n",
      "397: [D loss: 0.684643, acc: 0.509766]  [A loss: 1.066452, acc: 0.003906]\n",
      "398: [D loss: 0.667800, acc: 0.591797]  [A loss: 0.682510, acc: 0.593750]\n",
      "399: [D loss: 0.733146, acc: 0.498047]  [A loss: 1.054312, acc: 0.007812]\n",
      "400: [D loss: 0.685632, acc: 0.539062]  [A loss: 0.719484, acc: 0.394531]\n",
      "401: [D loss: 0.694352, acc: 0.523438]  [A loss: 1.044586, acc: 0.003906]\n",
      "402: [D loss: 0.682584, acc: 0.562500]  [A loss: 0.681338, acc: 0.539062]\n",
      "403: [D loss: 0.702972, acc: 0.505859]  [A loss: 0.977744, acc: 0.011719]\n",
      "404: [D loss: 0.666937, acc: 0.587891]  [A loss: 0.754078, acc: 0.324219]\n",
      "405: [D loss: 0.692179, acc: 0.521484]  [A loss: 0.942157, acc: 0.031250]\n",
      "406: [D loss: 0.664022, acc: 0.605469]  [A loss: 0.774336, acc: 0.234375]\n",
      "407: [D loss: 0.677964, acc: 0.564453]  [A loss: 0.948314, acc: 0.039062]\n",
      "408: [D loss: 0.676530, acc: 0.574219]  [A loss: 0.757078, acc: 0.281250]\n",
      "409: [D loss: 0.704475, acc: 0.531250]  [A loss: 0.998256, acc: 0.031250]\n",
      "410: [D loss: 0.710510, acc: 0.494141]  [A loss: 0.815463, acc: 0.160156]\n",
      "411: [D loss: 0.679277, acc: 0.537109]  [A loss: 0.926673, acc: 0.046875]\n",
      "412: [D loss: 0.673332, acc: 0.587891]  [A loss: 0.801283, acc: 0.187500]\n",
      "413: [D loss: 0.683687, acc: 0.542969]  [A loss: 0.934509, acc: 0.050781]\n",
      "414: [D loss: 0.670004, acc: 0.595703]  [A loss: 0.755691, acc: 0.316406]\n",
      "415: [D loss: 0.698066, acc: 0.533203]  [A loss: 1.021430, acc: 0.011719]\n",
      "416: [D loss: 0.676619, acc: 0.566406]  [A loss: 0.716439, acc: 0.421875]\n",
      "417: [D loss: 0.693828, acc: 0.519531]  [A loss: 1.110976, acc: 0.003906]\n",
      "418: [D loss: 0.674800, acc: 0.558594]  [A loss: 0.666905, acc: 0.617188]\n",
      "419: [D loss: 0.706110, acc: 0.509766]  [A loss: 1.036290, acc: 0.007812]\n",
      "420: [D loss: 0.670083, acc: 0.574219]  [A loss: 0.701007, acc: 0.484375]\n",
      "421: [D loss: 0.703040, acc: 0.521484]  [A loss: 1.030843, acc: 0.003906]\n",
      "422: [D loss: 0.673453, acc: 0.597656]  [A loss: 0.759883, acc: 0.277344]\n",
      "423: [D loss: 0.685126, acc: 0.527344]  [A loss: 0.949089, acc: 0.023438]\n",
      "424: [D loss: 0.671369, acc: 0.591797]  [A loss: 0.816634, acc: 0.164062]\n",
      "425: [D loss: 0.676970, acc: 0.576172]  [A loss: 0.920218, acc: 0.066406]\n",
      "426: [D loss: 0.666107, acc: 0.613281]  [A loss: 0.793365, acc: 0.242188]\n",
      "427: [D loss: 0.695647, acc: 0.503906]  [A loss: 0.937791, acc: 0.039062]\n",
      "428: [D loss: 0.676088, acc: 0.582031]  [A loss: 0.821070, acc: 0.179688]\n",
      "429: [D loss: 0.671149, acc: 0.568359]  [A loss: 0.894743, acc: 0.089844]\n",
      "430: [D loss: 0.665334, acc: 0.611328]  [A loss: 0.873562, acc: 0.117188]\n",
      "431: [D loss: 0.676080, acc: 0.572266]  [A loss: 0.892855, acc: 0.070312]\n",
      "432: [D loss: 0.680741, acc: 0.578125]  [A loss: 0.878165, acc: 0.113281]\n",
      "433: [D loss: 0.679102, acc: 0.562500]  [A loss: 0.903206, acc: 0.050781]\n",
      "434: [D loss: 0.671724, acc: 0.589844]  [A loss: 0.841467, acc: 0.183594]\n",
      "435: [D loss: 0.686952, acc: 0.546875]  [A loss: 0.990111, acc: 0.011719]\n",
      "436: [D loss: 0.670526, acc: 0.576172]  [A loss: 0.820850, acc: 0.156250]\n",
      "437: [D loss: 0.671291, acc: 0.580078]  [A loss: 1.014989, acc: 0.007812]\n",
      "438: [D loss: 0.677822, acc: 0.562500]  [A loss: 0.709183, acc: 0.476562]\n",
      "439: [D loss: 0.717888, acc: 0.523438]  [A loss: 1.229599, acc: 0.003906]\n",
      "440: [D loss: 0.685452, acc: 0.544922]  [A loss: 0.565058, acc: 0.882812]\n",
      "441: [D loss: 0.730985, acc: 0.500000]  [A loss: 0.992884, acc: 0.000000]\n",
      "442: [D loss: 0.675486, acc: 0.580078]  [A loss: 0.738281, acc: 0.386719]\n",
      "443: [D loss: 0.696414, acc: 0.523438]  [A loss: 0.910513, acc: 0.039062]\n",
      "444: [D loss: 0.671060, acc: 0.599609]  [A loss: 0.712682, acc: 0.441406]\n",
      "445: [D loss: 0.688465, acc: 0.523438]  [A loss: 0.905088, acc: 0.070312]\n",
      "446: [D loss: 0.668884, acc: 0.607422]  [A loss: 0.756081, acc: 0.343750]\n",
      "447: [D loss: 0.683197, acc: 0.539062]  [A loss: 0.961851, acc: 0.031250]\n",
      "448: [D loss: 0.672266, acc: 0.580078]  [A loss: 0.724413, acc: 0.398438]\n",
      "449: [D loss: 0.691918, acc: 0.515625]  [A loss: 0.956916, acc: 0.054688]\n",
      "450: [D loss: 0.696104, acc: 0.505859]  [A loss: 0.784608, acc: 0.273438]\n",
      "451: [D loss: 0.689592, acc: 0.544922]  [A loss: 0.899975, acc: 0.066406]\n",
      "452: [D loss: 0.673347, acc: 0.593750]  [A loss: 0.839997, acc: 0.128906]\n",
      "453: [D loss: 0.675619, acc: 0.566406]  [A loss: 0.909301, acc: 0.093750]\n",
      "454: [D loss: 0.672606, acc: 0.576172]  [A loss: 0.861760, acc: 0.113281]\n",
      "455: [D loss: 0.674943, acc: 0.589844]  [A loss: 0.861046, acc: 0.089844]\n",
      "456: [D loss: 0.668574, acc: 0.585938]  [A loss: 0.886134, acc: 0.105469]\n",
      "457: [D loss: 0.671796, acc: 0.585938]  [A loss: 0.890970, acc: 0.082031]\n",
      "458: [D loss: 0.668879, acc: 0.578125]  [A loss: 0.832644, acc: 0.183594]\n",
      "459: [D loss: 0.673226, acc: 0.548828]  [A loss: 1.014233, acc: 0.007812]\n",
      "460: [D loss: 0.669532, acc: 0.595703]  [A loss: 0.727696, acc: 0.398438]\n",
      "461: [D loss: 0.702464, acc: 0.525391]  [A loss: 1.147740, acc: 0.007812]\n",
      "462: [D loss: 0.682784, acc: 0.566406]  [A loss: 0.646179, acc: 0.660156]\n",
      "463: [D loss: 0.706362, acc: 0.507812]  [A loss: 0.993590, acc: 0.027344]\n",
      "464: [D loss: 0.680964, acc: 0.542969]  [A loss: 0.756835, acc: 0.308594]\n",
      "465: [D loss: 0.696535, acc: 0.519531]  [A loss: 0.953573, acc: 0.011719]\n",
      "466: [D loss: 0.682085, acc: 0.576172]  [A loss: 0.797098, acc: 0.226562]\n",
      "467: [D loss: 0.676319, acc: 0.568359]  [A loss: 0.869281, acc: 0.148438]\n",
      "468: [D loss: 0.665187, acc: 0.611328]  [A loss: 0.844898, acc: 0.128906]\n",
      "469: [D loss: 0.667919, acc: 0.595703]  [A loss: 0.897131, acc: 0.097656]\n",
      "470: [D loss: 0.669829, acc: 0.574219]  [A loss: 0.869511, acc: 0.144531]\n",
      "471: [D loss: 0.674459, acc: 0.568359]  [A loss: 0.891630, acc: 0.101562]\n",
      "472: [D loss: 0.676609, acc: 0.582031]  [A loss: 0.892840, acc: 0.109375]\n",
      "473: [D loss: 0.684552, acc: 0.560547]  [A loss: 0.850586, acc: 0.121094]\n",
      "474: [D loss: 0.667004, acc: 0.595703]  [A loss: 0.984843, acc: 0.035156]\n",
      "475: [D loss: 0.683429, acc: 0.576172]  [A loss: 0.823530, acc: 0.164062]\n",
      "476: [D loss: 0.674352, acc: 0.578125]  [A loss: 0.876501, acc: 0.117188]\n",
      "477: [D loss: 0.665084, acc: 0.615234]  [A loss: 0.907484, acc: 0.093750]\n",
      "478: [D loss: 0.686037, acc: 0.541016]  [A loss: 0.871139, acc: 0.093750]\n",
      "479: [D loss: 0.679474, acc: 0.582031]  [A loss: 0.981128, acc: 0.035156]\n",
      "480: [D loss: 0.678883, acc: 0.558594]  [A loss: 0.721307, acc: 0.453125]\n",
      "481: [D loss: 0.693079, acc: 0.527344]  [A loss: 1.143677, acc: 0.007812]\n",
      "482: [D loss: 0.675528, acc: 0.599609]  [A loss: 0.610036, acc: 0.710938]\n",
      "483: [D loss: 0.743940, acc: 0.515625]  [A loss: 1.045052, acc: 0.007812]\n",
      "484: [D loss: 0.695590, acc: 0.521484]  [A loss: 0.732247, acc: 0.339844]\n",
      "485: [D loss: 0.696542, acc: 0.523438]  [A loss: 0.918235, acc: 0.089844]\n",
      "486: [D loss: 0.685963, acc: 0.525391]  [A loss: 0.776850, acc: 0.222656]\n",
      "487: [D loss: 0.666185, acc: 0.568359]  [A loss: 0.828976, acc: 0.183594]\n",
      "488: [D loss: 0.687039, acc: 0.525391]  [A loss: 0.929124, acc: 0.042969]\n",
      "489: [D loss: 0.676427, acc: 0.576172]  [A loss: 0.812956, acc: 0.203125]\n",
      "490: [D loss: 0.684959, acc: 0.548828]  [A loss: 0.960437, acc: 0.054688]\n",
      "491: [D loss: 0.675346, acc: 0.578125]  [A loss: 0.753772, acc: 0.328125]\n",
      "492: [D loss: 0.684834, acc: 0.552734]  [A loss: 0.894515, acc: 0.062500]\n",
      "493: [D loss: 0.678341, acc: 0.578125]  [A loss: 0.796942, acc: 0.222656]\n",
      "494: [D loss: 0.677659, acc: 0.541016]  [A loss: 0.836612, acc: 0.175781]\n",
      "495: [D loss: 0.660600, acc: 0.628906]  [A loss: 0.901543, acc: 0.085938]\n",
      "496: [D loss: 0.674999, acc: 0.603516]  [A loss: 0.840922, acc: 0.144531]\n",
      "497: [D loss: 0.684653, acc: 0.570312]  [A loss: 0.971302, acc: 0.019531]\n",
      "498: [D loss: 0.679139, acc: 0.591797]  [A loss: 0.754420, acc: 0.324219]\n",
      "499: [D loss: 0.682881, acc: 0.548828]  [A loss: 0.985453, acc: 0.019531]\n",
      "500: [D loss: 0.677616, acc: 0.568359]  [A loss: 0.748209, acc: 0.386719]\n",
      "501: [D loss: 0.682211, acc: 0.552734]  [A loss: 0.947894, acc: 0.058594]\n",
      "502: [D loss: 0.666340, acc: 0.591797]  [A loss: 0.776234, acc: 0.269531]\n",
      "503: [D loss: 0.681414, acc: 0.525391]  [A loss: 1.024671, acc: 0.007812]\n",
      "504: [D loss: 0.667959, acc: 0.583984]  [A loss: 0.696359, acc: 0.523438]\n",
      "505: [D loss: 0.703570, acc: 0.523438]  [A loss: 1.058069, acc: 0.007812]\n",
      "506: [D loss: 0.667569, acc: 0.605469]  [A loss: 0.704249, acc: 0.476562]\n",
      "507: [D loss: 0.683820, acc: 0.531250]  [A loss: 0.969433, acc: 0.039062]\n",
      "508: [D loss: 0.679945, acc: 0.542969]  [A loss: 0.764975, acc: 0.308594]\n",
      "509: [D loss: 0.682132, acc: 0.527344]  [A loss: 0.913948, acc: 0.093750]\n",
      "510: [D loss: 0.680306, acc: 0.576172]  [A loss: 0.837451, acc: 0.144531]\n",
      "511: [D loss: 0.691745, acc: 0.542969]  [A loss: 0.878904, acc: 0.070312]\n",
      "512: [D loss: 0.668836, acc: 0.583984]  [A loss: 0.812316, acc: 0.160156]\n",
      "513: [D loss: 0.676782, acc: 0.562500]  [A loss: 0.926253, acc: 0.054688]\n",
      "514: [D loss: 0.666266, acc: 0.578125]  [A loss: 0.785574, acc: 0.238281]\n",
      "515: [D loss: 0.699926, acc: 0.525391]  [A loss: 1.021802, acc: 0.003906]\n",
      "516: [D loss: 0.681584, acc: 0.574219]  [A loss: 0.707661, acc: 0.500000]\n",
      "517: [D loss: 0.693792, acc: 0.529297]  [A loss: 0.964984, acc: 0.046875]\n",
      "518: [D loss: 0.658495, acc: 0.611328]  [A loss: 0.819750, acc: 0.183594]\n",
      "519: [D loss: 0.684033, acc: 0.582031]  [A loss: 0.945840, acc: 0.031250]\n",
      "520: [D loss: 0.680763, acc: 0.548828]  [A loss: 0.786417, acc: 0.234375]\n",
      "521: [D loss: 0.681272, acc: 0.539062]  [A loss: 0.965523, acc: 0.031250]\n",
      "522: [D loss: 0.654108, acc: 0.646484]  [A loss: 0.777969, acc: 0.300781]\n",
      "523: [D loss: 0.684505, acc: 0.535156]  [A loss: 0.979127, acc: 0.023438]\n",
      "524: [D loss: 0.671046, acc: 0.593750]  [A loss: 0.730709, acc: 0.410156]\n",
      "525: [D loss: 0.685486, acc: 0.533203]  [A loss: 0.996403, acc: 0.050781]\n",
      "526: [D loss: 0.651920, acc: 0.617188]  [A loss: 0.750610, acc: 0.375000]\n",
      "527: [D loss: 0.705061, acc: 0.527344]  [A loss: 1.089800, acc: 0.003906]\n",
      "528: [D loss: 0.688480, acc: 0.535156]  [A loss: 0.699430, acc: 0.496094]\n",
      "529: [D loss: 0.700699, acc: 0.511719]  [A loss: 0.954902, acc: 0.019531]\n",
      "530: [D loss: 0.666047, acc: 0.617188]  [A loss: 0.776730, acc: 0.285156]\n",
      "531: [D loss: 0.682280, acc: 0.546875]  [A loss: 0.931955, acc: 0.058594]\n",
      "532: [D loss: 0.678842, acc: 0.552734]  [A loss: 0.835257, acc: 0.175781]\n",
      "533: [D loss: 0.673198, acc: 0.578125]  [A loss: 0.860634, acc: 0.109375]\n",
      "534: [D loss: 0.664634, acc: 0.593750]  [A loss: 0.892895, acc: 0.117188]\n",
      "535: [D loss: 0.658951, acc: 0.630859]  [A loss: 0.891252, acc: 0.109375]\n",
      "536: [D loss: 0.666320, acc: 0.597656]  [A loss: 0.846553, acc: 0.167969]\n",
      "537: [D loss: 0.666713, acc: 0.568359]  [A loss: 0.910416, acc: 0.089844]\n",
      "538: [D loss: 0.674847, acc: 0.576172]  [A loss: 0.865779, acc: 0.136719]\n",
      "539: [D loss: 0.669170, acc: 0.570312]  [A loss: 0.893769, acc: 0.113281]\n",
      "540: [D loss: 0.663866, acc: 0.597656]  [A loss: 0.933825, acc: 0.082031]\n",
      "541: [D loss: 0.679172, acc: 0.580078]  [A loss: 0.904141, acc: 0.085938]\n",
      "542: [D loss: 0.665921, acc: 0.582031]  [A loss: 0.858903, acc: 0.164062]\n",
      "543: [D loss: 0.676345, acc: 0.580078]  [A loss: 0.927751, acc: 0.089844]\n",
      "544: [D loss: 0.666343, acc: 0.593750]  [A loss: 0.882505, acc: 0.109375]\n",
      "545: [D loss: 0.673740, acc: 0.570312]  [A loss: 0.945987, acc: 0.054688]\n",
      "546: [D loss: 0.671241, acc: 0.574219]  [A loss: 0.895860, acc: 0.109375]\n",
      "547: [D loss: 0.682854, acc: 0.564453]  [A loss: 0.895865, acc: 0.066406]\n",
      "548: [D loss: 0.668555, acc: 0.589844]  [A loss: 0.867500, acc: 0.125000]\n",
      "549: [D loss: 0.663842, acc: 0.570312]  [A loss: 0.887825, acc: 0.128906]\n",
      "550: [D loss: 0.685856, acc: 0.572266]  [A loss: 0.967475, acc: 0.035156]\n",
      "551: [D loss: 0.672223, acc: 0.583984]  [A loss: 0.838489, acc: 0.187500]\n",
      "552: [D loss: 0.679427, acc: 0.564453]  [A loss: 1.040965, acc: 0.015625]\n",
      "553: [D loss: 0.667180, acc: 0.609375]  [A loss: 0.742910, acc: 0.375000]\n",
      "554: [D loss: 0.748518, acc: 0.531250]  [A loss: 0.993177, acc: 0.031250]\n",
      "555: [D loss: 0.708547, acc: 0.486328]  [A loss: 0.932025, acc: 0.046875]\n",
      "556: [D loss: 0.710577, acc: 0.490234]  [A loss: 0.770369, acc: 0.253906]\n",
      "557: [D loss: 0.702251, acc: 0.513672]  [A loss: 0.940244, acc: 0.042969]\n",
      "558: [D loss: 0.674898, acc: 0.564453]  [A loss: 0.747854, acc: 0.324219]\n",
      "559: [D loss: 0.693276, acc: 0.513672]  [A loss: 1.003736, acc: 0.027344]\n",
      "560: [D loss: 0.657468, acc: 0.617188]  [A loss: 0.792309, acc: 0.285156]\n",
      "561: [D loss: 0.698028, acc: 0.539062]  [A loss: 1.100754, acc: 0.023438]\n",
      "562: [D loss: 0.682262, acc: 0.568359]  [A loss: 0.737067, acc: 0.398438]\n",
      "563: [D loss: 0.692975, acc: 0.544922]  [A loss: 1.050729, acc: 0.023438]\n",
      "564: [D loss: 0.676107, acc: 0.572266]  [A loss: 0.773828, acc: 0.265625]\n",
      "565: [D loss: 0.684513, acc: 0.537109]  [A loss: 0.956106, acc: 0.042969]\n",
      "566: [D loss: 0.677425, acc: 0.550781]  [A loss: 0.813880, acc: 0.207031]\n",
      "567: [D loss: 0.678281, acc: 0.544922]  [A loss: 0.893486, acc: 0.085938]\n",
      "568: [D loss: 0.675621, acc: 0.570312]  [A loss: 0.849454, acc: 0.179688]\n",
      "569: [D loss: 0.683601, acc: 0.566406]  [A loss: 1.012754, acc: 0.046875]\n",
      "570: [D loss: 0.664521, acc: 0.599609]  [A loss: 0.793931, acc: 0.234375]\n",
      "571: [D loss: 0.692113, acc: 0.527344]  [A loss: 0.971643, acc: 0.011719]\n",
      "572: [D loss: 0.666135, acc: 0.599609]  [A loss: 0.753250, acc: 0.320312]\n",
      "573: [D loss: 0.679657, acc: 0.550781]  [A loss: 1.017758, acc: 0.066406]\n",
      "574: [D loss: 0.658811, acc: 0.623047]  [A loss: 0.789772, acc: 0.257812]\n",
      "575: [D loss: 0.711233, acc: 0.527344]  [A loss: 1.083441, acc: 0.003906]\n",
      "576: [D loss: 0.678238, acc: 0.582031]  [A loss: 0.743187, acc: 0.359375]\n",
      "577: [D loss: 0.695311, acc: 0.531250]  [A loss: 1.015681, acc: 0.023438]\n",
      "578: [D loss: 0.678466, acc: 0.562500]  [A loss: 0.750456, acc: 0.359375]\n",
      "579: [D loss: 0.682268, acc: 0.529297]  [A loss: 0.974583, acc: 0.027344]\n",
      "580: [D loss: 0.672674, acc: 0.583984]  [A loss: 0.824475, acc: 0.226562]\n",
      "581: [D loss: 0.681391, acc: 0.558594]  [A loss: 0.939145, acc: 0.082031]\n",
      "582: [D loss: 0.669076, acc: 0.572266]  [A loss: 0.856383, acc: 0.140625]\n",
      "583: [D loss: 0.682129, acc: 0.554688]  [A loss: 0.948331, acc: 0.039062]\n",
      "584: [D loss: 0.656451, acc: 0.599609]  [A loss: 0.813684, acc: 0.210938]\n",
      "585: [D loss: 0.676029, acc: 0.546875]  [A loss: 0.959126, acc: 0.042969]\n",
      "586: [D loss: 0.669160, acc: 0.582031]  [A loss: 0.755945, acc: 0.367188]\n",
      "587: [D loss: 0.686366, acc: 0.572266]  [A loss: 1.014568, acc: 0.039062]\n",
      "588: [D loss: 0.670653, acc: 0.619141]  [A loss: 0.773774, acc: 0.277344]\n",
      "589: [D loss: 0.665562, acc: 0.576172]  [A loss: 1.005690, acc: 0.078125]\n",
      "590: [D loss: 0.673814, acc: 0.582031]  [A loss: 0.849379, acc: 0.156250]\n",
      "591: [D loss: 0.685498, acc: 0.542969]  [A loss: 1.001914, acc: 0.042969]\n",
      "592: [D loss: 0.673636, acc: 0.585938]  [A loss: 0.741196, acc: 0.382812]\n",
      "593: [D loss: 0.688107, acc: 0.533203]  [A loss: 0.987984, acc: 0.031250]\n",
      "594: [D loss: 0.667807, acc: 0.611328]  [A loss: 0.759573, acc: 0.347656]\n",
      "595: [D loss: 0.701897, acc: 0.537109]  [A loss: 0.997531, acc: 0.027344]\n",
      "596: [D loss: 0.683336, acc: 0.548828]  [A loss: 0.828567, acc: 0.199219]\n",
      "597: [D loss: 0.683894, acc: 0.541016]  [A loss: 0.883540, acc: 0.097656]\n",
      "598: [D loss: 0.665177, acc: 0.613281]  [A loss: 0.871309, acc: 0.167969]\n",
      "599: [D loss: 0.665532, acc: 0.580078]  [A loss: 0.912292, acc: 0.050781]\n",
      "600: [D loss: 0.674119, acc: 0.572266]  [A loss: 0.848261, acc: 0.152344]\n",
      "601: [D loss: 0.672770, acc: 0.578125]  [A loss: 0.986421, acc: 0.031250]\n",
      "602: [D loss: 0.670981, acc: 0.609375]  [A loss: 0.750856, acc: 0.382812]\n",
      "603: [D loss: 0.688809, acc: 0.523438]  [A loss: 1.003736, acc: 0.085938]\n",
      "604: [D loss: 0.686457, acc: 0.542969]  [A loss: 0.873006, acc: 0.109375]\n",
      "605: [D loss: 0.689009, acc: 0.550781]  [A loss: 0.935904, acc: 0.058594]\n",
      "606: [D loss: 0.654225, acc: 0.634766]  [A loss: 0.771838, acc: 0.304688]\n",
      "607: [D loss: 0.685609, acc: 0.542969]  [A loss: 1.015266, acc: 0.023438]\n",
      "608: [D loss: 0.671383, acc: 0.578125]  [A loss: 0.748582, acc: 0.382812]\n",
      "609: [D loss: 0.706866, acc: 0.513672]  [A loss: 1.034583, acc: 0.023438]\n",
      "610: [D loss: 0.682665, acc: 0.554688]  [A loss: 0.726876, acc: 0.406250]\n",
      "611: [D loss: 0.694234, acc: 0.513672]  [A loss: 0.935789, acc: 0.046875]\n",
      "612: [D loss: 0.665093, acc: 0.599609]  [A loss: 0.764576, acc: 0.347656]\n",
      "613: [D loss: 0.688075, acc: 0.554688]  [A loss: 1.015260, acc: 0.015625]\n",
      "614: [D loss: 0.689890, acc: 0.517578]  [A loss: 0.780275, acc: 0.285156]\n",
      "615: [D loss: 0.691298, acc: 0.523438]  [A loss: 0.954003, acc: 0.042969]\n",
      "616: [D loss: 0.659593, acc: 0.628906]  [A loss: 0.785101, acc: 0.265625]\n",
      "617: [D loss: 0.699750, acc: 0.523438]  [A loss: 0.948659, acc: 0.019531]\n",
      "618: [D loss: 0.674537, acc: 0.574219]  [A loss: 0.800188, acc: 0.222656]\n",
      "619: [D loss: 0.692159, acc: 0.541016]  [A loss: 0.924016, acc: 0.039062]\n",
      "620: [D loss: 0.674526, acc: 0.568359]  [A loss: 0.816226, acc: 0.167969]\n",
      "621: [D loss: 0.664244, acc: 0.595703]  [A loss: 0.920748, acc: 0.105469]\n",
      "622: [D loss: 0.657705, acc: 0.638672]  [A loss: 0.845937, acc: 0.148438]\n",
      "623: [D loss: 0.681725, acc: 0.542969]  [A loss: 0.900526, acc: 0.089844]\n",
      "624: [D loss: 0.675319, acc: 0.582031]  [A loss: 0.859201, acc: 0.156250]\n",
      "625: [D loss: 0.659967, acc: 0.619141]  [A loss: 0.876889, acc: 0.144531]\n",
      "626: [D loss: 0.693076, acc: 0.560547]  [A loss: 1.005944, acc: 0.031250]\n",
      "627: [D loss: 0.693197, acc: 0.529297]  [A loss: 0.819920, acc: 0.214844]\n",
      "628: [D loss: 0.695056, acc: 0.527344]  [A loss: 1.011167, acc: 0.042969]\n",
      "629: [D loss: 0.668445, acc: 0.605469]  [A loss: 0.758332, acc: 0.343750]\n",
      "630: [D loss: 0.691626, acc: 0.529297]  [A loss: 1.077414, acc: 0.011719]\n",
      "631: [D loss: 0.663590, acc: 0.613281]  [A loss: 0.728094, acc: 0.429688]\n",
      "632: [D loss: 0.707541, acc: 0.523438]  [A loss: 1.072168, acc: 0.015625]\n",
      "633: [D loss: 0.673486, acc: 0.578125]  [A loss: 0.683151, acc: 0.550781]\n",
      "634: [D loss: 0.698705, acc: 0.531250]  [A loss: 0.963050, acc: 0.035156]\n",
      "635: [D loss: 0.670681, acc: 0.595703]  [A loss: 0.770711, acc: 0.343750]\n",
      "636: [D loss: 0.691171, acc: 0.548828]  [A loss: 0.930013, acc: 0.082031]\n",
      "637: [D loss: 0.662919, acc: 0.601562]  [A loss: 0.844723, acc: 0.191406]\n",
      "638: [D loss: 0.682345, acc: 0.544922]  [A loss: 0.935726, acc: 0.050781]\n",
      "639: [D loss: 0.675445, acc: 0.601562]  [A loss: 0.780866, acc: 0.285156]\n",
      "640: [D loss: 0.668718, acc: 0.589844]  [A loss: 0.878656, acc: 0.140625]\n",
      "641: [D loss: 0.706859, acc: 0.513672]  [A loss: 0.859662, acc: 0.164062]\n",
      "642: [D loss: 0.684396, acc: 0.554688]  [A loss: 0.925107, acc: 0.066406]\n",
      "643: [D loss: 0.668893, acc: 0.599609]  [A loss: 0.820907, acc: 0.191406]\n",
      "644: [D loss: 0.687312, acc: 0.535156]  [A loss: 0.960770, acc: 0.074219]\n",
      "645: [D loss: 0.660159, acc: 0.599609]  [A loss: 0.845898, acc: 0.191406]\n",
      "646: [D loss: 0.692798, acc: 0.513672]  [A loss: 0.951650, acc: 0.105469]\n",
      "647: [D loss: 0.678283, acc: 0.546875]  [A loss: 0.780579, acc: 0.308594]\n",
      "648: [D loss: 0.700384, acc: 0.548828]  [A loss: 0.952039, acc: 0.058594]\n",
      "649: [D loss: 0.683094, acc: 0.550781]  [A loss: 0.785324, acc: 0.230469]\n",
      "650: [D loss: 0.688461, acc: 0.560547]  [A loss: 0.936612, acc: 0.062500]\n",
      "651: [D loss: 0.664701, acc: 0.593750]  [A loss: 0.818051, acc: 0.210938]\n",
      "652: [D loss: 0.702644, acc: 0.546875]  [A loss: 0.986456, acc: 0.031250]\n",
      "653: [D loss: 0.680892, acc: 0.564453]  [A loss: 0.763896, acc: 0.312500]\n",
      "654: [D loss: 0.688119, acc: 0.541016]  [A loss: 0.929724, acc: 0.050781]\n",
      "655: [D loss: 0.670318, acc: 0.568359]  [A loss: 0.843505, acc: 0.203125]\n",
      "656: [D loss: 0.672373, acc: 0.560547]  [A loss: 0.980543, acc: 0.066406]\n",
      "657: [D loss: 0.667474, acc: 0.580078]  [A loss: 0.765427, acc: 0.382812]\n",
      "658: [D loss: 0.681668, acc: 0.548828]  [A loss: 1.028499, acc: 0.039062]\n",
      "659: [D loss: 0.672600, acc: 0.585938]  [A loss: 0.767991, acc: 0.308594]\n",
      "660: [D loss: 0.686053, acc: 0.562500]  [A loss: 0.944084, acc: 0.054688]\n",
      "661: [D loss: 0.669457, acc: 0.593750]  [A loss: 0.769760, acc: 0.304688]\n",
      "662: [D loss: 0.701669, acc: 0.529297]  [A loss: 0.980199, acc: 0.050781]\n",
      "663: [D loss: 0.674785, acc: 0.574219]  [A loss: 0.746789, acc: 0.347656]\n",
      "664: [D loss: 0.681940, acc: 0.546875]  [A loss: 0.952191, acc: 0.031250]\n",
      "665: [D loss: 0.669321, acc: 0.603516]  [A loss: 0.835469, acc: 0.207031]\n",
      "666: [D loss: 0.683607, acc: 0.576172]  [A loss: 0.955589, acc: 0.062500]\n",
      "667: [D loss: 0.680133, acc: 0.544922]  [A loss: 0.813370, acc: 0.187500]\n",
      "668: [D loss: 0.678706, acc: 0.546875]  [A loss: 1.014115, acc: 0.039062]\n",
      "669: [D loss: 0.673832, acc: 0.578125]  [A loss: 0.805844, acc: 0.214844]\n",
      "670: [D loss: 0.674492, acc: 0.566406]  [A loss: 0.949341, acc: 0.054688]\n",
      "671: [D loss: 0.660726, acc: 0.613281]  [A loss: 0.857475, acc: 0.160156]\n",
      "672: [D loss: 0.680043, acc: 0.556641]  [A loss: 0.980791, acc: 0.066406]\n",
      "673: [D loss: 0.655268, acc: 0.609375]  [A loss: 0.819753, acc: 0.238281]\n",
      "674: [D loss: 0.705525, acc: 0.531250]  [A loss: 0.871623, acc: 0.109375]\n",
      "675: [D loss: 0.697168, acc: 0.498047]  [A loss: 0.972947, acc: 0.031250]\n",
      "676: [D loss: 0.659430, acc: 0.619141]  [A loss: 0.753385, acc: 0.386719]\n",
      "677: [D loss: 0.700649, acc: 0.525391]  [A loss: 1.023860, acc: 0.066406]\n",
      "678: [D loss: 0.689711, acc: 0.548828]  [A loss: 0.883271, acc: 0.136719]\n",
      "679: [D loss: 0.681827, acc: 0.568359]  [A loss: 0.885654, acc: 0.132812]\n",
      "680: [D loss: 0.673710, acc: 0.574219]  [A loss: 0.923338, acc: 0.097656]\n",
      "681: [D loss: 0.671987, acc: 0.591797]  [A loss: 0.813874, acc: 0.207031]\n",
      "682: [D loss: 0.678523, acc: 0.552734]  [A loss: 0.920288, acc: 0.074219]\n",
      "683: [D loss: 0.685138, acc: 0.533203]  [A loss: 0.849029, acc: 0.144531]\n",
      "684: [D loss: 0.673852, acc: 0.591797]  [A loss: 0.913254, acc: 0.132812]\n",
      "685: [D loss: 0.674224, acc: 0.578125]  [A loss: 0.836374, acc: 0.187500]\n",
      "686: [D loss: 0.673637, acc: 0.558594]  [A loss: 0.970560, acc: 0.074219]\n",
      "687: [D loss: 0.661179, acc: 0.617188]  [A loss: 0.844500, acc: 0.164062]\n",
      "688: [D loss: 0.687009, acc: 0.574219]  [A loss: 1.003986, acc: 0.042969]\n",
      "689: [D loss: 0.694102, acc: 0.556641]  [A loss: 0.769629, acc: 0.296875]\n",
      "690: [D loss: 0.692714, acc: 0.548828]  [A loss: 1.056601, acc: 0.062500]\n",
      "691: [D loss: 0.684931, acc: 0.550781]  [A loss: 0.807510, acc: 0.269531]\n",
      "692: [D loss: 0.680311, acc: 0.552734]  [A loss: 0.993659, acc: 0.078125]\n",
      "693: [D loss: 0.670364, acc: 0.589844]  [A loss: 0.780304, acc: 0.312500]\n",
      "694: [D loss: 0.676447, acc: 0.568359]  [A loss: 0.975240, acc: 0.066406]\n",
      "695: [D loss: 0.655106, acc: 0.613281]  [A loss: 0.804063, acc: 0.265625]\n",
      "696: [D loss: 0.684257, acc: 0.580078]  [A loss: 1.037834, acc: 0.015625]\n",
      "697: [D loss: 0.680695, acc: 0.552734]  [A loss: 0.745597, acc: 0.386719]\n",
      "698: [D loss: 0.704000, acc: 0.542969]  [A loss: 1.010471, acc: 0.035156]\n",
      "699: [D loss: 0.683809, acc: 0.560547]  [A loss: 0.781811, acc: 0.296875]\n",
      "700: [D loss: 0.692895, acc: 0.541016]  [A loss: 1.005538, acc: 0.058594]\n",
      "701: [D loss: 0.675879, acc: 0.574219]  [A loss: 0.762462, acc: 0.359375]\n",
      "702: [D loss: 0.683579, acc: 0.562500]  [A loss: 0.963532, acc: 0.054688]\n",
      "703: [D loss: 0.671958, acc: 0.609375]  [A loss: 0.858631, acc: 0.171875]\n",
      "704: [D loss: 0.694528, acc: 0.529297]  [A loss: 0.932435, acc: 0.070312]\n",
      "705: [D loss: 0.676145, acc: 0.572266]  [A loss: 0.824262, acc: 0.195312]\n",
      "706: [D loss: 0.677553, acc: 0.574219]  [A loss: 0.890158, acc: 0.109375]\n",
      "707: [D loss: 0.672501, acc: 0.593750]  [A loss: 0.834161, acc: 0.187500]\n",
      "708: [D loss: 0.667309, acc: 0.599609]  [A loss: 0.867502, acc: 0.199219]\n",
      "709: [D loss: 0.694973, acc: 0.539062]  [A loss: 0.911546, acc: 0.101562]\n",
      "710: [D loss: 0.676504, acc: 0.578125]  [A loss: 0.893118, acc: 0.140625]\n",
      "711: [D loss: 0.683181, acc: 0.566406]  [A loss: 0.864469, acc: 0.125000]\n",
      "712: [D loss: 0.681499, acc: 0.550781]  [A loss: 1.020164, acc: 0.050781]\n",
      "713: [D loss: 0.667830, acc: 0.589844]  [A loss: 0.748119, acc: 0.386719]\n",
      "714: [D loss: 0.698525, acc: 0.533203]  [A loss: 1.059710, acc: 0.019531]\n",
      "715: [D loss: 0.671252, acc: 0.566406]  [A loss: 0.712106, acc: 0.480469]\n",
      "716: [D loss: 0.722620, acc: 0.521484]  [A loss: 1.020490, acc: 0.054688]\n",
      "717: [D loss: 0.696133, acc: 0.505859]  [A loss: 0.871252, acc: 0.136719]\n",
      "718: [D loss: 0.698451, acc: 0.570312]  [A loss: 0.940162, acc: 0.058594]\n",
      "719: [D loss: 0.688114, acc: 0.531250]  [A loss: 0.814446, acc: 0.179688]\n",
      "720: [D loss: 0.688115, acc: 0.525391]  [A loss: 0.932372, acc: 0.062500]\n",
      "721: [D loss: 0.679033, acc: 0.556641]  [A loss: 0.902621, acc: 0.093750]\n",
      "722: [D loss: 0.686723, acc: 0.542969]  [A loss: 0.890891, acc: 0.085938]\n",
      "723: [D loss: 0.682697, acc: 0.562500]  [A loss: 0.893312, acc: 0.121094]\n",
      "724: [D loss: 0.673100, acc: 0.583984]  [A loss: 0.835666, acc: 0.187500]\n",
      "725: [D loss: 0.681714, acc: 0.537109]  [A loss: 0.955549, acc: 0.054688]\n",
      "726: [D loss: 0.671102, acc: 0.583984]  [A loss: 0.848772, acc: 0.175781]\n",
      "727: [D loss: 0.691602, acc: 0.562500]  [A loss: 0.986566, acc: 0.066406]\n",
      "728: [D loss: 0.676519, acc: 0.564453]  [A loss: 0.760476, acc: 0.324219]\n",
      "729: [D loss: 0.681474, acc: 0.556641]  [A loss: 1.060233, acc: 0.031250]\n",
      "730: [D loss: 0.675804, acc: 0.587891]  [A loss: 0.698478, acc: 0.480469]\n",
      "731: [D loss: 0.716610, acc: 0.523438]  [A loss: 1.042352, acc: 0.019531]\n",
      "732: [D loss: 0.690041, acc: 0.519531]  [A loss: 0.785103, acc: 0.277344]\n",
      "733: [D loss: 0.705278, acc: 0.521484]  [A loss: 0.967900, acc: 0.054688]\n",
      "734: [D loss: 0.680051, acc: 0.550781]  [A loss: 0.797932, acc: 0.269531]\n",
      "735: [D loss: 0.687850, acc: 0.519531]  [A loss: 0.914945, acc: 0.085938]\n",
      "736: [D loss: 0.678270, acc: 0.568359]  [A loss: 0.807838, acc: 0.191406]\n",
      "737: [D loss: 0.686415, acc: 0.556641]  [A loss: 0.892059, acc: 0.140625]\n",
      "738: [D loss: 0.675497, acc: 0.560547]  [A loss: 0.812551, acc: 0.250000]\n",
      "739: [D loss: 0.680017, acc: 0.550781]  [A loss: 0.913442, acc: 0.113281]\n",
      "740: [D loss: 0.680427, acc: 0.564453]  [A loss: 0.871426, acc: 0.136719]\n",
      "741: [D loss: 0.687997, acc: 0.554688]  [A loss: 0.799250, acc: 0.234375]\n",
      "742: [D loss: 0.679036, acc: 0.546875]  [A loss: 0.910241, acc: 0.125000]\n",
      "743: [D loss: 0.665332, acc: 0.597656]  [A loss: 0.786751, acc: 0.277344]\n",
      "744: [D loss: 0.705302, acc: 0.501953]  [A loss: 0.992930, acc: 0.046875]\n",
      "745: [D loss: 0.681548, acc: 0.576172]  [A loss: 0.841068, acc: 0.164062]\n",
      "746: [D loss: 0.689203, acc: 0.544922]  [A loss: 1.003761, acc: 0.011719]\n",
      "747: [D loss: 0.672520, acc: 0.589844]  [A loss: 0.721319, acc: 0.429688]\n",
      "748: [D loss: 0.698453, acc: 0.525391]  [A loss: 1.084344, acc: 0.031250]\n",
      "749: [D loss: 0.668456, acc: 0.583984]  [A loss: 0.695292, acc: 0.539062]\n",
      "750: [D loss: 0.714043, acc: 0.517578]  [A loss: 1.026703, acc: 0.050781]\n",
      "751: [D loss: 0.679746, acc: 0.560547]  [A loss: 0.698195, acc: 0.507812]\n",
      "752: [D loss: 0.707690, acc: 0.533203]  [A loss: 1.008546, acc: 0.070312]\n",
      "753: [D loss: 0.681186, acc: 0.552734]  [A loss: 0.756949, acc: 0.355469]\n",
      "754: [D loss: 0.700435, acc: 0.533203]  [A loss: 0.872799, acc: 0.093750]\n",
      "755: [D loss: 0.683792, acc: 0.537109]  [A loss: 0.839591, acc: 0.175781]\n",
      "756: [D loss: 0.689933, acc: 0.523438]  [A loss: 0.871825, acc: 0.144531]\n",
      "757: [D loss: 0.667929, acc: 0.578125]  [A loss: 0.817109, acc: 0.203125]\n",
      "758: [D loss: 0.683532, acc: 0.552734]  [A loss: 0.901809, acc: 0.117188]\n",
      "759: [D loss: 0.670684, acc: 0.583984]  [A loss: 0.827837, acc: 0.164062]\n",
      "760: [D loss: 0.682179, acc: 0.560547]  [A loss: 0.887475, acc: 0.117188]\n",
      "761: [D loss: 0.673033, acc: 0.578125]  [A loss: 0.863553, acc: 0.132812]\n",
      "762: [D loss: 0.672704, acc: 0.599609]  [A loss: 0.886029, acc: 0.160156]\n",
      "763: [D loss: 0.681470, acc: 0.568359]  [A loss: 0.852308, acc: 0.171875]\n",
      "764: [D loss: 0.692585, acc: 0.533203]  [A loss: 0.969874, acc: 0.070312]\n",
      "765: [D loss: 0.660658, acc: 0.611328]  [A loss: 0.785861, acc: 0.285156]\n",
      "766: [D loss: 0.702043, acc: 0.501953]  [A loss: 1.005127, acc: 0.058594]\n",
      "767: [D loss: 0.666639, acc: 0.603516]  [A loss: 0.768595, acc: 0.312500]\n",
      "768: [D loss: 0.681292, acc: 0.560547]  [A loss: 0.927916, acc: 0.109375]\n",
      "769: [D loss: 0.689911, acc: 0.550781]  [A loss: 0.822545, acc: 0.210938]\n",
      "770: [D loss: 0.679784, acc: 0.544922]  [A loss: 0.980602, acc: 0.054688]\n",
      "771: [D loss: 0.682242, acc: 0.554688]  [A loss: 0.785841, acc: 0.269531]\n",
      "772: [D loss: 0.674613, acc: 0.552734]  [A loss: 1.017784, acc: 0.046875]\n",
      "773: [D loss: 0.698718, acc: 0.539062]  [A loss: 0.697539, acc: 0.562500]\n",
      "774: [D loss: 0.723989, acc: 0.513672]  [A loss: 0.990772, acc: 0.058594]\n",
      "775: [D loss: 0.669190, acc: 0.585938]  [A loss: 0.737140, acc: 0.425781]\n",
      "776: [D loss: 0.694419, acc: 0.542969]  [A loss: 0.942398, acc: 0.058594]\n",
      "777: [D loss: 0.679670, acc: 0.576172]  [A loss: 0.808115, acc: 0.207031]\n",
      "778: [D loss: 0.693032, acc: 0.537109]  [A loss: 0.924588, acc: 0.085938]\n",
      "779: [D loss: 0.663024, acc: 0.605469]  [A loss: 0.793060, acc: 0.292969]\n",
      "780: [D loss: 0.697776, acc: 0.525391]  [A loss: 0.989024, acc: 0.039062]\n",
      "781: [D loss: 0.677842, acc: 0.576172]  [A loss: 0.820737, acc: 0.234375]\n",
      "782: [D loss: 0.692852, acc: 0.558594]  [A loss: 0.918904, acc: 0.074219]\n",
      "783: [D loss: 0.680764, acc: 0.558594]  [A loss: 0.804486, acc: 0.257812]\n",
      "784: [D loss: 0.681743, acc: 0.562500]  [A loss: 0.936530, acc: 0.070312]\n",
      "785: [D loss: 0.670631, acc: 0.568359]  [A loss: 0.885119, acc: 0.132812]\n",
      "786: [D loss: 0.692007, acc: 0.527344]  [A loss: 0.880489, acc: 0.109375]\n",
      "787: [D loss: 0.663570, acc: 0.619141]  [A loss: 0.814594, acc: 0.199219]\n",
      "788: [D loss: 0.693338, acc: 0.521484]  [A loss: 0.950733, acc: 0.101562]\n",
      "789: [D loss: 0.710658, acc: 0.478516]  [A loss: 0.896406, acc: 0.140625]\n",
      "790: [D loss: 0.681454, acc: 0.552734]  [A loss: 0.906785, acc: 0.097656]\n",
      "791: [D loss: 0.673720, acc: 0.572266]  [A loss: 0.845652, acc: 0.183594]\n",
      "792: [D loss: 0.681380, acc: 0.560547]  [A loss: 0.879208, acc: 0.125000]\n",
      "793: [D loss: 0.668906, acc: 0.593750]  [A loss: 0.844452, acc: 0.167969]\n",
      "794: [D loss: 0.681508, acc: 0.546875]  [A loss: 0.918024, acc: 0.093750]\n",
      "795: [D loss: 0.673402, acc: 0.593750]  [A loss: 0.830028, acc: 0.199219]\n",
      "796: [D loss: 0.689370, acc: 0.556641]  [A loss: 0.945254, acc: 0.074219]\n",
      "797: [D loss: 0.673855, acc: 0.603516]  [A loss: 0.798038, acc: 0.261719]\n",
      "798: [D loss: 0.686251, acc: 0.546875]  [A loss: 1.049397, acc: 0.078125]\n",
      "799: [D loss: 0.679134, acc: 0.556641]  [A loss: 0.822361, acc: 0.226562]\n",
      "800: [D loss: 0.678857, acc: 0.562500]  [A loss: 1.040908, acc: 0.035156]\n",
      "801: [D loss: 0.676859, acc: 0.599609]  [A loss: 0.715284, acc: 0.511719]\n",
      "802: [D loss: 0.690014, acc: 0.548828]  [A loss: 1.054817, acc: 0.023438]\n",
      "803: [D loss: 0.670025, acc: 0.578125]  [A loss: 0.657582, acc: 0.628906]\n",
      "804: [D loss: 0.712204, acc: 0.519531]  [A loss: 1.058367, acc: 0.054688]\n",
      "805: [D loss: 0.675761, acc: 0.568359]  [A loss: 0.758916, acc: 0.343750]\n",
      "806: [D loss: 0.686220, acc: 0.554688]  [A loss: 0.979416, acc: 0.117188]\n",
      "807: [D loss: 0.675701, acc: 0.570312]  [A loss: 0.765471, acc: 0.335938]\n",
      "808: [D loss: 0.682084, acc: 0.568359]  [A loss: 0.905922, acc: 0.128906]\n",
      "809: [D loss: 0.681372, acc: 0.544922]  [A loss: 0.790857, acc: 0.273438]\n",
      "810: [D loss: 0.685838, acc: 0.564453]  [A loss: 0.948453, acc: 0.074219]\n",
      "811: [D loss: 0.673639, acc: 0.580078]  [A loss: 0.775361, acc: 0.343750]\n",
      "812: [D loss: 0.692128, acc: 0.537109]  [A loss: 0.966728, acc: 0.074219]\n",
      "813: [D loss: 0.678527, acc: 0.580078]  [A loss: 0.772314, acc: 0.285156]\n",
      "814: [D loss: 0.689426, acc: 0.541016]  [A loss: 0.906768, acc: 0.101562]\n",
      "815: [D loss: 0.699899, acc: 0.537109]  [A loss: 0.886357, acc: 0.132812]\n",
      "816: [D loss: 0.679756, acc: 0.552734]  [A loss: 0.821184, acc: 0.222656]\n",
      "817: [D loss: 0.689305, acc: 0.507812]  [A loss: 0.894940, acc: 0.109375]\n",
      "818: [D loss: 0.668251, acc: 0.593750]  [A loss: 0.845444, acc: 0.175781]\n",
      "819: [D loss: 0.701439, acc: 0.541016]  [A loss: 0.898526, acc: 0.144531]\n",
      "820: [D loss: 0.693571, acc: 0.517578]  [A loss: 0.920878, acc: 0.050781]\n",
      "821: [D loss: 0.682684, acc: 0.591797]  [A loss: 0.842672, acc: 0.179688]\n",
      "822: [D loss: 0.674515, acc: 0.580078]  [A loss: 0.872693, acc: 0.160156]\n",
      "823: [D loss: 0.675860, acc: 0.576172]  [A loss: 0.916257, acc: 0.117188]\n",
      "824: [D loss: 0.669378, acc: 0.613281]  [A loss: 0.838514, acc: 0.210938]\n",
      "825: [D loss: 0.680635, acc: 0.564453]  [A loss: 0.931031, acc: 0.093750]\n",
      "826: [D loss: 0.699462, acc: 0.521484]  [A loss: 0.865451, acc: 0.187500]\n",
      "827: [D loss: 0.693395, acc: 0.535156]  [A loss: 1.041889, acc: 0.019531]\n",
      "828: [D loss: 0.686520, acc: 0.546875]  [A loss: 0.678944, acc: 0.566406]\n",
      "829: [D loss: 0.720907, acc: 0.509766]  [A loss: 1.102577, acc: 0.000000]\n",
      "830: [D loss: 0.675307, acc: 0.568359]  [A loss: 0.726650, acc: 0.441406]\n",
      "831: [D loss: 0.707573, acc: 0.529297]  [A loss: 1.055614, acc: 0.046875]\n",
      "832: [D loss: 0.673411, acc: 0.574219]  [A loss: 0.713962, acc: 0.484375]\n",
      "833: [D loss: 0.708505, acc: 0.517578]  [A loss: 1.019207, acc: 0.027344]\n",
      "834: [D loss: 0.674655, acc: 0.554688]  [A loss: 0.690425, acc: 0.523438]\n",
      "835: [D loss: 0.718821, acc: 0.542969]  [A loss: 0.960834, acc: 0.062500]\n",
      "836: [D loss: 0.694478, acc: 0.544922]  [A loss: 0.827036, acc: 0.207031]\n",
      "837: [D loss: 0.686844, acc: 0.539062]  [A loss: 0.901429, acc: 0.089844]\n",
      "838: [D loss: 0.672604, acc: 0.591797]  [A loss: 0.806180, acc: 0.257812]\n",
      "839: [D loss: 0.672641, acc: 0.570312]  [A loss: 0.891200, acc: 0.148438]\n",
      "840: [D loss: 0.680903, acc: 0.570312]  [A loss: 0.843463, acc: 0.207031]\n",
      "841: [D loss: 0.685852, acc: 0.550781]  [A loss: 0.975357, acc: 0.042969]\n",
      "842: [D loss: 0.685719, acc: 0.560547]  [A loss: 0.812282, acc: 0.281250]\n",
      "843: [D loss: 0.695899, acc: 0.539062]  [A loss: 0.986567, acc: 0.046875]\n",
      "844: [D loss: 0.677964, acc: 0.574219]  [A loss: 0.800307, acc: 0.304688]\n",
      "845: [D loss: 0.692651, acc: 0.548828]  [A loss: 0.894813, acc: 0.093750]\n",
      "846: [D loss: 0.679451, acc: 0.564453]  [A loss: 0.853693, acc: 0.175781]\n",
      "847: [D loss: 0.682078, acc: 0.564453]  [A loss: 0.898670, acc: 0.171875]\n",
      "848: [D loss: 0.682785, acc: 0.544922]  [A loss: 0.844589, acc: 0.199219]\n",
      "849: [D loss: 0.684410, acc: 0.562500]  [A loss: 0.864818, acc: 0.195312]\n",
      "850: [D loss: 0.674416, acc: 0.576172]  [A loss: 0.898947, acc: 0.152344]\n",
      "851: [D loss: 0.687425, acc: 0.560547]  [A loss: 0.797671, acc: 0.265625]\n",
      "852: [D loss: 0.693457, acc: 0.564453]  [A loss: 1.068630, acc: 0.023438]\n",
      "853: [D loss: 0.672883, acc: 0.554688]  [A loss: 0.685783, acc: 0.519531]\n",
      "854: [D loss: 0.719758, acc: 0.509766]  [A loss: 1.107558, acc: 0.023438]\n",
      "855: [D loss: 0.680508, acc: 0.554688]  [A loss: 0.674149, acc: 0.597656]\n",
      "856: [D loss: 0.701817, acc: 0.515625]  [A loss: 1.032538, acc: 0.054688]\n",
      "857: [D loss: 0.662911, acc: 0.621094]  [A loss: 0.774407, acc: 0.328125]\n",
      "858: [D loss: 0.723051, acc: 0.513672]  [A loss: 0.872192, acc: 0.164062]\n",
      "859: [D loss: 0.692085, acc: 0.560547]  [A loss: 0.935558, acc: 0.078125]\n",
      "860: [D loss: 0.689489, acc: 0.544922]  [A loss: 0.782848, acc: 0.269531]\n",
      "861: [D loss: 0.683811, acc: 0.542969]  [A loss: 0.900700, acc: 0.109375]\n",
      "862: [D loss: 0.663354, acc: 0.626953]  [A loss: 0.796894, acc: 0.332031]\n",
      "863: [D loss: 0.692809, acc: 0.558594]  [A loss: 0.954426, acc: 0.046875]\n",
      "864: [D loss: 0.670743, acc: 0.568359]  [A loss: 0.784003, acc: 0.285156]\n",
      "865: [D loss: 0.695057, acc: 0.537109]  [A loss: 1.038561, acc: 0.054688]\n",
      "866: [D loss: 0.677139, acc: 0.558594]  [A loss: 0.742491, acc: 0.410156]\n",
      "867: [D loss: 0.695349, acc: 0.542969]  [A loss: 1.017632, acc: 0.039062]\n",
      "868: [D loss: 0.671472, acc: 0.585938]  [A loss: 0.779659, acc: 0.343750]\n",
      "869: [D loss: 0.692704, acc: 0.546875]  [A loss: 0.898607, acc: 0.128906]\n",
      "870: [D loss: 0.693952, acc: 0.537109]  [A loss: 0.869096, acc: 0.179688]\n",
      "871: [D loss: 0.684406, acc: 0.535156]  [A loss: 0.849686, acc: 0.175781]\n",
      "872: [D loss: 0.677948, acc: 0.570312]  [A loss: 0.878803, acc: 0.140625]\n",
      "873: [D loss: 0.689073, acc: 0.558594]  [A loss: 0.904361, acc: 0.101562]\n",
      "874: [D loss: 0.687584, acc: 0.546875]  [A loss: 0.875838, acc: 0.117188]\n",
      "875: [D loss: 0.675951, acc: 0.580078]  [A loss: 0.845661, acc: 0.203125]\n",
      "876: [D loss: 0.679118, acc: 0.585938]  [A loss: 0.964087, acc: 0.082031]\n",
      "877: [D loss: 0.690453, acc: 0.535156]  [A loss: 0.724870, acc: 0.457031]\n",
      "878: [D loss: 0.699436, acc: 0.542969]  [A loss: 1.001434, acc: 0.054688]\n",
      "879: [D loss: 0.686446, acc: 0.546875]  [A loss: 0.849615, acc: 0.187500]\n",
      "880: [D loss: 0.688118, acc: 0.554688]  [A loss: 0.931256, acc: 0.101562]\n",
      "881: [D loss: 0.674027, acc: 0.572266]  [A loss: 0.726988, acc: 0.453125]\n",
      "882: [D loss: 0.682277, acc: 0.564453]  [A loss: 0.999558, acc: 0.058594]\n",
      "883: [D loss: 0.673670, acc: 0.560547]  [A loss: 0.727840, acc: 0.441406]\n",
      "884: [D loss: 0.722384, acc: 0.501953]  [A loss: 1.090513, acc: 0.031250]\n",
      "885: [D loss: 0.674474, acc: 0.574219]  [A loss: 0.686103, acc: 0.542969]\n",
      "886: [D loss: 0.707047, acc: 0.523438]  [A loss: 0.997504, acc: 0.082031]\n",
      "887: [D loss: 0.681120, acc: 0.554688]  [A loss: 0.762688, acc: 0.332031]\n",
      "888: [D loss: 0.695186, acc: 0.529297]  [A loss: 0.944328, acc: 0.089844]\n",
      "889: [D loss: 0.676517, acc: 0.572266]  [A loss: 0.899027, acc: 0.128906]\n",
      "890: [D loss: 0.678965, acc: 0.591797]  [A loss: 0.840951, acc: 0.199219]\n",
      "891: [D loss: 0.696818, acc: 0.537109]  [A loss: 0.906313, acc: 0.082031]\n",
      "892: [D loss: 0.675385, acc: 0.583984]  [A loss: 0.780114, acc: 0.289062]\n",
      "893: [D loss: 0.702523, acc: 0.560547]  [A loss: 1.013461, acc: 0.062500]\n",
      "894: [D loss: 0.672177, acc: 0.603516]  [A loss: 0.745771, acc: 0.417969]\n",
      "895: [D loss: 0.713919, acc: 0.511719]  [A loss: 1.063256, acc: 0.027344]\n",
      "896: [D loss: 0.681065, acc: 0.556641]  [A loss: 0.690002, acc: 0.515625]\n",
      "897: [D loss: 0.723276, acc: 0.509766]  [A loss: 1.005891, acc: 0.027344]\n",
      "898: [D loss: 0.672233, acc: 0.585938]  [A loss: 0.761596, acc: 0.410156]\n",
      "899: [D loss: 0.686898, acc: 0.548828]  [A loss: 0.975892, acc: 0.074219]\n",
      "900: [D loss: 0.677875, acc: 0.542969]  [A loss: 0.811274, acc: 0.257812]\n",
      "901: [D loss: 0.703427, acc: 0.535156]  [A loss: 0.867910, acc: 0.121094]\n",
      "902: [D loss: 0.689176, acc: 0.568359]  [A loss: 0.887007, acc: 0.132812]\n",
      "903: [D loss: 0.676132, acc: 0.580078]  [A loss: 0.796171, acc: 0.265625]\n",
      "904: [D loss: 0.700296, acc: 0.531250]  [A loss: 0.944974, acc: 0.070312]\n",
      "905: [D loss: 0.673015, acc: 0.580078]  [A loss: 0.776264, acc: 0.296875]\n",
      "906: [D loss: 0.686940, acc: 0.546875]  [A loss: 0.968580, acc: 0.074219]\n",
      "907: [D loss: 0.677570, acc: 0.560547]  [A loss: 0.754155, acc: 0.375000]\n",
      "908: [D loss: 0.706726, acc: 0.525391]  [A loss: 0.959829, acc: 0.113281]\n",
      "909: [D loss: 0.688472, acc: 0.531250]  [A loss: 0.806540, acc: 0.277344]\n",
      "910: [D loss: 0.709846, acc: 0.523438]  [A loss: 0.990285, acc: 0.058594]\n",
      "911: [D loss: 0.665169, acc: 0.609375]  [A loss: 0.758475, acc: 0.359375]\n",
      "912: [D loss: 0.709518, acc: 0.531250]  [A loss: 0.925010, acc: 0.078125]\n",
      "913: [D loss: 0.673570, acc: 0.570312]  [A loss: 0.753035, acc: 0.386719]\n",
      "914: [D loss: 0.698380, acc: 0.537109]  [A loss: 0.933701, acc: 0.109375]\n",
      "915: [D loss: 0.665100, acc: 0.601562]  [A loss: 0.856291, acc: 0.210938]\n",
      "916: [D loss: 0.684886, acc: 0.541016]  [A loss: 0.841131, acc: 0.210938]\n",
      "917: [D loss: 0.683146, acc: 0.554688]  [A loss: 0.906277, acc: 0.105469]\n",
      "918: [D loss: 0.687413, acc: 0.570312]  [A loss: 0.862901, acc: 0.187500]\n",
      "919: [D loss: 0.667962, acc: 0.587891]  [A loss: 0.844105, acc: 0.210938]\n",
      "920: [D loss: 0.687081, acc: 0.560547]  [A loss: 0.913850, acc: 0.121094]\n",
      "921: [D loss: 0.694373, acc: 0.529297]  [A loss: 0.873946, acc: 0.175781]\n",
      "922: [D loss: 0.700624, acc: 0.537109]  [A loss: 0.820427, acc: 0.222656]\n",
      "923: [D loss: 0.695738, acc: 0.550781]  [A loss: 1.031612, acc: 0.035156]\n",
      "924: [D loss: 0.683042, acc: 0.568359]  [A loss: 0.712538, acc: 0.437500]\n",
      "925: [D loss: 0.733086, acc: 0.505859]  [A loss: 1.092956, acc: 0.015625]\n",
      "926: [D loss: 0.685688, acc: 0.544922]  [A loss: 0.709380, acc: 0.484375]\n",
      "927: [D loss: 0.714493, acc: 0.490234]  [A loss: 0.979573, acc: 0.062500]\n",
      "928: [D loss: 0.687654, acc: 0.539062]  [A loss: 0.716665, acc: 0.437500]\n",
      "929: [D loss: 0.698676, acc: 0.542969]  [A loss: 0.942839, acc: 0.085938]\n",
      "930: [D loss: 0.684973, acc: 0.572266]  [A loss: 0.829059, acc: 0.199219]\n",
      "931: [D loss: 0.687333, acc: 0.560547]  [A loss: 0.861081, acc: 0.187500]\n",
      "932: [D loss: 0.686295, acc: 0.548828]  [A loss: 0.801288, acc: 0.281250]\n",
      "933: [D loss: 0.692587, acc: 0.546875]  [A loss: 0.902149, acc: 0.105469]\n",
      "934: [D loss: 0.674290, acc: 0.603516]  [A loss: 0.811958, acc: 0.289062]\n",
      "935: [D loss: 0.711859, acc: 0.496094]  [A loss: 0.998121, acc: 0.042969]\n",
      "936: [D loss: 0.672672, acc: 0.605469]  [A loss: 0.731152, acc: 0.398438]\n",
      "937: [D loss: 0.691884, acc: 0.558594]  [A loss: 1.001861, acc: 0.023438]\n",
      "938: [D loss: 0.682932, acc: 0.535156]  [A loss: 0.710327, acc: 0.464844]\n",
      "939: [D loss: 0.690017, acc: 0.544922]  [A loss: 0.985029, acc: 0.066406]\n",
      "940: [D loss: 0.681821, acc: 0.539062]  [A loss: 0.717695, acc: 0.480469]\n",
      "941: [D loss: 0.709433, acc: 0.533203]  [A loss: 1.050434, acc: 0.027344]\n",
      "942: [D loss: 0.686080, acc: 0.554688]  [A loss: 0.659696, acc: 0.632812]\n",
      "943: [D loss: 0.720575, acc: 0.513672]  [A loss: 0.987180, acc: 0.046875]\n",
      "944: [D loss: 0.686848, acc: 0.531250]  [A loss: 0.725387, acc: 0.453125]\n",
      "945: [D loss: 0.704009, acc: 0.515625]  [A loss: 0.963802, acc: 0.082031]\n",
      "946: [D loss: 0.673930, acc: 0.599609]  [A loss: 0.760528, acc: 0.347656]\n",
      "947: [D loss: 0.698036, acc: 0.519531]  [A loss: 0.838237, acc: 0.207031]\n",
      "948: [D loss: 0.680328, acc: 0.562500]  [A loss: 0.820976, acc: 0.187500]\n",
      "949: [D loss: 0.688505, acc: 0.552734]  [A loss: 0.866402, acc: 0.160156]\n",
      "950: [D loss: 0.691968, acc: 0.529297]  [A loss: 0.801358, acc: 0.234375]\n",
      "951: [D loss: 0.696526, acc: 0.535156]  [A loss: 0.877036, acc: 0.191406]\n",
      "952: [D loss: 0.692566, acc: 0.554688]  [A loss: 0.895284, acc: 0.105469]\n",
      "953: [D loss: 0.684571, acc: 0.558594]  [A loss: 0.804577, acc: 0.250000]\n",
      "954: [D loss: 0.688329, acc: 0.560547]  [A loss: 0.879877, acc: 0.121094]\n",
      "955: [D loss: 0.686954, acc: 0.531250]  [A loss: 0.876601, acc: 0.148438]\n",
      "956: [D loss: 0.689717, acc: 0.541016]  [A loss: 0.768705, acc: 0.304688]\n",
      "957: [D loss: 0.690253, acc: 0.541016]  [A loss: 0.952938, acc: 0.101562]\n",
      "958: [D loss: 0.681787, acc: 0.550781]  [A loss: 0.771675, acc: 0.300781]\n",
      "959: [D loss: 0.692316, acc: 0.556641]  [A loss: 0.973170, acc: 0.089844]\n",
      "960: [D loss: 0.677697, acc: 0.595703]  [A loss: 0.766856, acc: 0.304688]\n",
      "961: [D loss: 0.690654, acc: 0.544922]  [A loss: 0.958996, acc: 0.082031]\n",
      "962: [D loss: 0.671021, acc: 0.591797]  [A loss: 0.786573, acc: 0.281250]\n",
      "963: [D loss: 0.690001, acc: 0.539062]  [A loss: 0.869850, acc: 0.144531]\n",
      "964: [D loss: 0.683237, acc: 0.601562]  [A loss: 0.802581, acc: 0.230469]\n",
      "965: [D loss: 0.693648, acc: 0.544922]  [A loss: 0.947620, acc: 0.058594]\n",
      "966: [D loss: 0.700980, acc: 0.478516]  [A loss: 0.822247, acc: 0.175781]\n",
      "967: [D loss: 0.678757, acc: 0.583984]  [A loss: 0.854237, acc: 0.167969]\n",
      "968: [D loss: 0.680742, acc: 0.580078]  [A loss: 0.862714, acc: 0.156250]\n",
      "969: [D loss: 0.690976, acc: 0.529297]  [A loss: 0.880623, acc: 0.156250]\n",
      "970: [D loss: 0.679280, acc: 0.554688]  [A loss: 0.845503, acc: 0.152344]\n",
      "971: [D loss: 0.665323, acc: 0.576172]  [A loss: 0.886020, acc: 0.136719]\n",
      "972: [D loss: 0.691754, acc: 0.539062]  [A loss: 0.987626, acc: 0.046875]\n",
      "973: [D loss: 0.678617, acc: 0.544922]  [A loss: 0.829441, acc: 0.246094]\n",
      "974: [D loss: 0.684710, acc: 0.556641]  [A loss: 0.978482, acc: 0.058594]\n",
      "975: [D loss: 0.676398, acc: 0.572266]  [A loss: 0.687673, acc: 0.554688]\n",
      "976: [D loss: 0.738193, acc: 0.517578]  [A loss: 1.169655, acc: 0.019531]\n",
      "977: [D loss: 0.705417, acc: 0.505859]  [A loss: 0.608557, acc: 0.718750]\n",
      "978: [D loss: 0.730672, acc: 0.500000]  [A loss: 0.994352, acc: 0.054688]\n",
      "979: [D loss: 0.699232, acc: 0.490234]  [A loss: 0.793218, acc: 0.242188]\n",
      "980: [D loss: 0.701960, acc: 0.525391]  [A loss: 0.900036, acc: 0.121094]\n",
      "981: [D loss: 0.678556, acc: 0.580078]  [A loss: 0.767921, acc: 0.328125]\n",
      "982: [D loss: 0.691501, acc: 0.531250]  [A loss: 0.808463, acc: 0.195312]\n",
      "983: [D loss: 0.684130, acc: 0.568359]  [A loss: 0.798338, acc: 0.214844]\n",
      "984: [D loss: 0.684725, acc: 0.542969]  [A loss: 0.815811, acc: 0.203125]\n",
      "985: [D loss: 0.675580, acc: 0.589844]  [A loss: 0.826099, acc: 0.199219]\n",
      "986: [D loss: 0.685592, acc: 0.554688]  [A loss: 0.888015, acc: 0.117188]\n",
      "987: [D loss: 0.676975, acc: 0.578125]  [A loss: 0.793594, acc: 0.230469]\n",
      "988: [D loss: 0.680433, acc: 0.542969]  [A loss: 0.843091, acc: 0.226562]\n",
      "989: [D loss: 0.708511, acc: 0.525391]  [A loss: 0.807539, acc: 0.234375]\n",
      "990: [D loss: 0.699312, acc: 0.529297]  [A loss: 0.984671, acc: 0.039062]\n",
      "991: [D loss: 0.689137, acc: 0.537109]  [A loss: 0.731793, acc: 0.425781]\n",
      "992: [D loss: 0.703560, acc: 0.525391]  [A loss: 0.947055, acc: 0.074219]\n",
      "993: [D loss: 0.679762, acc: 0.568359]  [A loss: 0.771036, acc: 0.335938]\n",
      "994: [D loss: 0.707343, acc: 0.527344]  [A loss: 0.990478, acc: 0.031250]\n",
      "995: [D loss: 0.680519, acc: 0.574219]  [A loss: 0.739486, acc: 0.359375]\n",
      "996: [D loss: 0.694525, acc: 0.527344]  [A loss: 0.890671, acc: 0.113281]\n",
      "997: [D loss: 0.678855, acc: 0.570312]  [A loss: 0.784847, acc: 0.289062]\n",
      "998: [D loss: 0.682979, acc: 0.544922]  [A loss: 0.883837, acc: 0.101562]\n",
      "999: [D loss: 0.678705, acc: 0.570312]  [A loss: 0.798238, acc: 0.285156]\n",
      "1000: [D loss: 0.691503, acc: 0.529297]  [A loss: 0.920855, acc: 0.113281]\n",
      "1001: [D loss: 0.687677, acc: 0.529297]  [A loss: 0.752996, acc: 0.316406]\n",
      "1002: [D loss: 0.695902, acc: 0.533203]  [A loss: 0.911896, acc: 0.062500]\n",
      "1003: [D loss: 0.694846, acc: 0.537109]  [A loss: 0.916575, acc: 0.097656]\n",
      "1004: [D loss: 0.679385, acc: 0.585938]  [A loss: 0.800425, acc: 0.281250]\n",
      "1005: [D loss: 0.691811, acc: 0.560547]  [A loss: 0.999159, acc: 0.085938]\n",
      "1006: [D loss: 0.665488, acc: 0.626953]  [A loss: 0.713951, acc: 0.453125]\n",
      "1007: [D loss: 0.706050, acc: 0.533203]  [A loss: 1.028747, acc: 0.023438]\n",
      "1008: [D loss: 0.684092, acc: 0.564453]  [A loss: 0.779119, acc: 0.273438]\n",
      "1009: [D loss: 0.700513, acc: 0.541016]  [A loss: 0.952287, acc: 0.070312]\n",
      "1010: [D loss: 0.674318, acc: 0.566406]  [A loss: 0.793058, acc: 0.250000]\n",
      "1011: [D loss: 0.684326, acc: 0.554688]  [A loss: 0.895933, acc: 0.089844]\n",
      "1012: [D loss: 0.680459, acc: 0.558594]  [A loss: 0.787326, acc: 0.281250]\n",
      "1013: [D loss: 0.702025, acc: 0.535156]  [A loss: 0.902594, acc: 0.105469]\n",
      "1014: [D loss: 0.685117, acc: 0.548828]  [A loss: 0.793519, acc: 0.277344]\n",
      "1015: [D loss: 0.686768, acc: 0.539062]  [A loss: 0.960819, acc: 0.082031]\n",
      "1016: [D loss: 0.674222, acc: 0.593750]  [A loss: 0.750116, acc: 0.386719]\n",
      "1017: [D loss: 0.713240, acc: 0.509766]  [A loss: 0.971719, acc: 0.058594]\n",
      "1018: [D loss: 0.695519, acc: 0.500000]  [A loss: 0.854747, acc: 0.136719]\n",
      "1019: [D loss: 0.698808, acc: 0.521484]  [A loss: 0.874729, acc: 0.140625]\n",
      "1020: [D loss: 0.675002, acc: 0.554688]  [A loss: 0.847941, acc: 0.195312]\n",
      "1021: [D loss: 0.704171, acc: 0.519531]  [A loss: 0.866036, acc: 0.113281]\n",
      "1022: [D loss: 0.685306, acc: 0.541016]  [A loss: 0.957814, acc: 0.089844]\n",
      "1023: [D loss: 0.678107, acc: 0.576172]  [A loss: 0.791173, acc: 0.308594]\n",
      "1024: [D loss: 0.694943, acc: 0.548828]  [A loss: 0.999013, acc: 0.074219]\n",
      "1025: [D loss: 0.683322, acc: 0.548828]  [A loss: 0.673595, acc: 0.574219]\n",
      "1026: [D loss: 0.722427, acc: 0.517578]  [A loss: 1.087149, acc: 0.035156]\n",
      "1027: [D loss: 0.712135, acc: 0.501953]  [A loss: 0.712958, acc: 0.468750]\n",
      "1028: [D loss: 0.721579, acc: 0.501953]  [A loss: 1.012113, acc: 0.027344]\n",
      "1029: [D loss: 0.691261, acc: 0.521484]  [A loss: 0.736153, acc: 0.449219]\n",
      "1030: [D loss: 0.695101, acc: 0.529297]  [A loss: 0.962353, acc: 0.058594]\n",
      "1031: [D loss: 0.676232, acc: 0.564453]  [A loss: 0.725672, acc: 0.414062]\n",
      "1032: [D loss: 0.695515, acc: 0.523438]  [A loss: 0.900342, acc: 0.085938]\n",
      "1033: [D loss: 0.669474, acc: 0.611328]  [A loss: 0.827778, acc: 0.199219]\n",
      "1034: [D loss: 0.685560, acc: 0.552734]  [A loss: 0.878760, acc: 0.109375]\n",
      "1035: [D loss: 0.674497, acc: 0.570312]  [A loss: 0.840858, acc: 0.203125]\n",
      "1036: [D loss: 0.679813, acc: 0.585938]  [A loss: 0.817434, acc: 0.214844]\n",
      "1037: [D loss: 0.685720, acc: 0.562500]  [A loss: 0.903377, acc: 0.109375]\n",
      "1038: [D loss: 0.678032, acc: 0.552734]  [A loss: 0.774850, acc: 0.304688]\n",
      "1039: [D loss: 0.709261, acc: 0.513672]  [A loss: 0.903738, acc: 0.121094]\n",
      "1040: [D loss: 0.690628, acc: 0.533203]  [A loss: 0.877148, acc: 0.144531]\n",
      "1041: [D loss: 0.677067, acc: 0.587891]  [A loss: 0.803368, acc: 0.281250]\n",
      "1042: [D loss: 0.685004, acc: 0.550781]  [A loss: 0.967274, acc: 0.074219]\n",
      "1043: [D loss: 0.687321, acc: 0.548828]  [A loss: 0.749323, acc: 0.351562]\n",
      "1044: [D loss: 0.669822, acc: 0.560547]  [A loss: 0.971169, acc: 0.054688]\n",
      "1045: [D loss: 0.680046, acc: 0.589844]  [A loss: 0.775430, acc: 0.300781]\n",
      "1046: [D loss: 0.700661, acc: 0.533203]  [A loss: 0.971272, acc: 0.046875]\n",
      "1047: [D loss: 0.678080, acc: 0.574219]  [A loss: 0.715300, acc: 0.445312]\n",
      "1048: [D loss: 0.726463, acc: 0.484375]  [A loss: 1.064163, acc: 0.019531]\n",
      "1049: [D loss: 0.682268, acc: 0.533203]  [A loss: 0.706397, acc: 0.464844]\n",
      "1050: [D loss: 0.711279, acc: 0.531250]  [A loss: 0.977244, acc: 0.054688]\n",
      "1051: [D loss: 0.684167, acc: 0.552734]  [A loss: 0.738335, acc: 0.437500]\n",
      "1052: [D loss: 0.697580, acc: 0.513672]  [A loss: 0.912007, acc: 0.117188]\n",
      "1053: [D loss: 0.687414, acc: 0.580078]  [A loss: 0.824746, acc: 0.203125]\n",
      "1054: [D loss: 0.694887, acc: 0.539062]  [A loss: 0.842960, acc: 0.175781]\n",
      "1055: [D loss: 0.687458, acc: 0.535156]  [A loss: 0.894516, acc: 0.089844]\n",
      "1056: [D loss: 0.683213, acc: 0.574219]  [A loss: 0.762383, acc: 0.312500]\n",
      "1057: [D loss: 0.700162, acc: 0.527344]  [A loss: 0.997215, acc: 0.050781]\n",
      "1058: [D loss: 0.688278, acc: 0.548828]  [A loss: 0.761251, acc: 0.328125]\n",
      "1059: [D loss: 0.681899, acc: 0.544922]  [A loss: 0.942427, acc: 0.074219]\n",
      "1060: [D loss: 0.677198, acc: 0.562500]  [A loss: 0.765580, acc: 0.339844]\n",
      "1061: [D loss: 0.694118, acc: 0.509766]  [A loss: 0.994164, acc: 0.042969]\n",
      "1062: [D loss: 0.686476, acc: 0.558594]  [A loss: 0.705336, acc: 0.507812]\n",
      "1063: [D loss: 0.700072, acc: 0.544922]  [A loss: 0.933819, acc: 0.097656]\n",
      "1064: [D loss: 0.680470, acc: 0.542969]  [A loss: 0.772101, acc: 0.292969]\n",
      "1065: [D loss: 0.688599, acc: 0.548828]  [A loss: 0.919035, acc: 0.113281]\n",
      "1066: [D loss: 0.688007, acc: 0.539062]  [A loss: 0.760465, acc: 0.316406]\n",
      "1067: [D loss: 0.701364, acc: 0.521484]  [A loss: 0.979561, acc: 0.066406]\n",
      "1068: [D loss: 0.684932, acc: 0.574219]  [A loss: 0.699444, acc: 0.511719]\n",
      "1069: [D loss: 0.721386, acc: 0.521484]  [A loss: 0.990673, acc: 0.035156]\n",
      "1070: [D loss: 0.684373, acc: 0.548828]  [A loss: 0.754153, acc: 0.351562]\n",
      "1071: [D loss: 0.703502, acc: 0.519531]  [A loss: 0.930190, acc: 0.089844]\n",
      "1072: [D loss: 0.679685, acc: 0.564453]  [A loss: 0.770981, acc: 0.328125]\n",
      "1073: [D loss: 0.689665, acc: 0.548828]  [A loss: 0.908779, acc: 0.089844]\n",
      "1074: [D loss: 0.680775, acc: 0.572266]  [A loss: 0.783484, acc: 0.257812]\n",
      "1075: [D loss: 0.688771, acc: 0.562500]  [A loss: 0.901094, acc: 0.117188]\n",
      "1076: [D loss: 0.676288, acc: 0.564453]  [A loss: 0.787260, acc: 0.285156]\n",
      "1077: [D loss: 0.680682, acc: 0.568359]  [A loss: 0.887624, acc: 0.136719]\n",
      "1078: [D loss: 0.673472, acc: 0.576172]  [A loss: 0.795026, acc: 0.269531]\n",
      "1079: [D loss: 0.701797, acc: 0.529297]  [A loss: 0.961349, acc: 0.074219]\n",
      "1080: [D loss: 0.710967, acc: 0.492188]  [A loss: 0.785385, acc: 0.289062]\n",
      "1081: [D loss: 0.694394, acc: 0.535156]  [A loss: 0.987598, acc: 0.039062]\n",
      "1082: [D loss: 0.680989, acc: 0.542969]  [A loss: 0.703588, acc: 0.480469]\n",
      "1083: [D loss: 0.719923, acc: 0.511719]  [A loss: 0.992899, acc: 0.050781]\n",
      "1084: [D loss: 0.675298, acc: 0.560547]  [A loss: 0.690278, acc: 0.539062]\n",
      "1085: [D loss: 0.712772, acc: 0.511719]  [A loss: 1.002389, acc: 0.046875]\n",
      "1086: [D loss: 0.688139, acc: 0.548828]  [A loss: 0.733354, acc: 0.386719]\n",
      "1087: [D loss: 0.693844, acc: 0.529297]  [A loss: 0.900394, acc: 0.113281]\n",
      "1088: [D loss: 0.677159, acc: 0.570312]  [A loss: 0.842235, acc: 0.179688]\n",
      "1089: [D loss: 0.686343, acc: 0.546875]  [A loss: 0.853956, acc: 0.183594]\n",
      "1090: [D loss: 0.688131, acc: 0.562500]  [A loss: 0.831452, acc: 0.179688]\n",
      "1091: [D loss: 0.695912, acc: 0.535156]  [A loss: 0.861707, acc: 0.128906]\n",
      "1092: [D loss: 0.683540, acc: 0.583984]  [A loss: 0.834890, acc: 0.195312]\n",
      "1093: [D loss: 0.670957, acc: 0.574219]  [A loss: 0.808238, acc: 0.261719]\n",
      "1094: [D loss: 0.707980, acc: 0.523438]  [A loss: 0.901288, acc: 0.105469]\n",
      "1095: [D loss: 0.686750, acc: 0.544922]  [A loss: 0.894116, acc: 0.089844]\n",
      "1096: [D loss: 0.689259, acc: 0.542969]  [A loss: 0.810547, acc: 0.242188]\n",
      "1097: [D loss: 0.682805, acc: 0.550781]  [A loss: 0.926384, acc: 0.078125]\n",
      "1098: [D loss: 0.668803, acc: 0.595703]  [A loss: 0.763381, acc: 0.339844]\n",
      "1099: [D loss: 0.702431, acc: 0.517578]  [A loss: 1.047062, acc: 0.054688]\n",
      "1100: [D loss: 0.687244, acc: 0.533203]  [A loss: 0.759010, acc: 0.367188]\n",
      "1101: [D loss: 0.706629, acc: 0.527344]  [A loss: 1.067759, acc: 0.023438]\n",
      "1102: [D loss: 0.692320, acc: 0.554688]  [A loss: 0.645617, acc: 0.617188]\n",
      "1103: [D loss: 0.726551, acc: 0.519531]  [A loss: 0.977655, acc: 0.046875]\n",
      "1104: [D loss: 0.697209, acc: 0.513672]  [A loss: 0.777016, acc: 0.300781]\n",
      "1105: [D loss: 0.704149, acc: 0.498047]  [A loss: 0.968086, acc: 0.062500]\n",
      "1106: [D loss: 0.665980, acc: 0.582031]  [A loss: 0.728581, acc: 0.429688]\n",
      "1107: [D loss: 0.705307, acc: 0.535156]  [A loss: 0.961395, acc: 0.066406]\n",
      "1108: [D loss: 0.683774, acc: 0.556641]  [A loss: 0.756049, acc: 0.332031]\n",
      "1109: [D loss: 0.694634, acc: 0.535156]  [A loss: 0.927928, acc: 0.093750]\n",
      "1110: [D loss: 0.701198, acc: 0.507812]  [A loss: 0.789725, acc: 0.285156]\n",
      "1111: [D loss: 0.702842, acc: 0.525391]  [A loss: 0.910024, acc: 0.121094]\n",
      "1112: [D loss: 0.688170, acc: 0.542969]  [A loss: 0.733498, acc: 0.375000]\n",
      "1113: [D loss: 0.700730, acc: 0.537109]  [A loss: 0.970917, acc: 0.078125]\n",
      "1114: [D loss: 0.676575, acc: 0.566406]  [A loss: 0.738465, acc: 0.414062]\n",
      "1115: [D loss: 0.694393, acc: 0.529297]  [A loss: 0.874485, acc: 0.117188]\n",
      "1116: [D loss: 0.674264, acc: 0.601562]  [A loss: 0.756677, acc: 0.351562]\n",
      "1117: [D loss: 0.691998, acc: 0.521484]  [A loss: 0.988129, acc: 0.062500]\n",
      "1118: [D loss: 0.678172, acc: 0.570312]  [A loss: 0.725515, acc: 0.460938]\n",
      "1119: [D loss: 0.705191, acc: 0.507812]  [A loss: 0.923735, acc: 0.078125]\n",
      "1120: [D loss: 0.672066, acc: 0.587891]  [A loss: 0.753625, acc: 0.355469]\n",
      "1121: [D loss: 0.704276, acc: 0.517578]  [A loss: 0.948029, acc: 0.070312]\n",
      "1122: [D loss: 0.691600, acc: 0.548828]  [A loss: 0.854301, acc: 0.164062]\n",
      "1123: [D loss: 0.686876, acc: 0.539062]  [A loss: 0.874194, acc: 0.152344]\n",
      "1124: [D loss: 0.669166, acc: 0.572266]  [A loss: 0.784620, acc: 0.312500]\n",
      "1125: [D loss: 0.740144, acc: 0.451172]  [A loss: 0.988026, acc: 0.058594]\n",
      "1126: [D loss: 0.670796, acc: 0.580078]  [A loss: 0.796352, acc: 0.285156]\n",
      "1127: [D loss: 0.685714, acc: 0.564453]  [A loss: 0.936606, acc: 0.117188]\n",
      "1128: [D loss: 0.672301, acc: 0.562500]  [A loss: 0.828903, acc: 0.281250]\n",
      "1129: [D loss: 0.686980, acc: 0.546875]  [A loss: 0.842748, acc: 0.203125]\n",
      "1130: [D loss: 0.690491, acc: 0.558594]  [A loss: 0.914007, acc: 0.101562]\n",
      "1131: [D loss: 0.685681, acc: 0.556641]  [A loss: 0.862783, acc: 0.160156]\n",
      "1132: [D loss: 0.692331, acc: 0.541016]  [A loss: 0.910821, acc: 0.101562]\n",
      "1133: [D loss: 0.682838, acc: 0.556641]  [A loss: 0.790524, acc: 0.242188]\n",
      "1134: [D loss: 0.686104, acc: 0.548828]  [A loss: 1.012399, acc: 0.050781]\n",
      "1135: [D loss: 0.673959, acc: 0.591797]  [A loss: 0.688139, acc: 0.515625]\n",
      "1136: [D loss: 0.710100, acc: 0.529297]  [A loss: 1.143117, acc: 0.019531]\n",
      "1137: [D loss: 0.673190, acc: 0.570312]  [A loss: 0.615604, acc: 0.695312]\n",
      "1138: [D loss: 0.736879, acc: 0.505859]  [A loss: 1.026749, acc: 0.046875]\n",
      "1139: [D loss: 0.697762, acc: 0.537109]  [A loss: 0.711401, acc: 0.445312]\n",
      "1140: [D loss: 0.694481, acc: 0.527344]  [A loss: 0.891512, acc: 0.109375]\n",
      "1141: [D loss: 0.681449, acc: 0.546875]  [A loss: 0.791458, acc: 0.304688]\n",
      "1142: [D loss: 0.696558, acc: 0.537109]  [A loss: 0.870742, acc: 0.175781]\n",
      "1143: [D loss: 0.680616, acc: 0.566406]  [A loss: 0.825623, acc: 0.210938]\n",
      "1144: [D loss: 0.684897, acc: 0.570312]  [A loss: 0.879317, acc: 0.187500]\n",
      "1145: [D loss: 0.689155, acc: 0.550781]  [A loss: 0.802683, acc: 0.296875]\n",
      "1146: [D loss: 0.679477, acc: 0.562500]  [A loss: 0.858830, acc: 0.191406]\n",
      "1147: [D loss: 0.679707, acc: 0.548828]  [A loss: 0.858988, acc: 0.160156]\n",
      "1148: [D loss: 0.687703, acc: 0.542969]  [A loss: 0.875458, acc: 0.167969]\n",
      "1149: [D loss: 0.688841, acc: 0.527344]  [A loss: 0.924075, acc: 0.101562]\n",
      "1150: [D loss: 0.687482, acc: 0.568359]  [A loss: 0.785216, acc: 0.300781]\n",
      "1151: [D loss: 0.691253, acc: 0.548828]  [A loss: 0.960453, acc: 0.097656]\n",
      "1152: [D loss: 0.686566, acc: 0.548828]  [A loss: 0.768260, acc: 0.332031]\n",
      "1153: [D loss: 0.699977, acc: 0.541016]  [A loss: 1.024674, acc: 0.070312]\n",
      "1154: [D loss: 0.675473, acc: 0.593750]  [A loss: 0.669723, acc: 0.578125]\n",
      "1155: [D loss: 0.706150, acc: 0.517578]  [A loss: 0.968052, acc: 0.078125]\n",
      "1156: [D loss: 0.677747, acc: 0.562500]  [A loss: 0.757295, acc: 0.382812]\n",
      "1157: [D loss: 0.706895, acc: 0.513672]  [A loss: 1.035924, acc: 0.039062]\n",
      "1158: [D loss: 0.687758, acc: 0.548828]  [A loss: 0.680415, acc: 0.574219]\n",
      "1159: [D loss: 0.725269, acc: 0.517578]  [A loss: 1.018566, acc: 0.082031]\n",
      "1160: [D loss: 0.699585, acc: 0.531250]  [A loss: 0.750713, acc: 0.363281]\n",
      "1161: [D loss: 0.686978, acc: 0.539062]  [A loss: 0.848925, acc: 0.214844]\n",
      "1162: [D loss: 0.697250, acc: 0.529297]  [A loss: 0.794137, acc: 0.289062]\n",
      "1163: [D loss: 0.696630, acc: 0.527344]  [A loss: 0.903588, acc: 0.097656]\n",
      "1164: [D loss: 0.683614, acc: 0.542969]  [A loss: 0.778468, acc: 0.312500]\n",
      "1165: [D loss: 0.696685, acc: 0.513672]  [A loss: 0.872052, acc: 0.156250]\n",
      "1166: [D loss: 0.694827, acc: 0.535156]  [A loss: 0.793596, acc: 0.289062]\n",
      "1167: [D loss: 0.694346, acc: 0.521484]  [A loss: 0.844193, acc: 0.175781]\n",
      "1168: [D loss: 0.679245, acc: 0.537109]  [A loss: 0.806483, acc: 0.304688]\n",
      "1169: [D loss: 0.687890, acc: 0.554688]  [A loss: 0.888259, acc: 0.156250]\n",
      "1170: [D loss: 0.683247, acc: 0.531250]  [A loss: 0.780367, acc: 0.320312]\n",
      "1171: [D loss: 0.716132, acc: 0.513672]  [A loss: 1.001596, acc: 0.066406]\n",
      "1172: [D loss: 0.670305, acc: 0.605469]  [A loss: 0.732357, acc: 0.398438]\n",
      "1173: [D loss: 0.699449, acc: 0.525391]  [A loss: 0.990829, acc: 0.050781]\n",
      "1174: [D loss: 0.683796, acc: 0.550781]  [A loss: 0.767836, acc: 0.324219]\n",
      "1175: [D loss: 0.712090, acc: 0.509766]  [A loss: 0.909931, acc: 0.125000]\n",
      "1176: [D loss: 0.681419, acc: 0.562500]  [A loss: 0.725518, acc: 0.410156]\n",
      "1177: [D loss: 0.699915, acc: 0.535156]  [A loss: 0.977973, acc: 0.046875]\n",
      "1178: [D loss: 0.692208, acc: 0.513672]  [A loss: 0.772904, acc: 0.328125]\n",
      "1179: [D loss: 0.711443, acc: 0.519531]  [A loss: 0.986153, acc: 0.039062]\n",
      "1180: [D loss: 0.682790, acc: 0.554688]  [A loss: 0.701507, acc: 0.515625]\n",
      "1181: [D loss: 0.712717, acc: 0.509766]  [A loss: 0.998188, acc: 0.082031]\n",
      "1182: [D loss: 0.679333, acc: 0.568359]  [A loss: 0.709146, acc: 0.511719]\n",
      "1183: [D loss: 0.707023, acc: 0.533203]  [A loss: 0.934036, acc: 0.082031]\n",
      "1184: [D loss: 0.683533, acc: 0.542969]  [A loss: 0.729865, acc: 0.421875]\n",
      "1185: [D loss: 0.707972, acc: 0.505859]  [A loss: 0.887498, acc: 0.136719]\n",
      "1186: [D loss: 0.695905, acc: 0.521484]  [A loss: 0.808738, acc: 0.269531]\n",
      "1187: [D loss: 0.687354, acc: 0.542969]  [A loss: 0.848097, acc: 0.187500]\n",
      "1188: [D loss: 0.688247, acc: 0.554688]  [A loss: 0.841080, acc: 0.191406]\n",
      "1189: [D loss: 0.675595, acc: 0.562500]  [A loss: 0.898836, acc: 0.121094]\n",
      "1190: [D loss: 0.686795, acc: 0.562500]  [A loss: 0.795046, acc: 0.312500]\n",
      "1191: [D loss: 0.709320, acc: 0.509766]  [A loss: 0.918700, acc: 0.058594]\n",
      "1192: [D loss: 0.681581, acc: 0.548828]  [A loss: 0.792153, acc: 0.285156]\n",
      "1193: [D loss: 0.704118, acc: 0.525391]  [A loss: 0.874631, acc: 0.140625]\n",
      "1194: [D loss: 0.698088, acc: 0.525391]  [A loss: 0.753749, acc: 0.339844]\n",
      "1195: [D loss: 0.697779, acc: 0.539062]  [A loss: 0.940364, acc: 0.058594]\n",
      "1196: [D loss: 0.714147, acc: 0.486328]  [A loss: 0.858015, acc: 0.148438]\n",
      "1197: [D loss: 0.689334, acc: 0.535156]  [A loss: 0.838512, acc: 0.214844]\n",
      "1198: [D loss: 0.699188, acc: 0.509766]  [A loss: 0.842763, acc: 0.210938]\n",
      "1199: [D loss: 0.692961, acc: 0.541016]  [A loss: 0.846074, acc: 0.164062]\n",
      "1200: [D loss: 0.690165, acc: 0.529297]  [A loss: 0.846599, acc: 0.140625]\n",
      "1201: [D loss: 0.682614, acc: 0.568359]  [A loss: 0.905433, acc: 0.105469]\n",
      "1202: [D loss: 0.689213, acc: 0.564453]  [A loss: 0.768005, acc: 0.339844]\n",
      "1203: [D loss: 0.692477, acc: 0.556641]  [A loss: 0.999113, acc: 0.054688]\n",
      "1204: [D loss: 0.689160, acc: 0.562500]  [A loss: 0.716467, acc: 0.468750]\n",
      "1205: [D loss: 0.712077, acc: 0.515625]  [A loss: 1.073692, acc: 0.027344]\n",
      "1206: [D loss: 0.686488, acc: 0.529297]  [A loss: 0.689689, acc: 0.527344]\n",
      "1207: [D loss: 0.711838, acc: 0.531250]  [A loss: 1.016303, acc: 0.070312]\n",
      "1208: [D loss: 0.696402, acc: 0.505859]  [A loss: 0.686284, acc: 0.554688]\n",
      "1209: [D loss: 0.712828, acc: 0.523438]  [A loss: 0.928749, acc: 0.074219]\n",
      "1210: [D loss: 0.677323, acc: 0.560547]  [A loss: 0.828364, acc: 0.214844]\n",
      "1211: [D loss: 0.698523, acc: 0.500000]  [A loss: 0.858399, acc: 0.144531]\n",
      "1212: [D loss: 0.692725, acc: 0.546875]  [A loss: 0.794413, acc: 0.285156]\n",
      "1213: [D loss: 0.688541, acc: 0.552734]  [A loss: 0.871645, acc: 0.152344]\n",
      "1214: [D loss: 0.690548, acc: 0.541016]  [A loss: 0.800653, acc: 0.292969]\n",
      "1215: [D loss: 0.695677, acc: 0.535156]  [A loss: 0.893977, acc: 0.121094]\n",
      "1216: [D loss: 0.684349, acc: 0.554688]  [A loss: 0.744105, acc: 0.378906]\n",
      "1217: [D loss: 0.716496, acc: 0.507812]  [A loss: 0.998418, acc: 0.050781]\n",
      "1218: [D loss: 0.683987, acc: 0.548828]  [A loss: 0.702179, acc: 0.476562]\n",
      "1219: [D loss: 0.703442, acc: 0.523438]  [A loss: 0.973422, acc: 0.066406]\n",
      "1220: [D loss: 0.676639, acc: 0.572266]  [A loss: 0.699355, acc: 0.539062]\n",
      "1221: [D loss: 0.720042, acc: 0.515625]  [A loss: 0.923964, acc: 0.101562]\n",
      "1222: [D loss: 0.696863, acc: 0.529297]  [A loss: 0.910233, acc: 0.089844]\n",
      "1223: [D loss: 0.694854, acc: 0.529297]  [A loss: 0.804990, acc: 0.261719]\n",
      "1224: [D loss: 0.698141, acc: 0.511719]  [A loss: 0.894559, acc: 0.113281]\n",
      "1225: [D loss: 0.682037, acc: 0.576172]  [A loss: 0.824234, acc: 0.191406]\n",
      "1226: [D loss: 0.692604, acc: 0.527344]  [A loss: 0.823301, acc: 0.199219]\n",
      "1227: [D loss: 0.673081, acc: 0.576172]  [A loss: 0.901623, acc: 0.164062]\n",
      "1228: [D loss: 0.686846, acc: 0.537109]  [A loss: 0.776419, acc: 0.316406]\n",
      "1229: [D loss: 0.696796, acc: 0.529297]  [A loss: 0.932037, acc: 0.082031]\n",
      "1230: [D loss: 0.675510, acc: 0.574219]  [A loss: 0.748393, acc: 0.398438]\n",
      "1231: [D loss: 0.700796, acc: 0.521484]  [A loss: 0.980447, acc: 0.078125]\n",
      "1232: [D loss: 0.686576, acc: 0.537109]  [A loss: 0.721582, acc: 0.445312]\n",
      "1233: [D loss: 0.701851, acc: 0.529297]  [A loss: 1.000098, acc: 0.054688]\n",
      "1234: [D loss: 0.682280, acc: 0.548828]  [A loss: 0.723763, acc: 0.382812]\n",
      "1235: [D loss: 0.705119, acc: 0.500000]  [A loss: 0.939314, acc: 0.082031]\n",
      "1236: [D loss: 0.678629, acc: 0.576172]  [A loss: 0.816265, acc: 0.199219]\n",
      "1237: [D loss: 0.689230, acc: 0.535156]  [A loss: 0.886144, acc: 0.148438]\n",
      "1238: [D loss: 0.693282, acc: 0.544922]  [A loss: 0.839166, acc: 0.199219]\n",
      "1239: [D loss: 0.691088, acc: 0.544922]  [A loss: 0.864502, acc: 0.156250]\n",
      "1240: [D loss: 0.691781, acc: 0.560547]  [A loss: 0.809797, acc: 0.214844]\n",
      "1241: [D loss: 0.692726, acc: 0.527344]  [A loss: 0.812288, acc: 0.226562]\n",
      "1242: [D loss: 0.693688, acc: 0.533203]  [A loss: 0.940911, acc: 0.089844]\n",
      "1243: [D loss: 0.683175, acc: 0.574219]  [A loss: 0.739808, acc: 0.390625]\n",
      "1244: [D loss: 0.723159, acc: 0.509766]  [A loss: 1.015549, acc: 0.039062]\n",
      "1245: [D loss: 0.690375, acc: 0.554688]  [A loss: 0.683617, acc: 0.484375]\n",
      "1246: [D loss: 0.710990, acc: 0.525391]  [A loss: 0.945912, acc: 0.074219]\n",
      "1247: [D loss: 0.677498, acc: 0.556641]  [A loss: 0.753804, acc: 0.378906]\n",
      "1248: [D loss: 0.680161, acc: 0.568359]  [A loss: 0.916617, acc: 0.128906]\n",
      "1249: [D loss: 0.721174, acc: 0.494141]  [A loss: 0.758200, acc: 0.367188]\n",
      "1250: [D loss: 0.711798, acc: 0.527344]  [A loss: 0.956947, acc: 0.046875]\n",
      "1251: [D loss: 0.678985, acc: 0.566406]  [A loss: 0.706990, acc: 0.503906]\n",
      "1252: [D loss: 0.707480, acc: 0.525391]  [A loss: 0.917751, acc: 0.097656]\n",
      "1253: [D loss: 0.676323, acc: 0.583984]  [A loss: 0.778926, acc: 0.296875]\n",
      "1254: [D loss: 0.695838, acc: 0.529297]  [A loss: 0.909258, acc: 0.125000]\n",
      "1255: [D loss: 0.688072, acc: 0.554688]  [A loss: 0.774220, acc: 0.335938]\n",
      "1256: [D loss: 0.700898, acc: 0.521484]  [A loss: 0.939059, acc: 0.101562]\n",
      "1257: [D loss: 0.686058, acc: 0.544922]  [A loss: 0.722976, acc: 0.429688]\n",
      "1258: [D loss: 0.699014, acc: 0.513672]  [A loss: 0.931963, acc: 0.085938]\n",
      "1259: [D loss: 0.691254, acc: 0.544922]  [A loss: 0.776837, acc: 0.324219]\n",
      "1260: [D loss: 0.698571, acc: 0.529297]  [A loss: 0.831894, acc: 0.207031]\n",
      "1261: [D loss: 0.705324, acc: 0.515625]  [A loss: 0.956164, acc: 0.054688]\n",
      "1262: [D loss: 0.688518, acc: 0.544922]  [A loss: 0.694954, acc: 0.535156]\n",
      "1263: [D loss: 0.714331, acc: 0.515625]  [A loss: 0.956766, acc: 0.070312]\n",
      "1264: [D loss: 0.684805, acc: 0.562500]  [A loss: 0.729863, acc: 0.414062]\n",
      "1265: [D loss: 0.691888, acc: 0.519531]  [A loss: 0.874569, acc: 0.125000]\n",
      "1266: [D loss: 0.677709, acc: 0.589844]  [A loss: 0.753699, acc: 0.382812]\n",
      "1267: [D loss: 0.701699, acc: 0.519531]  [A loss: 0.897190, acc: 0.113281]\n",
      "1268: [D loss: 0.686846, acc: 0.531250]  [A loss: 0.786345, acc: 0.296875]\n",
      "1269: [D loss: 0.681478, acc: 0.550781]  [A loss: 0.836370, acc: 0.175781]\n",
      "1270: [D loss: 0.672629, acc: 0.589844]  [A loss: 0.791301, acc: 0.300781]\n",
      "1271: [D loss: 0.701111, acc: 0.531250]  [A loss: 0.860116, acc: 0.148438]\n",
      "1272: [D loss: 0.700883, acc: 0.505859]  [A loss: 0.884107, acc: 0.125000]\n",
      "1273: [D loss: 0.681200, acc: 0.572266]  [A loss: 0.782806, acc: 0.316406]\n",
      "1274: [D loss: 0.699046, acc: 0.519531]  [A loss: 0.982699, acc: 0.101562]\n",
      "1275: [D loss: 0.690233, acc: 0.562500]  [A loss: 0.708529, acc: 0.480469]\n",
      "1276: [D loss: 0.708491, acc: 0.513672]  [A loss: 1.000264, acc: 0.054688]\n",
      "1277: [D loss: 0.693425, acc: 0.539062]  [A loss: 0.709288, acc: 0.468750]\n",
      "1278: [D loss: 0.703399, acc: 0.507812]  [A loss: 0.992852, acc: 0.039062]\n",
      "1279: [D loss: 0.677330, acc: 0.574219]  [A loss: 0.696415, acc: 0.492188]\n",
      "1280: [D loss: 0.707840, acc: 0.505859]  [A loss: 0.903186, acc: 0.078125]\n",
      "1281: [D loss: 0.678413, acc: 0.583984]  [A loss: 0.758738, acc: 0.339844]\n",
      "1282: [D loss: 0.697173, acc: 0.542969]  [A loss: 0.877774, acc: 0.132812]\n",
      "1283: [D loss: 0.693940, acc: 0.529297]  [A loss: 0.779878, acc: 0.269531]\n",
      "1284: [D loss: 0.709333, acc: 0.496094]  [A loss: 0.954900, acc: 0.074219]\n",
      "1285: [D loss: 0.684024, acc: 0.537109]  [A loss: 0.736553, acc: 0.414062]\n",
      "1286: [D loss: 0.697619, acc: 0.548828]  [A loss: 0.887234, acc: 0.101562]\n",
      "1287: [D loss: 0.685856, acc: 0.552734]  [A loss: 0.808040, acc: 0.261719]\n",
      "1288: [D loss: 0.699378, acc: 0.529297]  [A loss: 0.863269, acc: 0.132812]\n",
      "1289: [D loss: 0.695429, acc: 0.515625]  [A loss: 0.852026, acc: 0.183594]\n",
      "1290: [D loss: 0.690719, acc: 0.554688]  [A loss: 0.881707, acc: 0.132812]\n",
      "1291: [D loss: 0.678591, acc: 0.568359]  [A loss: 0.762434, acc: 0.320312]\n",
      "1292: [D loss: 0.699447, acc: 0.525391]  [A loss: 0.894244, acc: 0.113281]\n",
      "1293: [D loss: 0.681275, acc: 0.539062]  [A loss: 0.839820, acc: 0.187500]\n",
      "1294: [D loss: 0.696564, acc: 0.556641]  [A loss: 0.846824, acc: 0.171875]\n",
      "1295: [D loss: 0.688334, acc: 0.576172]  [A loss: 0.847059, acc: 0.171875]\n",
      "1296: [D loss: 0.680651, acc: 0.544922]  [A loss: 0.916044, acc: 0.113281]\n",
      "1297: [D loss: 0.672945, acc: 0.595703]  [A loss: 0.757297, acc: 0.378906]\n",
      "1298: [D loss: 0.713767, acc: 0.513672]  [A loss: 0.963730, acc: 0.101562]\n",
      "1299: [D loss: 0.676427, acc: 0.583984]  [A loss: 0.817261, acc: 0.242188]\n",
      "1300: [D loss: 0.701193, acc: 0.519531]  [A loss: 1.017412, acc: 0.062500]\n",
      "1301: [D loss: 0.692666, acc: 0.533203]  [A loss: 0.742706, acc: 0.394531]\n",
      "1302: [D loss: 0.711703, acc: 0.503906]  [A loss: 1.038905, acc: 0.054688]\n",
      "1303: [D loss: 0.704136, acc: 0.521484]  [A loss: 0.716798, acc: 0.500000]\n",
      "1304: [D loss: 0.700866, acc: 0.537109]  [A loss: 0.953378, acc: 0.078125]\n",
      "1305: [D loss: 0.676629, acc: 0.558594]  [A loss: 0.717687, acc: 0.441406]\n",
      "1306: [D loss: 0.710651, acc: 0.521484]  [A loss: 0.973632, acc: 0.058594]\n",
      "1307: [D loss: 0.674424, acc: 0.564453]  [A loss: 0.776776, acc: 0.300781]\n",
      "1308: [D loss: 0.690712, acc: 0.554688]  [A loss: 0.904697, acc: 0.101562]\n",
      "1309: [D loss: 0.683449, acc: 0.546875]  [A loss: 0.827275, acc: 0.234375]\n",
      "1310: [D loss: 0.704575, acc: 0.521484]  [A loss: 0.825148, acc: 0.210938]\n",
      "1311: [D loss: 0.680855, acc: 0.576172]  [A loss: 0.870620, acc: 0.125000]\n",
      "1312: [D loss: 0.682412, acc: 0.564453]  [A loss: 0.825369, acc: 0.246094]\n",
      "1313: [D loss: 0.684047, acc: 0.554688]  [A loss: 0.857580, acc: 0.160156]\n",
      "1314: [D loss: 0.685619, acc: 0.535156]  [A loss: 0.825061, acc: 0.199219]\n",
      "1315: [D loss: 0.680418, acc: 0.564453]  [A loss: 0.839733, acc: 0.226562]\n",
      "1316: [D loss: 0.696626, acc: 0.515625]  [A loss: 0.850207, acc: 0.203125]\n",
      "1317: [D loss: 0.691096, acc: 0.546875]  [A loss: 0.874355, acc: 0.121094]\n",
      "1318: [D loss: 0.681747, acc: 0.601562]  [A loss: 0.917863, acc: 0.121094]\n",
      "1319: [D loss: 0.680310, acc: 0.548828]  [A loss: 0.833686, acc: 0.218750]\n",
      "1320: [D loss: 0.694714, acc: 0.535156]  [A loss: 0.863951, acc: 0.171875]\n",
      "1321: [D loss: 0.674099, acc: 0.607422]  [A loss: 0.930640, acc: 0.105469]\n",
      "1322: [D loss: 0.695246, acc: 0.541016]  [A loss: 0.797300, acc: 0.273438]\n",
      "1323: [D loss: 0.705503, acc: 0.515625]  [A loss: 1.011994, acc: 0.062500]\n",
      "1324: [D loss: 0.690975, acc: 0.554688]  [A loss: 0.704687, acc: 0.453125]\n",
      "1325: [D loss: 0.699065, acc: 0.529297]  [A loss: 0.991711, acc: 0.093750]\n",
      "1326: [D loss: 0.692432, acc: 0.537109]  [A loss: 0.661772, acc: 0.617188]\n",
      "1327: [D loss: 0.739564, acc: 0.513672]  [A loss: 1.036368, acc: 0.070312]\n",
      "1328: [D loss: 0.686916, acc: 0.544922]  [A loss: 0.737583, acc: 0.410156]\n",
      "1329: [D loss: 0.705837, acc: 0.515625]  [A loss: 0.876314, acc: 0.125000]\n",
      "1330: [D loss: 0.692186, acc: 0.525391]  [A loss: 0.775887, acc: 0.304688]\n",
      "1331: [D loss: 0.685311, acc: 0.548828]  [A loss: 0.838504, acc: 0.199219]\n",
      "1332: [D loss: 0.692718, acc: 0.541016]  [A loss: 0.805915, acc: 0.257812]\n",
      "1333: [D loss: 0.695468, acc: 0.525391]  [A loss: 0.877770, acc: 0.136719]\n",
      "1334: [D loss: 0.681005, acc: 0.556641]  [A loss: 0.790794, acc: 0.269531]\n",
      "1335: [D loss: 0.700079, acc: 0.521484]  [A loss: 0.906660, acc: 0.121094]\n",
      "1336: [D loss: 0.681352, acc: 0.539062]  [A loss: 0.774185, acc: 0.308594]\n",
      "1337: [D loss: 0.694471, acc: 0.531250]  [A loss: 0.953124, acc: 0.097656]\n",
      "1338: [D loss: 0.676631, acc: 0.566406]  [A loss: 0.741329, acc: 0.468750]\n",
      "1339: [D loss: 0.711831, acc: 0.519531]  [A loss: 1.005190, acc: 0.054688]\n",
      "1340: [D loss: 0.681812, acc: 0.562500]  [A loss: 0.698604, acc: 0.531250]\n",
      "1341: [D loss: 0.711212, acc: 0.505859]  [A loss: 0.954908, acc: 0.078125]\n",
      "1342: [D loss: 0.688534, acc: 0.554688]  [A loss: 0.740097, acc: 0.410156]\n",
      "1343: [D loss: 0.720301, acc: 0.519531]  [A loss: 0.935653, acc: 0.109375]\n",
      "1344: [D loss: 0.691932, acc: 0.539062]  [A loss: 0.768808, acc: 0.343750]\n",
      "1345: [D loss: 0.692844, acc: 0.523438]  [A loss: 0.876024, acc: 0.140625]\n",
      "1346: [D loss: 0.685929, acc: 0.541016]  [A loss: 0.763143, acc: 0.351562]\n",
      "1347: [D loss: 0.702592, acc: 0.501953]  [A loss: 0.866900, acc: 0.214844]\n",
      "1348: [D loss: 0.682443, acc: 0.568359]  [A loss: 0.766610, acc: 0.339844]\n",
      "1349: [D loss: 0.707623, acc: 0.505859]  [A loss: 0.937274, acc: 0.121094]\n",
      "1350: [D loss: 0.705938, acc: 0.484375]  [A loss: 0.768586, acc: 0.335938]\n",
      "1351: [D loss: 0.698262, acc: 0.548828]  [A loss: 0.882559, acc: 0.179688]\n",
      "1352: [D loss: 0.683287, acc: 0.568359]  [A loss: 0.770101, acc: 0.328125]\n",
      "1353: [D loss: 0.696133, acc: 0.556641]  [A loss: 0.889581, acc: 0.140625]\n",
      "1354: [D loss: 0.697990, acc: 0.507812]  [A loss: 0.742069, acc: 0.406250]\n",
      "1355: [D loss: 0.718247, acc: 0.503906]  [A loss: 0.936451, acc: 0.105469]\n",
      "1356: [D loss: 0.688347, acc: 0.525391]  [A loss: 0.747802, acc: 0.394531]\n",
      "1357: [D loss: 0.691098, acc: 0.531250]  [A loss: 0.890088, acc: 0.101562]\n",
      "1358: [D loss: 0.696643, acc: 0.525391]  [A loss: 0.772062, acc: 0.308594]\n",
      "1359: [D loss: 0.681960, acc: 0.550781]  [A loss: 0.845442, acc: 0.195312]\n",
      "1360: [D loss: 0.684400, acc: 0.558594]  [A loss: 0.782396, acc: 0.320312]\n",
      "1361: [D loss: 0.710405, acc: 0.498047]  [A loss: 0.939396, acc: 0.093750]\n",
      "1362: [D loss: 0.694530, acc: 0.546875]  [A loss: 0.736571, acc: 0.445312]\n",
      "1363: [D loss: 0.705329, acc: 0.541016]  [A loss: 0.952998, acc: 0.054688]\n",
      "1364: [D loss: 0.683339, acc: 0.554688]  [A loss: 0.755182, acc: 0.359375]\n",
      "1365: [D loss: 0.685136, acc: 0.556641]  [A loss: 0.882899, acc: 0.117188]\n",
      "1366: [D loss: 0.676663, acc: 0.548828]  [A loss: 0.800707, acc: 0.250000]\n",
      "1367: [D loss: 0.679877, acc: 0.566406]  [A loss: 0.831684, acc: 0.210938]\n",
      "1368: [D loss: 0.687316, acc: 0.542969]  [A loss: 0.878874, acc: 0.175781]\n",
      "1369: [D loss: 0.682820, acc: 0.572266]  [A loss: 0.789834, acc: 0.253906]\n",
      "1370: [D loss: 0.705268, acc: 0.513672]  [A loss: 0.835195, acc: 0.214844]\n",
      "1371: [D loss: 0.681364, acc: 0.564453]  [A loss: 0.898110, acc: 0.140625]\n",
      "1372: [D loss: 0.692239, acc: 0.521484]  [A loss: 0.763718, acc: 0.328125]\n",
      "1373: [D loss: 0.696499, acc: 0.548828]  [A loss: 0.972896, acc: 0.062500]\n",
      "1374: [D loss: 0.685007, acc: 0.548828]  [A loss: 0.704429, acc: 0.488281]\n",
      "1375: [D loss: 0.705135, acc: 0.525391]  [A loss: 0.989216, acc: 0.023438]\n",
      "1376: [D loss: 0.676790, acc: 0.583984]  [A loss: 0.723348, acc: 0.449219]\n",
      "1377: [D loss: 0.703331, acc: 0.529297]  [A loss: 0.914444, acc: 0.082031]\n",
      "1378: [D loss: 0.683103, acc: 0.556641]  [A loss: 0.741266, acc: 0.429688]\n",
      "1379: [D loss: 0.728638, acc: 0.503906]  [A loss: 0.994077, acc: 0.042969]\n",
      "1380: [D loss: 0.672620, acc: 0.574219]  [A loss: 0.731598, acc: 0.406250]\n",
      "1381: [D loss: 0.697652, acc: 0.537109]  [A loss: 0.872151, acc: 0.140625]\n",
      "1382: [D loss: 0.687481, acc: 0.527344]  [A loss: 0.851327, acc: 0.160156]\n",
      "1383: [D loss: 0.690007, acc: 0.541016]  [A loss: 0.842078, acc: 0.187500]\n",
      "1384: [D loss: 0.685175, acc: 0.525391]  [A loss: 0.822424, acc: 0.265625]\n",
      "1385: [D loss: 0.712475, acc: 0.511719]  [A loss: 0.865114, acc: 0.187500]\n",
      "1386: [D loss: 0.695007, acc: 0.521484]  [A loss: 0.892254, acc: 0.117188]\n",
      "1387: [D loss: 0.687350, acc: 0.562500]  [A loss: 0.762458, acc: 0.339844]\n",
      "1388: [D loss: 0.687682, acc: 0.546875]  [A loss: 0.969307, acc: 0.070312]\n",
      "1389: [D loss: 0.676180, acc: 0.570312]  [A loss: 0.759110, acc: 0.339844]\n",
      "1390: [D loss: 0.705730, acc: 0.503906]  [A loss: 1.011514, acc: 0.085938]\n",
      "1391: [D loss: 0.684069, acc: 0.578125]  [A loss: 0.735087, acc: 0.453125]\n",
      "1392: [D loss: 0.706720, acc: 0.529297]  [A loss: 0.921397, acc: 0.093750]\n",
      "1393: [D loss: 0.667274, acc: 0.603516]  [A loss: 0.752461, acc: 0.347656]\n",
      "1394: [D loss: 0.699539, acc: 0.529297]  [A loss: 0.921538, acc: 0.128906]\n",
      "1395: [D loss: 0.689888, acc: 0.535156]  [A loss: 0.762372, acc: 0.347656]\n",
      "1396: [D loss: 0.709527, acc: 0.509766]  [A loss: 0.905342, acc: 0.144531]\n",
      "1397: [D loss: 0.678292, acc: 0.580078]  [A loss: 0.842805, acc: 0.210938]\n",
      "1398: [D loss: 0.688304, acc: 0.554688]  [A loss: 0.805847, acc: 0.253906]\n",
      "1399: [D loss: 0.679445, acc: 0.583984]  [A loss: 0.845555, acc: 0.218750]\n",
      "1400: [D loss: 0.675546, acc: 0.556641]  [A loss: 0.840931, acc: 0.250000]\n",
      "1401: [D loss: 0.687027, acc: 0.544922]  [A loss: 0.860748, acc: 0.187500]\n",
      "1402: [D loss: 0.699979, acc: 0.515625]  [A loss: 0.901269, acc: 0.156250]\n",
      "1403: [D loss: 0.699563, acc: 0.507812]  [A loss: 0.785164, acc: 0.300781]\n",
      "1404: [D loss: 0.691110, acc: 0.560547]  [A loss: 0.890914, acc: 0.128906]\n",
      "1405: [D loss: 0.682157, acc: 0.546875]  [A loss: 0.827936, acc: 0.261719]\n",
      "1406: [D loss: 0.702950, acc: 0.501953]  [A loss: 0.948267, acc: 0.097656]\n",
      "1407: [D loss: 0.679808, acc: 0.558594]  [A loss: 0.783035, acc: 0.292969]\n",
      "1408: [D loss: 0.697811, acc: 0.541016]  [A loss: 0.890832, acc: 0.152344]\n",
      "1409: [D loss: 0.686795, acc: 0.570312]  [A loss: 0.917563, acc: 0.125000]\n",
      "1410: [D loss: 0.689671, acc: 0.533203]  [A loss: 0.789198, acc: 0.316406]\n",
      "1411: [D loss: 0.704027, acc: 0.515625]  [A loss: 0.969314, acc: 0.050781]\n",
      "1412: [D loss: 0.677259, acc: 0.548828]  [A loss: 0.734622, acc: 0.417969]\n",
      "1413: [D loss: 0.709043, acc: 0.503906]  [A loss: 1.006227, acc: 0.058594]\n",
      "1414: [D loss: 0.683973, acc: 0.554688]  [A loss: 0.633962, acc: 0.652344]\n",
      "1415: [D loss: 0.744328, acc: 0.503906]  [A loss: 1.031365, acc: 0.058594]\n",
      "1416: [D loss: 0.702558, acc: 0.515625]  [A loss: 0.817918, acc: 0.207031]\n",
      "1417: [D loss: 0.688553, acc: 0.558594]  [A loss: 0.851340, acc: 0.187500]\n",
      "1418: [D loss: 0.693097, acc: 0.529297]  [A loss: 0.768586, acc: 0.367188]\n",
      "1419: [D loss: 0.706108, acc: 0.521484]  [A loss: 0.852430, acc: 0.175781]\n",
      "1420: [D loss: 0.688879, acc: 0.531250]  [A loss: 0.812170, acc: 0.253906]\n",
      "1421: [D loss: 0.714465, acc: 0.490234]  [A loss: 0.908808, acc: 0.082031]\n",
      "1422: [D loss: 0.677819, acc: 0.556641]  [A loss: 0.781715, acc: 0.289062]\n",
      "1423: [D loss: 0.701321, acc: 0.517578]  [A loss: 0.891420, acc: 0.140625]\n",
      "1424: [D loss: 0.687259, acc: 0.546875]  [A loss: 0.795760, acc: 0.277344]\n",
      "1425: [D loss: 0.687588, acc: 0.548828]  [A loss: 0.882786, acc: 0.191406]\n",
      "1426: [D loss: 0.691284, acc: 0.533203]  [A loss: 0.833345, acc: 0.250000]\n",
      "1427: [D loss: 0.680856, acc: 0.572266]  [A loss: 0.837059, acc: 0.183594]\n",
      "1428: [D loss: 0.698178, acc: 0.525391]  [A loss: 0.919785, acc: 0.089844]\n",
      "1429: [D loss: 0.684100, acc: 0.548828]  [A loss: 0.736738, acc: 0.414062]\n",
      "1430: [D loss: 0.690369, acc: 0.550781]  [A loss: 0.945085, acc: 0.101562]\n",
      "1431: [D loss: 0.681393, acc: 0.531250]  [A loss: 0.783834, acc: 0.339844]\n",
      "1432: [D loss: 0.690395, acc: 0.535156]  [A loss: 0.868820, acc: 0.179688]\n",
      "1433: [D loss: 0.700507, acc: 0.521484]  [A loss: 0.826465, acc: 0.250000]\n",
      "1434: [D loss: 0.689041, acc: 0.564453]  [A loss: 0.813338, acc: 0.269531]\n",
      "1435: [D loss: 0.692931, acc: 0.539062]  [A loss: 0.827005, acc: 0.230469]\n",
      "1436: [D loss: 0.691473, acc: 0.550781]  [A loss: 1.035031, acc: 0.054688]\n",
      "1437: [D loss: 0.689287, acc: 0.529297]  [A loss: 0.669142, acc: 0.597656]\n",
      "1438: [D loss: 0.709138, acc: 0.529297]  [A loss: 1.006009, acc: 0.042969]\n",
      "1439: [D loss: 0.679615, acc: 0.576172]  [A loss: 0.669351, acc: 0.578125]\n",
      "1440: [D loss: 0.700731, acc: 0.527344]  [A loss: 0.889783, acc: 0.132812]\n",
      "1441: [D loss: 0.703327, acc: 0.509766]  [A loss: 0.770778, acc: 0.300781]\n",
      "1442: [D loss: 0.712321, acc: 0.498047]  [A loss: 0.967452, acc: 0.058594]\n",
      "1443: [D loss: 0.694274, acc: 0.544922]  [A loss: 0.705822, acc: 0.500000]\n",
      "1444: [D loss: 0.723944, acc: 0.492188]  [A loss: 0.915036, acc: 0.121094]\n",
      "1445: [D loss: 0.692070, acc: 0.541016]  [A loss: 0.751339, acc: 0.332031]\n",
      "1446: [D loss: 0.700756, acc: 0.519531]  [A loss: 0.939110, acc: 0.089844]\n",
      "1447: [D loss: 0.688785, acc: 0.525391]  [A loss: 0.746362, acc: 0.378906]\n",
      "1448: [D loss: 0.694779, acc: 0.515625]  [A loss: 0.868276, acc: 0.195312]\n",
      "1449: [D loss: 0.680946, acc: 0.566406]  [A loss: 0.783348, acc: 0.300781]\n",
      "1450: [D loss: 0.699096, acc: 0.541016]  [A loss: 0.873428, acc: 0.148438]\n",
      "1451: [D loss: 0.686126, acc: 0.550781]  [A loss: 0.749207, acc: 0.359375]\n",
      "1452: [D loss: 0.695520, acc: 0.523438]  [A loss: 0.893817, acc: 0.117188]\n",
      "1453: [D loss: 0.687767, acc: 0.564453]  [A loss: 0.804824, acc: 0.281250]\n",
      "1454: [D loss: 0.694028, acc: 0.509766]  [A loss: 0.876326, acc: 0.160156]\n",
      "1455: [D loss: 0.686168, acc: 0.564453]  [A loss: 0.774427, acc: 0.351562]\n",
      "1456: [D loss: 0.689279, acc: 0.576172]  [A loss: 0.849071, acc: 0.207031]\n",
      "1457: [D loss: 0.688022, acc: 0.554688]  [A loss: 0.875260, acc: 0.160156]\n",
      "1458: [D loss: 0.690800, acc: 0.541016]  [A loss: 0.779150, acc: 0.316406]\n",
      "1459: [D loss: 0.692122, acc: 0.519531]  [A loss: 0.950613, acc: 0.089844]\n",
      "1460: [D loss: 0.678745, acc: 0.556641]  [A loss: 0.727878, acc: 0.445312]\n",
      "1461: [D loss: 0.707545, acc: 0.525391]  [A loss: 0.926480, acc: 0.113281]\n",
      "1462: [D loss: 0.693944, acc: 0.529297]  [A loss: 0.814489, acc: 0.253906]\n",
      "1463: [D loss: 0.697081, acc: 0.544922]  [A loss: 0.922102, acc: 0.136719]\n",
      "1464: [D loss: 0.687196, acc: 0.554688]  [A loss: 0.768925, acc: 0.285156]\n",
      "1465: [D loss: 0.721432, acc: 0.480469]  [A loss: 0.910114, acc: 0.105469]\n",
      "1466: [D loss: 0.703006, acc: 0.494141]  [A loss: 0.886647, acc: 0.132812]\n",
      "1467: [D loss: 0.688805, acc: 0.525391]  [A loss: 0.789393, acc: 0.316406]\n",
      "1468: [D loss: 0.703084, acc: 0.535156]  [A loss: 0.873144, acc: 0.132812]\n",
      "1469: [D loss: 0.681735, acc: 0.576172]  [A loss: 0.908050, acc: 0.132812]\n",
      "1470: [D loss: 0.681520, acc: 0.548828]  [A loss: 0.752782, acc: 0.371094]\n",
      "1471: [D loss: 0.696760, acc: 0.527344]  [A loss: 0.984458, acc: 0.070312]\n",
      "1472: [D loss: 0.695375, acc: 0.537109]  [A loss: 0.721439, acc: 0.457031]\n",
      "1473: [D loss: 0.699664, acc: 0.529297]  [A loss: 0.944887, acc: 0.089844]\n",
      "1474: [D loss: 0.686911, acc: 0.554688]  [A loss: 0.821579, acc: 0.257812]\n",
      "1475: [D loss: 0.687601, acc: 0.552734]  [A loss: 0.963005, acc: 0.097656]\n",
      "1476: [D loss: 0.685145, acc: 0.556641]  [A loss: 0.722346, acc: 0.472656]\n",
      "1477: [D loss: 0.690959, acc: 0.523438]  [A loss: 0.928674, acc: 0.113281]\n",
      "1478: [D loss: 0.695582, acc: 0.529297]  [A loss: 0.763811, acc: 0.304688]\n",
      "1479: [D loss: 0.695447, acc: 0.544922]  [A loss: 0.896717, acc: 0.109375]\n",
      "1480: [D loss: 0.689803, acc: 0.523438]  [A loss: 0.800252, acc: 0.273438]\n",
      "1481: [D loss: 0.708010, acc: 0.509766]  [A loss: 0.930420, acc: 0.121094]\n",
      "1482: [D loss: 0.705604, acc: 0.533203]  [A loss: 0.797023, acc: 0.253906]\n",
      "1483: [D loss: 0.688720, acc: 0.558594]  [A loss: 0.878344, acc: 0.167969]\n",
      "1484: [D loss: 0.673556, acc: 0.583984]  [A loss: 0.773743, acc: 0.351562]\n",
      "1485: [D loss: 0.702753, acc: 0.523438]  [A loss: 0.920148, acc: 0.105469]\n",
      "1486: [D loss: 0.694670, acc: 0.548828]  [A loss: 0.764005, acc: 0.351562]\n",
      "1487: [D loss: 0.697561, acc: 0.533203]  [A loss: 0.938084, acc: 0.097656]\n",
      "1488: [D loss: 0.693346, acc: 0.546875]  [A loss: 0.778530, acc: 0.308594]\n",
      "1489: [D loss: 0.703591, acc: 0.515625]  [A loss: 0.968765, acc: 0.078125]\n",
      "1490: [D loss: 0.667874, acc: 0.613281]  [A loss: 0.729347, acc: 0.410156]\n",
      "1491: [D loss: 0.701386, acc: 0.529297]  [A loss: 0.970308, acc: 0.058594]\n",
      "1492: [D loss: 0.683355, acc: 0.558594]  [A loss: 0.733245, acc: 0.406250]\n",
      "1493: [D loss: 0.702708, acc: 0.523438]  [A loss: 0.985873, acc: 0.050781]\n",
      "1494: [D loss: 0.691652, acc: 0.533203]  [A loss: 0.706913, acc: 0.492188]\n",
      "1495: [D loss: 0.719065, acc: 0.486328]  [A loss: 1.010103, acc: 0.027344]\n",
      "1496: [D loss: 0.682134, acc: 0.546875]  [A loss: 0.740555, acc: 0.437500]\n",
      "1497: [D loss: 0.709176, acc: 0.513672]  [A loss: 0.910447, acc: 0.128906]\n",
      "1498: [D loss: 0.691051, acc: 0.531250]  [A loss: 0.710633, acc: 0.460938]\n",
      "1499: [D loss: 0.711619, acc: 0.529297]  [A loss: 0.928474, acc: 0.117188]\n",
      "1500: [D loss: 0.680121, acc: 0.552734]  [A loss: 0.823598, acc: 0.250000]\n",
      "1501: [D loss: 0.700410, acc: 0.521484]  [A loss: 0.903562, acc: 0.125000]\n",
      "1502: [D loss: 0.689921, acc: 0.548828]  [A loss: 0.758291, acc: 0.359375]\n",
      "1503: [D loss: 0.704350, acc: 0.519531]  [A loss: 0.929164, acc: 0.089844]\n",
      "1504: [D loss: 0.678973, acc: 0.582031]  [A loss: 0.720118, acc: 0.453125]\n",
      "1505: [D loss: 0.698441, acc: 0.529297]  [A loss: 0.956674, acc: 0.093750]\n",
      "1506: [D loss: 0.679515, acc: 0.562500]  [A loss: 0.725450, acc: 0.382812]\n",
      "1507: [D loss: 0.714642, acc: 0.505859]  [A loss: 0.837526, acc: 0.191406]\n",
      "1508: [D loss: 0.695230, acc: 0.541016]  [A loss: 0.941723, acc: 0.062500]\n",
      "1509: [D loss: 0.686866, acc: 0.554688]  [A loss: 0.723329, acc: 0.437500]\n",
      "1510: [D loss: 0.689940, acc: 0.523438]  [A loss: 0.915243, acc: 0.128906]\n",
      "1511: [D loss: 0.687284, acc: 0.558594]  [A loss: 0.770997, acc: 0.292969]\n",
      "1512: [D loss: 0.705977, acc: 0.521484]  [A loss: 0.847282, acc: 0.207031]\n",
      "1513: [D loss: 0.691210, acc: 0.533203]  [A loss: 0.749854, acc: 0.386719]\n",
      "1514: [D loss: 0.714494, acc: 0.505859]  [A loss: 0.892380, acc: 0.152344]\n",
      "1515: [D loss: 0.672582, acc: 0.574219]  [A loss: 0.745531, acc: 0.398438]\n",
      "1516: [D loss: 0.717447, acc: 0.503906]  [A loss: 0.856138, acc: 0.175781]\n",
      "1517: [D loss: 0.678814, acc: 0.578125]  [A loss: 0.829274, acc: 0.187500]\n",
      "1518: [D loss: 0.710459, acc: 0.488281]  [A loss: 0.847189, acc: 0.175781]\n",
      "1519: [D loss: 0.695579, acc: 0.515625]  [A loss: 0.834423, acc: 0.156250]\n",
      "1520: [D loss: 0.696294, acc: 0.501953]  [A loss: 0.923669, acc: 0.085938]\n",
      "1521: [D loss: 0.684940, acc: 0.517578]  [A loss: 0.793954, acc: 0.300781]\n",
      "1522: [D loss: 0.668115, acc: 0.572266]  [A loss: 0.855666, acc: 0.167969]\n",
      "1523: [D loss: 0.688888, acc: 0.523438]  [A loss: 0.835127, acc: 0.199219]\n",
      "1524: [D loss: 0.699107, acc: 0.521484]  [A loss: 0.903040, acc: 0.125000]\n",
      "1525: [D loss: 0.687467, acc: 0.554688]  [A loss: 0.796302, acc: 0.281250]\n",
      "1526: [D loss: 0.699391, acc: 0.533203]  [A loss: 0.946598, acc: 0.070312]\n",
      "1527: [D loss: 0.686850, acc: 0.548828]  [A loss: 0.717344, acc: 0.464844]\n",
      "1528: [D loss: 0.707543, acc: 0.509766]  [A loss: 0.947500, acc: 0.136719]\n",
      "1529: [D loss: 0.694483, acc: 0.548828]  [A loss: 0.694226, acc: 0.503906]\n",
      "1530: [D loss: 0.724312, acc: 0.529297]  [A loss: 0.986716, acc: 0.085938]\n",
      "1531: [D loss: 0.720307, acc: 0.474609]  [A loss: 0.801628, acc: 0.242188]\n",
      "1532: [D loss: 0.683619, acc: 0.552734]  [A loss: 0.890267, acc: 0.121094]\n",
      "1533: [D loss: 0.686355, acc: 0.558594]  [A loss: 0.777700, acc: 0.312500]\n",
      "1534: [D loss: 0.693130, acc: 0.552734]  [A loss: 0.893601, acc: 0.132812]\n",
      "1535: [D loss: 0.677132, acc: 0.570312]  [A loss: 0.768576, acc: 0.328125]\n",
      "1536: [D loss: 0.696137, acc: 0.535156]  [A loss: 0.918073, acc: 0.164062]\n",
      "1537: [D loss: 0.690111, acc: 0.556641]  [A loss: 0.762589, acc: 0.328125]\n",
      "1538: [D loss: 0.690625, acc: 0.560547]  [A loss: 0.859662, acc: 0.164062]\n",
      "1539: [D loss: 0.693356, acc: 0.519531]  [A loss: 0.896517, acc: 0.121094]\n",
      "1540: [D loss: 0.674528, acc: 0.599609]  [A loss: 0.720412, acc: 0.441406]\n",
      "1541: [D loss: 0.706992, acc: 0.533203]  [A loss: 1.002058, acc: 0.039062]\n",
      "1542: [D loss: 0.683741, acc: 0.562500]  [A loss: 0.716651, acc: 0.472656]\n",
      "1543: [D loss: 0.720222, acc: 0.492188]  [A loss: 0.984534, acc: 0.058594]\n",
      "1544: [D loss: 0.700386, acc: 0.521484]  [A loss: 0.706879, acc: 0.503906]\n",
      "1545: [D loss: 0.709132, acc: 0.517578]  [A loss: 0.897281, acc: 0.089844]\n",
      "1546: [D loss: 0.675334, acc: 0.580078]  [A loss: 0.732546, acc: 0.425781]\n",
      "1547: [D loss: 0.714303, acc: 0.498047]  [A loss: 0.889092, acc: 0.105469]\n",
      "1548: [D loss: 0.695379, acc: 0.533203]  [A loss: 0.786449, acc: 0.277344]\n",
      "1549: [D loss: 0.688745, acc: 0.541016]  [A loss: 0.819351, acc: 0.195312]\n",
      "1550: [D loss: 0.703726, acc: 0.525391]  [A loss: 0.809602, acc: 0.292969]\n",
      "1551: [D loss: 0.704713, acc: 0.515625]  [A loss: 0.837035, acc: 0.183594]\n",
      "1552: [D loss: 0.698244, acc: 0.517578]  [A loss: 0.779817, acc: 0.292969]\n",
      "1553: [D loss: 0.694157, acc: 0.542969]  [A loss: 0.859421, acc: 0.171875]\n",
      "1554: [D loss: 0.702767, acc: 0.529297]  [A loss: 0.836737, acc: 0.199219]\n",
      "1555: [D loss: 0.695018, acc: 0.544922]  [A loss: 0.878787, acc: 0.171875]\n",
      "1556: [D loss: 0.695230, acc: 0.576172]  [A loss: 0.767413, acc: 0.335938]\n",
      "1557: [D loss: 0.696660, acc: 0.515625]  [A loss: 0.931743, acc: 0.093750]\n",
      "1558: [D loss: 0.709339, acc: 0.490234]  [A loss: 0.792897, acc: 0.277344]\n",
      "1559: [D loss: 0.698997, acc: 0.542969]  [A loss: 0.877417, acc: 0.152344]\n",
      "1560: [D loss: 0.693342, acc: 0.523438]  [A loss: 0.799019, acc: 0.257812]\n",
      "1561: [D loss: 0.700965, acc: 0.546875]  [A loss: 0.887628, acc: 0.117188]\n",
      "1562: [D loss: 0.684629, acc: 0.535156]  [A loss: 0.774214, acc: 0.343750]\n",
      "1563: [D loss: 0.688538, acc: 0.537109]  [A loss: 0.918411, acc: 0.089844]\n",
      "1564: [D loss: 0.688799, acc: 0.548828]  [A loss: 0.755623, acc: 0.371094]\n",
      "1565: [D loss: 0.704054, acc: 0.515625]  [A loss: 0.871818, acc: 0.156250]\n",
      "1566: [D loss: 0.686587, acc: 0.558594]  [A loss: 0.794403, acc: 0.292969]\n",
      "1567: [D loss: 0.689159, acc: 0.521484]  [A loss: 0.924964, acc: 0.093750]\n",
      "1568: [D loss: 0.691353, acc: 0.537109]  [A loss: 0.719910, acc: 0.453125]\n",
      "1569: [D loss: 0.715235, acc: 0.523438]  [A loss: 0.909957, acc: 0.101562]\n",
      "1570: [D loss: 0.686993, acc: 0.533203]  [A loss: 0.738649, acc: 0.398438]\n",
      "1571: [D loss: 0.711042, acc: 0.517578]  [A loss: 0.934257, acc: 0.093750]\n",
      "1572: [D loss: 0.707352, acc: 0.548828]  [A loss: 0.778018, acc: 0.308594]\n",
      "1573: [D loss: 0.701279, acc: 0.519531]  [A loss: 0.953678, acc: 0.066406]\n",
      "1574: [D loss: 0.677436, acc: 0.568359]  [A loss: 0.781058, acc: 0.328125]\n",
      "1575: [D loss: 0.708358, acc: 0.521484]  [A loss: 0.919167, acc: 0.140625]\n",
      "1576: [D loss: 0.693336, acc: 0.539062]  [A loss: 0.695726, acc: 0.542969]\n",
      "1577: [D loss: 0.713938, acc: 0.517578]  [A loss: 0.909774, acc: 0.113281]\n",
      "1578: [D loss: 0.676666, acc: 0.560547]  [A loss: 0.716611, acc: 0.429688]\n",
      "1579: [D loss: 0.711605, acc: 0.535156]  [A loss: 0.976789, acc: 0.070312]\n",
      "1580: [D loss: 0.683926, acc: 0.541016]  [A loss: 0.749632, acc: 0.398438]\n",
      "1581: [D loss: 0.713006, acc: 0.535156]  [A loss: 0.880283, acc: 0.121094]\n",
      "1582: [D loss: 0.685260, acc: 0.558594]  [A loss: 0.764828, acc: 0.324219]\n",
      "1583: [D loss: 0.687670, acc: 0.556641]  [A loss: 0.852518, acc: 0.183594]\n",
      "1584: [D loss: 0.690593, acc: 0.541016]  [A loss: 0.856266, acc: 0.160156]\n",
      "1585: [D loss: 0.681164, acc: 0.591797]  [A loss: 0.879632, acc: 0.156250]\n",
      "1586: [D loss: 0.676786, acc: 0.570312]  [A loss: 0.789465, acc: 0.292969]\n",
      "1587: [D loss: 0.691974, acc: 0.537109]  [A loss: 0.831654, acc: 0.230469]\n",
      "1588: [D loss: 0.704134, acc: 0.511719]  [A loss: 0.911655, acc: 0.082031]\n",
      "1589: [D loss: 0.674895, acc: 0.599609]  [A loss: 0.789150, acc: 0.281250]\n",
      "1590: [D loss: 0.692237, acc: 0.542969]  [A loss: 0.937060, acc: 0.082031]\n",
      "1591: [D loss: 0.693545, acc: 0.521484]  [A loss: 0.745157, acc: 0.398438]\n",
      "1592: [D loss: 0.717643, acc: 0.490234]  [A loss: 0.846344, acc: 0.164062]\n",
      "1593: [D loss: 0.695332, acc: 0.546875]  [A loss: 0.891932, acc: 0.136719]\n",
      "1594: [D loss: 0.697711, acc: 0.501953]  [A loss: 0.812778, acc: 0.250000]\n",
      "1595: [D loss: 0.694632, acc: 0.552734]  [A loss: 0.918526, acc: 0.093750]\n",
      "1596: [D loss: 0.682733, acc: 0.552734]  [A loss: 0.761653, acc: 0.335938]\n",
      "1597: [D loss: 0.706703, acc: 0.511719]  [A loss: 0.895790, acc: 0.128906]\n",
      "1598: [D loss: 0.689115, acc: 0.546875]  [A loss: 0.746007, acc: 0.359375]\n",
      "1599: [D loss: 0.710371, acc: 0.501953]  [A loss: 0.897062, acc: 0.074219]\n",
      "1600: [D loss: 0.684136, acc: 0.539062]  [A loss: 0.822629, acc: 0.234375]\n",
      "1601: [D loss: 0.692696, acc: 0.542969]  [A loss: 0.960468, acc: 0.078125]\n",
      "1602: [D loss: 0.687739, acc: 0.539062]  [A loss: 0.748280, acc: 0.363281]\n",
      "1603: [D loss: 0.704318, acc: 0.533203]  [A loss: 0.955388, acc: 0.105469]\n",
      "1604: [D loss: 0.702023, acc: 0.509766]  [A loss: 0.779193, acc: 0.347656]\n",
      "1605: [D loss: 0.684064, acc: 0.566406]  [A loss: 0.786173, acc: 0.312500]\n",
      "1606: [D loss: 0.679458, acc: 0.576172]  [A loss: 0.879970, acc: 0.132812]\n",
      "1607: [D loss: 0.699091, acc: 0.533203]  [A loss: 0.832901, acc: 0.226562]\n",
      "1608: [D loss: 0.702668, acc: 0.507812]  [A loss: 0.881559, acc: 0.175781]\n",
      "1609: [D loss: 0.696126, acc: 0.542969]  [A loss: 0.814369, acc: 0.246094]\n",
      "1610: [D loss: 0.691856, acc: 0.560547]  [A loss: 0.881316, acc: 0.167969]\n",
      "1611: [D loss: 0.686565, acc: 0.558594]  [A loss: 0.812589, acc: 0.281250]\n",
      "1612: [D loss: 0.703335, acc: 0.501953]  [A loss: 0.818810, acc: 0.218750]\n",
      "1613: [D loss: 0.695185, acc: 0.523438]  [A loss: 0.932274, acc: 0.066406]\n",
      "1614: [D loss: 0.685936, acc: 0.554688]  [A loss: 0.705643, acc: 0.484375]\n",
      "1615: [D loss: 0.705163, acc: 0.546875]  [A loss: 1.079312, acc: 0.023438]\n",
      "1616: [D loss: 0.702486, acc: 0.513672]  [A loss: 0.708357, acc: 0.480469]\n",
      "1617: [D loss: 0.711996, acc: 0.513672]  [A loss: 0.839318, acc: 0.187500]\n",
      "1618: [D loss: 0.696149, acc: 0.511719]  [A loss: 0.920850, acc: 0.093750]\n",
      "1619: [D loss: 0.684026, acc: 0.566406]  [A loss: 0.747730, acc: 0.394531]\n",
      "1620: [D loss: 0.700590, acc: 0.519531]  [A loss: 0.885263, acc: 0.125000]\n",
      "1621: [D loss: 0.691618, acc: 0.535156]  [A loss: 0.764168, acc: 0.351562]\n",
      "1622: [D loss: 0.699437, acc: 0.533203]  [A loss: 0.902608, acc: 0.105469]\n",
      "1623: [D loss: 0.693672, acc: 0.531250]  [A loss: 0.765505, acc: 0.308594]\n",
      "1624: [D loss: 0.689756, acc: 0.554688]  [A loss: 0.844828, acc: 0.203125]\n",
      "1625: [D loss: 0.695488, acc: 0.500000]  [A loss: 0.756741, acc: 0.355469]\n",
      "1626: [D loss: 0.690064, acc: 0.546875]  [A loss: 0.876452, acc: 0.152344]\n",
      "1627: [D loss: 0.697651, acc: 0.505859]  [A loss: 0.799966, acc: 0.238281]\n",
      "1628: [D loss: 0.703150, acc: 0.542969]  [A loss: 0.951936, acc: 0.082031]\n",
      "1629: [D loss: 0.687626, acc: 0.533203]  [A loss: 0.726216, acc: 0.417969]\n",
      "1630: [D loss: 0.726328, acc: 0.484375]  [A loss: 0.950056, acc: 0.054688]\n",
      "1631: [D loss: 0.696421, acc: 0.535156]  [A loss: 0.724891, acc: 0.425781]\n",
      "1632: [D loss: 0.702846, acc: 0.523438]  [A loss: 0.904447, acc: 0.109375]\n",
      "1633: [D loss: 0.686864, acc: 0.544922]  [A loss: 0.757005, acc: 0.339844]\n",
      "1634: [D loss: 0.702006, acc: 0.513672]  [A loss: 0.900816, acc: 0.113281]\n",
      "1635: [D loss: 0.696234, acc: 0.529297]  [A loss: 0.750050, acc: 0.378906]\n",
      "1636: [D loss: 0.700045, acc: 0.525391]  [A loss: 0.890217, acc: 0.121094]\n",
      "1637: [D loss: 0.681825, acc: 0.568359]  [A loss: 0.742470, acc: 0.378906]\n",
      "1638: [D loss: 0.711866, acc: 0.488281]  [A loss: 0.907611, acc: 0.105469]\n",
      "1639: [D loss: 0.678164, acc: 0.566406]  [A loss: 0.816448, acc: 0.222656]\n",
      "1640: [D loss: 0.703467, acc: 0.535156]  [A loss: 0.774711, acc: 0.269531]\n",
      "1641: [D loss: 0.708172, acc: 0.513672]  [A loss: 0.907822, acc: 0.097656]\n",
      "1642: [D loss: 0.680710, acc: 0.568359]  [A loss: 0.743523, acc: 0.382812]\n",
      "1643: [D loss: 0.692146, acc: 0.541016]  [A loss: 0.901983, acc: 0.140625]\n",
      "1644: [D loss: 0.690952, acc: 0.535156]  [A loss: 0.696690, acc: 0.503906]\n",
      "1645: [D loss: 0.700738, acc: 0.509766]  [A loss: 0.946868, acc: 0.070312]\n",
      "1646: [D loss: 0.712423, acc: 0.460938]  [A loss: 0.814564, acc: 0.238281]\n",
      "1647: [D loss: 0.709096, acc: 0.503906]  [A loss: 0.945610, acc: 0.074219]\n",
      "1648: [D loss: 0.704503, acc: 0.496094]  [A loss: 0.724856, acc: 0.437500]\n",
      "1649: [D loss: 0.721907, acc: 0.503906]  [A loss: 0.923431, acc: 0.089844]\n",
      "1650: [D loss: 0.699456, acc: 0.484375]  [A loss: 0.742264, acc: 0.410156]\n",
      "1651: [D loss: 0.701828, acc: 0.511719]  [A loss: 0.829400, acc: 0.179688]\n",
      "1652: [D loss: 0.696141, acc: 0.537109]  [A loss: 0.851182, acc: 0.144531]\n",
      "1653: [D loss: 0.682361, acc: 0.560547]  [A loss: 0.779075, acc: 0.300781]\n",
      "1654: [D loss: 0.681305, acc: 0.537109]  [A loss: 0.844383, acc: 0.214844]\n",
      "1655: [D loss: 0.701373, acc: 0.521484]  [A loss: 0.860327, acc: 0.164062]\n",
      "1656: [D loss: 0.686202, acc: 0.535156]  [A loss: 0.811709, acc: 0.273438]\n",
      "1657: [D loss: 0.692514, acc: 0.548828]  [A loss: 0.844565, acc: 0.171875]\n",
      "1658: [D loss: 0.699319, acc: 0.517578]  [A loss: 0.802472, acc: 0.277344]\n",
      "1659: [D loss: 0.688689, acc: 0.527344]  [A loss: 0.846544, acc: 0.179688]\n",
      "1660: [D loss: 0.686605, acc: 0.562500]  [A loss: 0.827192, acc: 0.257812]\n",
      "1661: [D loss: 0.696860, acc: 0.531250]  [A loss: 0.820593, acc: 0.242188]\n",
      "1662: [D loss: 0.704598, acc: 0.513672]  [A loss: 0.995440, acc: 0.062500]\n",
      "1663: [D loss: 0.690173, acc: 0.554688]  [A loss: 0.696933, acc: 0.511719]\n",
      "1664: [D loss: 0.717270, acc: 0.509766]  [A loss: 0.922489, acc: 0.101562]\n",
      "1665: [D loss: 0.686076, acc: 0.511719]  [A loss: 0.781674, acc: 0.316406]\n",
      "1666: [D loss: 0.709695, acc: 0.494141]  [A loss: 0.915956, acc: 0.101562]\n",
      "1667: [D loss: 0.686915, acc: 0.568359]  [A loss: 0.869231, acc: 0.207031]\n",
      "1668: [D loss: 0.700144, acc: 0.517578]  [A loss: 0.837149, acc: 0.199219]\n",
      "1669: [D loss: 0.692642, acc: 0.539062]  [A loss: 0.824276, acc: 0.191406]\n",
      "1670: [D loss: 0.686016, acc: 0.568359]  [A loss: 0.809862, acc: 0.265625]\n",
      "1671: [D loss: 0.705478, acc: 0.511719]  [A loss: 0.846254, acc: 0.183594]\n",
      "1672: [D loss: 0.701078, acc: 0.496094]  [A loss: 0.916423, acc: 0.070312]\n",
      "1673: [D loss: 0.689453, acc: 0.554688]  [A loss: 0.746414, acc: 0.375000]\n",
      "1674: [D loss: 0.696450, acc: 0.521484]  [A loss: 0.996203, acc: 0.042969]\n",
      "1675: [D loss: 0.691969, acc: 0.539062]  [A loss: 0.716697, acc: 0.468750]\n",
      "1676: [D loss: 0.704436, acc: 0.525391]  [A loss: 0.914579, acc: 0.093750]\n",
      "1677: [D loss: 0.688078, acc: 0.554688]  [A loss: 0.805420, acc: 0.230469]\n",
      "1678: [D loss: 0.683420, acc: 0.544922]  [A loss: 0.840893, acc: 0.207031]\n",
      "1679: [D loss: 0.699476, acc: 0.513672]  [A loss: 0.801478, acc: 0.273438]\n",
      "1680: [D loss: 0.697014, acc: 0.523438]  [A loss: 1.001055, acc: 0.058594]\n",
      "1681: [D loss: 0.692091, acc: 0.539062]  [A loss: 0.696337, acc: 0.492188]\n",
      "1682: [D loss: 0.722226, acc: 0.519531]  [A loss: 0.953837, acc: 0.078125]\n",
      "1683: [D loss: 0.681019, acc: 0.558594]  [A loss: 0.727121, acc: 0.394531]\n",
      "1684: [D loss: 0.712312, acc: 0.509766]  [A loss: 1.042311, acc: 0.039062]\n",
      "1685: [D loss: 0.695489, acc: 0.539062]  [A loss: 0.724343, acc: 0.410156]\n",
      "1686: [D loss: 0.707873, acc: 0.519531]  [A loss: 0.857624, acc: 0.148438]\n",
      "1687: [D loss: 0.705555, acc: 0.494141]  [A loss: 0.873944, acc: 0.152344]\n",
      "1688: [D loss: 0.695480, acc: 0.535156]  [A loss: 0.778556, acc: 0.316406]\n",
      "1689: [D loss: 0.688748, acc: 0.548828]  [A loss: 0.828609, acc: 0.238281]\n",
      "1690: [D loss: 0.672968, acc: 0.589844]  [A loss: 0.780702, acc: 0.355469]\n",
      "1691: [D loss: 0.680689, acc: 0.562500]  [A loss: 0.790843, acc: 0.324219]\n",
      "1692: [D loss: 0.697305, acc: 0.515625]  [A loss: 0.786808, acc: 0.261719]\n",
      "1693: [D loss: 0.688332, acc: 0.546875]  [A loss: 0.885654, acc: 0.140625]\n",
      "1694: [D loss: 0.673974, acc: 0.587891]  [A loss: 0.767308, acc: 0.347656]\n",
      "1695: [D loss: 0.692756, acc: 0.527344]  [A loss: 0.893557, acc: 0.113281]\n",
      "1696: [D loss: 0.688341, acc: 0.531250]  [A loss: 0.759190, acc: 0.351562]\n",
      "1697: [D loss: 0.690394, acc: 0.521484]  [A loss: 0.835292, acc: 0.195312]\n",
      "1698: [D loss: 0.694501, acc: 0.537109]  [A loss: 0.834098, acc: 0.199219]\n",
      "1699: [D loss: 0.689397, acc: 0.537109]  [A loss: 0.870148, acc: 0.156250]\n",
      "1700: [D loss: 0.689758, acc: 0.558594]  [A loss: 0.768590, acc: 0.316406]\n",
      "1701: [D loss: 0.688507, acc: 0.537109]  [A loss: 0.888240, acc: 0.148438]\n",
      "1702: [D loss: 0.691567, acc: 0.523438]  [A loss: 0.814933, acc: 0.238281]\n",
      "1703: [D loss: 0.698185, acc: 0.509766]  [A loss: 0.876159, acc: 0.105469]\n",
      "1704: [D loss: 0.699336, acc: 0.507812]  [A loss: 0.790006, acc: 0.285156]\n",
      "1705: [D loss: 0.702010, acc: 0.511719]  [A loss: 0.907181, acc: 0.101562]\n",
      "1706: [D loss: 0.701540, acc: 0.533203]  [A loss: 0.848528, acc: 0.144531]\n",
      "1707: [D loss: 0.704592, acc: 0.503906]  [A loss: 0.813295, acc: 0.214844]\n",
      "1708: [D loss: 0.707309, acc: 0.509766]  [A loss: 0.913296, acc: 0.144531]\n",
      "1709: [D loss: 0.676585, acc: 0.578125]  [A loss: 0.786149, acc: 0.281250]\n",
      "1710: [D loss: 0.696639, acc: 0.531250]  [A loss: 0.911194, acc: 0.101562]\n",
      "1711: [D loss: 0.690522, acc: 0.515625]  [A loss: 0.721659, acc: 0.476562]\n",
      "1712: [D loss: 0.708897, acc: 0.498047]  [A loss: 0.964805, acc: 0.066406]\n",
      "1713: [D loss: 0.695530, acc: 0.494141]  [A loss: 0.733800, acc: 0.410156]\n",
      "1714: [D loss: 0.712693, acc: 0.509766]  [A loss: 0.954942, acc: 0.058594]\n",
      "1715: [D loss: 0.704552, acc: 0.482422]  [A loss: 0.682261, acc: 0.519531]\n",
      "1716: [D loss: 0.712589, acc: 0.507812]  [A loss: 1.010240, acc: 0.046875]\n",
      "1717: [D loss: 0.700183, acc: 0.544922]  [A loss: 0.671025, acc: 0.574219]\n",
      "1718: [D loss: 0.714004, acc: 0.521484]  [A loss: 0.870299, acc: 0.125000]\n",
      "1719: [D loss: 0.697917, acc: 0.525391]  [A loss: 0.842352, acc: 0.160156]\n",
      "1720: [D loss: 0.689642, acc: 0.558594]  [A loss: 0.832660, acc: 0.226562]\n",
      "1721: [D loss: 0.688427, acc: 0.544922]  [A loss: 0.784244, acc: 0.285156]\n",
      "1722: [D loss: 0.695788, acc: 0.533203]  [A loss: 0.842806, acc: 0.203125]\n",
      "1723: [D loss: 0.682120, acc: 0.548828]  [A loss: 0.840457, acc: 0.203125]\n",
      "1724: [D loss: 0.697684, acc: 0.509766]  [A loss: 0.829436, acc: 0.214844]\n",
      "1725: [D loss: 0.691658, acc: 0.548828]  [A loss: 0.856104, acc: 0.175781]\n",
      "1726: [D loss: 0.711326, acc: 0.500000]  [A loss: 0.781475, acc: 0.335938]\n",
      "1727: [D loss: 0.706600, acc: 0.511719]  [A loss: 0.837916, acc: 0.199219]\n",
      "1728: [D loss: 0.703475, acc: 0.505859]  [A loss: 0.851101, acc: 0.140625]\n",
      "1729: [D loss: 0.700690, acc: 0.515625]  [A loss: 0.837849, acc: 0.207031]\n",
      "1730: [D loss: 0.694430, acc: 0.541016]  [A loss: 0.914921, acc: 0.109375]\n",
      "1731: [D loss: 0.687766, acc: 0.535156]  [A loss: 0.743639, acc: 0.414062]\n",
      "1732: [D loss: 0.709492, acc: 0.525391]  [A loss: 0.867273, acc: 0.117188]\n",
      "1733: [D loss: 0.678799, acc: 0.583984]  [A loss: 0.778965, acc: 0.339844]\n",
      "1734: [D loss: 0.710114, acc: 0.529297]  [A loss: 0.971850, acc: 0.085938]\n",
      "1735: [D loss: 0.687716, acc: 0.541016]  [A loss: 0.709950, acc: 0.445312]\n",
      "1736: [D loss: 0.707471, acc: 0.542969]  [A loss: 0.912111, acc: 0.109375]\n",
      "1737: [D loss: 0.695019, acc: 0.527344]  [A loss: 0.736750, acc: 0.417969]\n",
      "1738: [D loss: 0.702860, acc: 0.535156]  [A loss: 0.894902, acc: 0.117188]\n",
      "1739: [D loss: 0.703477, acc: 0.525391]  [A loss: 0.763097, acc: 0.308594]\n",
      "1740: [D loss: 0.712919, acc: 0.503906]  [A loss: 0.909246, acc: 0.101562]\n",
      "1741: [D loss: 0.691169, acc: 0.531250]  [A loss: 0.738067, acc: 0.417969]\n",
      "1742: [D loss: 0.698561, acc: 0.542969]  [A loss: 0.938723, acc: 0.085938]\n",
      "1743: [D loss: 0.700434, acc: 0.494141]  [A loss: 0.750875, acc: 0.363281]\n",
      "1744: [D loss: 0.710387, acc: 0.501953]  [A loss: 0.936452, acc: 0.062500]\n",
      "1745: [D loss: 0.687408, acc: 0.548828]  [A loss: 0.710044, acc: 0.468750]\n",
      "1746: [D loss: 0.729962, acc: 0.505859]  [A loss: 0.937638, acc: 0.070312]\n",
      "1747: [D loss: 0.684615, acc: 0.537109]  [A loss: 0.736037, acc: 0.433594]\n",
      "1748: [D loss: 0.716642, acc: 0.501953]  [A loss: 0.886070, acc: 0.082031]\n",
      "1749: [D loss: 0.696725, acc: 0.509766]  [A loss: 0.788088, acc: 0.269531]\n",
      "1750: [D loss: 0.703680, acc: 0.533203]  [A loss: 0.865140, acc: 0.156250]\n",
      "1751: [D loss: 0.695618, acc: 0.521484]  [A loss: 0.813972, acc: 0.222656]\n",
      "1752: [D loss: 0.701886, acc: 0.527344]  [A loss: 0.830147, acc: 0.175781]\n",
      "1753: [D loss: 0.686309, acc: 0.539062]  [A loss: 0.780629, acc: 0.292969]\n",
      "1754: [D loss: 0.698862, acc: 0.531250]  [A loss: 0.861255, acc: 0.148438]\n",
      "1755: [D loss: 0.690682, acc: 0.527344]  [A loss: 0.774563, acc: 0.281250]\n",
      "1756: [D loss: 0.710212, acc: 0.513672]  [A loss: 0.932939, acc: 0.074219]\n",
      "1757: [D loss: 0.681687, acc: 0.554688]  [A loss: 0.667078, acc: 0.554688]\n",
      "1758: [D loss: 0.735415, acc: 0.507812]  [A loss: 1.036564, acc: 0.046875]\n",
      "1759: [D loss: 0.705166, acc: 0.486328]  [A loss: 0.704982, acc: 0.492188]\n",
      "1760: [D loss: 0.745635, acc: 0.501953]  [A loss: 0.827745, acc: 0.214844]\n",
      "1761: [D loss: 0.708360, acc: 0.511719]  [A loss: 0.965501, acc: 0.054688]\n",
      "1762: [D loss: 0.686204, acc: 0.552734]  [A loss: 0.732880, acc: 0.406250]\n",
      "1763: [D loss: 0.716597, acc: 0.498047]  [A loss: 0.885769, acc: 0.125000]\n",
      "1764: [D loss: 0.682971, acc: 0.548828]  [A loss: 0.743875, acc: 0.394531]\n",
      "1765: [D loss: 0.715227, acc: 0.501953]  [A loss: 0.885202, acc: 0.136719]\n",
      "1766: [D loss: 0.698681, acc: 0.498047]  [A loss: 0.792339, acc: 0.253906]\n",
      "1767: [D loss: 0.699423, acc: 0.517578]  [A loss: 0.797711, acc: 0.265625]\n",
      "1768: [D loss: 0.703935, acc: 0.500000]  [A loss: 0.855728, acc: 0.144531]\n",
      "1769: [D loss: 0.692604, acc: 0.531250]  [A loss: 0.802956, acc: 0.246094]\n",
      "1770: [D loss: 0.699938, acc: 0.511719]  [A loss: 0.817402, acc: 0.191406]\n",
      "1771: [D loss: 0.698281, acc: 0.531250]  [A loss: 0.847869, acc: 0.183594]\n",
      "1772: [D loss: 0.692441, acc: 0.558594]  [A loss: 0.785698, acc: 0.257812]\n",
      "1773: [D loss: 0.680204, acc: 0.552734]  [A loss: 0.843743, acc: 0.187500]\n",
      "1774: [D loss: 0.697186, acc: 0.509766]  [A loss: 0.818650, acc: 0.238281]\n",
      "1775: [D loss: 0.688275, acc: 0.529297]  [A loss: 0.783136, acc: 0.312500]\n",
      "1776: [D loss: 0.691880, acc: 0.539062]  [A loss: 0.821297, acc: 0.226562]\n",
      "1777: [D loss: 0.690853, acc: 0.539062]  [A loss: 0.868394, acc: 0.144531]\n",
      "1778: [D loss: 0.694929, acc: 0.531250]  [A loss: 0.789411, acc: 0.300781]\n",
      "1779: [D loss: 0.687693, acc: 0.550781]  [A loss: 0.846094, acc: 0.203125]\n",
      "1780: [D loss: 0.693804, acc: 0.537109]  [A loss: 0.812763, acc: 0.218750]\n",
      "1781: [D loss: 0.693979, acc: 0.541016]  [A loss: 0.810831, acc: 0.218750]\n",
      "1782: [D loss: 0.696787, acc: 0.521484]  [A loss: 0.823117, acc: 0.203125]\n",
      "1783: [D loss: 0.685787, acc: 0.521484]  [A loss: 0.782381, acc: 0.292969]\n",
      "1784: [D loss: 0.698509, acc: 0.535156]  [A loss: 0.926825, acc: 0.070312]\n",
      "1785: [D loss: 0.694969, acc: 0.523438]  [A loss: 0.739637, acc: 0.398438]\n",
      "1786: [D loss: 0.706773, acc: 0.523438]  [A loss: 0.928302, acc: 0.093750]\n",
      "1787: [D loss: 0.691633, acc: 0.513672]  [A loss: 0.759563, acc: 0.339844]\n",
      "1788: [D loss: 0.700580, acc: 0.535156]  [A loss: 0.901182, acc: 0.105469]\n",
      "1789: [D loss: 0.704842, acc: 0.503906]  [A loss: 0.785980, acc: 0.316406]\n",
      "1790: [D loss: 0.698479, acc: 0.550781]  [A loss: 0.962939, acc: 0.085938]\n",
      "1791: [D loss: 0.689725, acc: 0.535156]  [A loss: 0.686535, acc: 0.535156]\n",
      "1792: [D loss: 0.742416, acc: 0.474609]  [A loss: 0.853723, acc: 0.210938]\n",
      "1793: [D loss: 0.719557, acc: 0.515625]  [A loss: 1.101305, acc: 0.019531]\n",
      "1794: [D loss: 0.706792, acc: 0.505859]  [A loss: 0.672290, acc: 0.574219]\n",
      "1795: [D loss: 0.716896, acc: 0.527344]  [A loss: 0.935358, acc: 0.070312]\n",
      "1796: [D loss: 0.681721, acc: 0.583984]  [A loss: 0.736460, acc: 0.410156]\n",
      "1797: [D loss: 0.704509, acc: 0.505859]  [A loss: 0.854345, acc: 0.156250]\n",
      "1798: [D loss: 0.688053, acc: 0.562500]  [A loss: 0.803236, acc: 0.261719]\n",
      "1799: [D loss: 0.694104, acc: 0.501953]  [A loss: 0.780101, acc: 0.296875]\n",
      "1800: [D loss: 0.699755, acc: 0.517578]  [A loss: 0.769430, acc: 0.324219]\n",
      "1801: [D loss: 0.696641, acc: 0.511719]  [A loss: 0.820524, acc: 0.234375]\n",
      "1802: [D loss: 0.701726, acc: 0.500000]  [A loss: 0.809366, acc: 0.238281]\n",
      "1803: [D loss: 0.698198, acc: 0.531250]  [A loss: 0.787646, acc: 0.277344]\n",
      "1804: [D loss: 0.704683, acc: 0.529297]  [A loss: 0.824710, acc: 0.222656]\n",
      "1805: [D loss: 0.685294, acc: 0.566406]  [A loss: 0.768431, acc: 0.355469]\n",
      "1806: [D loss: 0.698848, acc: 0.546875]  [A loss: 0.889821, acc: 0.121094]\n",
      "1807: [D loss: 0.689573, acc: 0.521484]  [A loss: 0.785763, acc: 0.230469]\n",
      "1808: [D loss: 0.687611, acc: 0.550781]  [A loss: 0.879752, acc: 0.152344]\n",
      "1809: [D loss: 0.682968, acc: 0.566406]  [A loss: 0.730439, acc: 0.402344]\n",
      "1810: [D loss: 0.699494, acc: 0.525391]  [A loss: 0.827735, acc: 0.230469]\n",
      "1811: [D loss: 0.708118, acc: 0.505859]  [A loss: 0.898995, acc: 0.128906]\n",
      "1812: [D loss: 0.679549, acc: 0.589844]  [A loss: 0.746001, acc: 0.394531]\n",
      "1813: [D loss: 0.705475, acc: 0.519531]  [A loss: 0.863135, acc: 0.175781]\n",
      "1814: [D loss: 0.690661, acc: 0.529297]  [A loss: 0.833941, acc: 0.195312]\n",
      "1815: [D loss: 0.702618, acc: 0.523438]  [A loss: 0.868183, acc: 0.179688]\n",
      "1816: [D loss: 0.686266, acc: 0.572266]  [A loss: 0.817648, acc: 0.250000]\n",
      "1817: [D loss: 0.699212, acc: 0.529297]  [A loss: 0.854802, acc: 0.179688]\n",
      "1818: [D loss: 0.688109, acc: 0.562500]  [A loss: 0.817095, acc: 0.207031]\n",
      "1819: [D loss: 0.700443, acc: 0.527344]  [A loss: 0.904806, acc: 0.128906]\n",
      "1820: [D loss: 0.678652, acc: 0.568359]  [A loss: 0.762671, acc: 0.339844]\n",
      "1821: [D loss: 0.696102, acc: 0.542969]  [A loss: 0.933643, acc: 0.082031]\n",
      "1822: [D loss: 0.684584, acc: 0.550781]  [A loss: 0.756888, acc: 0.386719]\n",
      "1823: [D loss: 0.694985, acc: 0.521484]  [A loss: 0.975514, acc: 0.136719]\n",
      "1824: [D loss: 0.708984, acc: 0.517578]  [A loss: 0.832846, acc: 0.210938]\n",
      "1825: [D loss: 0.698509, acc: 0.527344]  [A loss: 0.802630, acc: 0.257812]\n",
      "1826: [D loss: 0.700688, acc: 0.550781]  [A loss: 0.834871, acc: 0.203125]\n",
      "1827: [D loss: 0.685473, acc: 0.556641]  [A loss: 0.871621, acc: 0.148438]\n",
      "1828: [D loss: 0.690288, acc: 0.552734]  [A loss: 0.811506, acc: 0.261719]\n",
      "1829: [D loss: 0.693592, acc: 0.544922]  [A loss: 0.931210, acc: 0.085938]\n",
      "1830: [D loss: 0.701945, acc: 0.490234]  [A loss: 0.762819, acc: 0.339844]\n",
      "1831: [D loss: 0.702353, acc: 0.523438]  [A loss: 0.999996, acc: 0.082031]\n",
      "1832: [D loss: 0.702838, acc: 0.503906]  [A loss: 0.661924, acc: 0.597656]\n",
      "1833: [D loss: 0.718499, acc: 0.480469]  [A loss: 0.901404, acc: 0.093750]\n",
      "1834: [D loss: 0.694958, acc: 0.550781]  [A loss: 0.789877, acc: 0.292969]\n",
      "1835: [D loss: 0.714590, acc: 0.511719]  [A loss: 0.962587, acc: 0.058594]\n",
      "1836: [D loss: 0.684639, acc: 0.546875]  [A loss: 0.686353, acc: 0.558594]\n",
      "1837: [D loss: 0.722297, acc: 0.527344]  [A loss: 0.946379, acc: 0.078125]\n",
      "1838: [D loss: 0.687601, acc: 0.539062]  [A loss: 0.704473, acc: 0.492188]\n",
      "1839: [D loss: 0.704798, acc: 0.541016]  [A loss: 0.940818, acc: 0.085938]\n",
      "1840: [D loss: 0.700963, acc: 0.521484]  [A loss: 0.722681, acc: 0.468750]\n",
      "1841: [D loss: 0.703317, acc: 0.521484]  [A loss: 0.911943, acc: 0.089844]\n",
      "1842: [D loss: 0.690486, acc: 0.519531]  [A loss: 0.732227, acc: 0.402344]\n",
      "1843: [D loss: 0.703809, acc: 0.539062]  [A loss: 0.841400, acc: 0.207031]\n",
      "1844: [D loss: 0.686977, acc: 0.574219]  [A loss: 0.788871, acc: 0.273438]\n",
      "1845: [D loss: 0.706671, acc: 0.525391]  [A loss: 0.891163, acc: 0.132812]\n",
      "1846: [D loss: 0.691678, acc: 0.542969]  [A loss: 0.771270, acc: 0.332031]\n",
      "1847: [D loss: 0.689782, acc: 0.511719]  [A loss: 0.865716, acc: 0.175781]\n",
      "1848: [D loss: 0.695508, acc: 0.531250]  [A loss: 0.804953, acc: 0.250000]\n",
      "1849: [D loss: 0.689566, acc: 0.560547]  [A loss: 0.817971, acc: 0.199219]\n",
      "1850: [D loss: 0.708174, acc: 0.472656]  [A loss: 0.923462, acc: 0.113281]\n",
      "1851: [D loss: 0.691561, acc: 0.531250]  [A loss: 0.733033, acc: 0.406250]\n",
      "1852: [D loss: 0.703128, acc: 0.531250]  [A loss: 0.895695, acc: 0.089844]\n",
      "1853: [D loss: 0.688213, acc: 0.533203]  [A loss: 0.790352, acc: 0.281250]\n",
      "1854: [D loss: 0.700610, acc: 0.523438]  [A loss: 0.849571, acc: 0.167969]\n",
      "1855: [D loss: 0.699815, acc: 0.517578]  [A loss: 0.807716, acc: 0.238281]\n",
      "1856: [D loss: 0.687508, acc: 0.552734]  [A loss: 0.817597, acc: 0.222656]\n",
      "1857: [D loss: 0.700864, acc: 0.505859]  [A loss: 0.822305, acc: 0.214844]\n",
      "1858: [D loss: 0.708575, acc: 0.509766]  [A loss: 0.847405, acc: 0.187500]\n",
      "1859: [D loss: 0.697821, acc: 0.507812]  [A loss: 0.760854, acc: 0.359375]\n",
      "1860: [D loss: 0.707490, acc: 0.494141]  [A loss: 0.942701, acc: 0.082031]\n",
      "1861: [D loss: 0.674328, acc: 0.582031]  [A loss: 0.654769, acc: 0.625000]\n",
      "1862: [D loss: 0.740423, acc: 0.496094]  [A loss: 1.048676, acc: 0.039062]\n",
      "1863: [D loss: 0.696122, acc: 0.537109]  [A loss: 0.706246, acc: 0.515625]\n",
      "1864: [D loss: 0.754558, acc: 0.511719]  [A loss: 0.862428, acc: 0.152344]\n",
      "1865: [D loss: 0.710680, acc: 0.480469]  [A loss: 0.906831, acc: 0.105469]\n",
      "1866: [D loss: 0.695347, acc: 0.521484]  [A loss: 0.781520, acc: 0.304688]\n",
      "1867: [D loss: 0.729609, acc: 0.468750]  [A loss: 0.919433, acc: 0.062500]\n",
      "1868: [D loss: 0.693486, acc: 0.525391]  [A loss: 0.779833, acc: 0.292969]\n",
      "1869: [D loss: 0.708699, acc: 0.507812]  [A loss: 0.917081, acc: 0.085938]\n",
      "1870: [D loss: 0.690848, acc: 0.521484]  [A loss: 0.777695, acc: 0.328125]\n",
      "1871: [D loss: 0.694839, acc: 0.542969]  [A loss: 0.881631, acc: 0.128906]\n",
      "1872: [D loss: 0.706885, acc: 0.525391]  [A loss: 0.764175, acc: 0.375000]\n",
      "1873: [D loss: 0.689926, acc: 0.539062]  [A loss: 0.779546, acc: 0.304688]\n",
      "1874: [D loss: 0.701109, acc: 0.507812]  [A loss: 0.832626, acc: 0.210938]\n",
      "1875: [D loss: 0.679237, acc: 0.593750]  [A loss: 0.780222, acc: 0.328125]\n",
      "1876: [D loss: 0.709929, acc: 0.490234]  [A loss: 0.840199, acc: 0.195312]\n",
      "1877: [D loss: 0.701587, acc: 0.529297]  [A loss: 0.807880, acc: 0.207031]\n",
      "1878: [D loss: 0.691860, acc: 0.533203]  [A loss: 0.861587, acc: 0.132812]\n",
      "1879: [D loss: 0.683897, acc: 0.564453]  [A loss: 0.825284, acc: 0.218750]\n",
      "1880: [D loss: 0.698610, acc: 0.505859]  [A loss: 0.869271, acc: 0.144531]\n",
      "1881: [D loss: 0.691066, acc: 0.519531]  [A loss: 0.766281, acc: 0.332031]\n",
      "1882: [D loss: 0.716920, acc: 0.505859]  [A loss: 0.940625, acc: 0.113281]\n",
      "1883: [D loss: 0.690975, acc: 0.533203]  [A loss: 0.693673, acc: 0.542969]\n",
      "1884: [D loss: 0.712692, acc: 0.494141]  [A loss: 0.848027, acc: 0.160156]\n",
      "1885: [D loss: 0.693411, acc: 0.537109]  [A loss: 0.776590, acc: 0.304688]\n",
      "1886: [D loss: 0.702581, acc: 0.519531]  [A loss: 0.928865, acc: 0.085938]\n",
      "1887: [D loss: 0.682654, acc: 0.558594]  [A loss: 0.715228, acc: 0.476562]\n",
      "1888: [D loss: 0.722290, acc: 0.503906]  [A loss: 0.901218, acc: 0.121094]\n",
      "1889: [D loss: 0.697296, acc: 0.519531]  [A loss: 0.824977, acc: 0.222656]\n",
      "1890: [D loss: 0.690996, acc: 0.572266]  [A loss: 0.872111, acc: 0.167969]\n",
      "1891: [D loss: 0.693527, acc: 0.523438]  [A loss: 0.788700, acc: 0.269531]\n",
      "1892: [D loss: 0.701642, acc: 0.527344]  [A loss: 0.887683, acc: 0.121094]\n",
      "1893: [D loss: 0.701437, acc: 0.503906]  [A loss: 0.800174, acc: 0.246094]\n",
      "1894: [D loss: 0.686749, acc: 0.546875]  [A loss: 0.834590, acc: 0.175781]\n",
      "1895: [D loss: 0.697446, acc: 0.498047]  [A loss: 0.906293, acc: 0.093750]\n",
      "1896: [D loss: 0.695109, acc: 0.529297]  [A loss: 0.724870, acc: 0.460938]\n",
      "1897: [D loss: 0.704985, acc: 0.513672]  [A loss: 0.974771, acc: 0.062500]\n",
      "1898: [D loss: 0.691441, acc: 0.535156]  [A loss: 0.763396, acc: 0.335938]\n",
      "1899: [D loss: 0.703074, acc: 0.533203]  [A loss: 0.852481, acc: 0.175781]\n",
      "1900: [D loss: 0.698387, acc: 0.492188]  [A loss: 0.845406, acc: 0.191406]\n",
      "1901: [D loss: 0.698458, acc: 0.531250]  [A loss: 0.843531, acc: 0.183594]\n",
      "1902: [D loss: 0.703950, acc: 0.507812]  [A loss: 0.804908, acc: 0.218750]\n",
      "1903: [D loss: 0.691737, acc: 0.531250]  [A loss: 0.969305, acc: 0.074219]\n",
      "1904: [D loss: 0.690982, acc: 0.513672]  [A loss: 0.669817, acc: 0.589844]\n",
      "1905: [D loss: 0.721487, acc: 0.500000]  [A loss: 0.954004, acc: 0.074219]\n",
      "1906: [D loss: 0.683713, acc: 0.537109]  [A loss: 0.695193, acc: 0.519531]\n",
      "1907: [D loss: 0.710346, acc: 0.523438]  [A loss: 0.919592, acc: 0.113281]\n",
      "1908: [D loss: 0.694252, acc: 0.533203]  [A loss: 0.787404, acc: 0.296875]\n",
      "1909: [D loss: 0.689409, acc: 0.562500]  [A loss: 0.807717, acc: 0.253906]\n",
      "1910: [D loss: 0.704975, acc: 0.515625]  [A loss: 0.853453, acc: 0.132812]\n",
      "1911: [D loss: 0.697827, acc: 0.527344]  [A loss: 0.773193, acc: 0.300781]\n",
      "1912: [D loss: 0.691026, acc: 0.541016]  [A loss: 0.836794, acc: 0.187500]\n",
      "1913: [D loss: 0.683166, acc: 0.552734]  [A loss: 0.898020, acc: 0.113281]\n",
      "1914: [D loss: 0.681199, acc: 0.560547]  [A loss: 0.754429, acc: 0.390625]\n",
      "1915: [D loss: 0.700583, acc: 0.519531]  [A loss: 0.936738, acc: 0.085938]\n",
      "1916: [D loss: 0.698038, acc: 0.496094]  [A loss: 0.750065, acc: 0.371094]\n",
      "1917: [D loss: 0.707829, acc: 0.507812]  [A loss: 0.929105, acc: 0.093750]\n",
      "1918: [D loss: 0.688331, acc: 0.537109]  [A loss: 0.739014, acc: 0.386719]\n",
      "1919: [D loss: 0.702754, acc: 0.537109]  [A loss: 0.977805, acc: 0.070312]\n",
      "1920: [D loss: 0.685226, acc: 0.537109]  [A loss: 0.716394, acc: 0.480469]\n",
      "1921: [D loss: 0.711582, acc: 0.529297]  [A loss: 0.970048, acc: 0.132812]\n",
      "1922: [D loss: 0.697865, acc: 0.513672]  [A loss: 0.702606, acc: 0.500000]\n",
      "1923: [D loss: 0.709414, acc: 0.552734]  [A loss: 0.821589, acc: 0.246094]\n",
      "1924: [D loss: 0.710773, acc: 0.492188]  [A loss: 0.793381, acc: 0.273438]\n",
      "1925: [D loss: 0.710553, acc: 0.525391]  [A loss: 1.006785, acc: 0.031250]\n",
      "1926: [D loss: 0.683114, acc: 0.564453]  [A loss: 0.705761, acc: 0.476562]\n",
      "1927: [D loss: 0.714992, acc: 0.482422]  [A loss: 0.877782, acc: 0.136719]\n",
      "1928: [D loss: 0.688885, acc: 0.535156]  [A loss: 0.815162, acc: 0.277344]\n",
      "1929: [D loss: 0.698521, acc: 0.509766]  [A loss: 0.854823, acc: 0.175781]\n",
      "1930: [D loss: 0.690581, acc: 0.519531]  [A loss: 0.809527, acc: 0.242188]\n",
      "1931: [D loss: 0.700741, acc: 0.531250]  [A loss: 0.855491, acc: 0.152344]\n",
      "1932: [D loss: 0.694435, acc: 0.511719]  [A loss: 0.798683, acc: 0.292969]\n",
      "1933: [D loss: 0.701645, acc: 0.498047]  [A loss: 0.837187, acc: 0.195312]\n",
      "1934: [D loss: 0.700621, acc: 0.523438]  [A loss: 0.879778, acc: 0.164062]\n",
      "1935: [D loss: 0.696515, acc: 0.533203]  [A loss: 0.788447, acc: 0.320312]\n",
      "1936: [D loss: 0.696082, acc: 0.535156]  [A loss: 0.909277, acc: 0.101562]\n",
      "1937: [D loss: 0.694838, acc: 0.507812]  [A loss: 0.704209, acc: 0.511719]\n",
      "1938: [D loss: 0.722640, acc: 0.523438]  [A loss: 1.024723, acc: 0.082031]\n",
      "1939: [D loss: 0.703057, acc: 0.542969]  [A loss: 0.733334, acc: 0.437500]\n",
      "1940: [D loss: 0.716733, acc: 0.515625]  [A loss: 0.915384, acc: 0.089844]\n",
      "1941: [D loss: 0.695891, acc: 0.548828]  [A loss: 0.815335, acc: 0.230469]\n",
      "1942: [D loss: 0.697646, acc: 0.527344]  [A loss: 0.844485, acc: 0.160156]\n",
      "1943: [D loss: 0.697151, acc: 0.531250]  [A loss: 0.733806, acc: 0.425781]\n",
      "1944: [D loss: 0.697210, acc: 0.509766]  [A loss: 0.862168, acc: 0.175781]\n",
      "1945: [D loss: 0.717347, acc: 0.466797]  [A loss: 0.832649, acc: 0.203125]\n",
      "1946: [D loss: 0.688546, acc: 0.533203]  [A loss: 0.890503, acc: 0.097656]\n",
      "1947: [D loss: 0.677996, acc: 0.570312]  [A loss: 0.817790, acc: 0.250000]\n",
      "1948: [D loss: 0.696913, acc: 0.535156]  [A loss: 0.842690, acc: 0.187500]\n",
      "1949: [D loss: 0.671870, acc: 0.580078]  [A loss: 0.752291, acc: 0.433594]\n",
      "1950: [D loss: 0.714545, acc: 0.511719]  [A loss: 0.877813, acc: 0.160156]\n",
      "1951: [D loss: 0.686032, acc: 0.564453]  [A loss: 0.829034, acc: 0.214844]\n",
      "1952: [D loss: 0.682915, acc: 0.568359]  [A loss: 0.860959, acc: 0.214844]\n",
      "1953: [D loss: 0.689364, acc: 0.537109]  [A loss: 0.880378, acc: 0.144531]\n",
      "1954: [D loss: 0.694361, acc: 0.527344]  [A loss: 0.818263, acc: 0.269531]\n",
      "1955: [D loss: 0.687696, acc: 0.552734]  [A loss: 0.857443, acc: 0.183594]\n",
      "1956: [D loss: 0.684494, acc: 0.554688]  [A loss: 0.807263, acc: 0.214844]\n",
      "1957: [D loss: 0.696749, acc: 0.552734]  [A loss: 0.884811, acc: 0.125000]\n",
      "1958: [D loss: 0.696910, acc: 0.537109]  [A loss: 0.798865, acc: 0.265625]\n",
      "1959: [D loss: 0.703360, acc: 0.500000]  [A loss: 1.002609, acc: 0.050781]\n",
      "1960: [D loss: 0.694538, acc: 0.525391]  [A loss: 0.660464, acc: 0.632812]\n",
      "1961: [D loss: 0.719998, acc: 0.509766]  [A loss: 0.999514, acc: 0.046875]\n",
      "1962: [D loss: 0.682596, acc: 0.546875]  [A loss: 0.678533, acc: 0.566406]\n",
      "1963: [D loss: 0.732633, acc: 0.496094]  [A loss: 1.021873, acc: 0.074219]\n",
      "1964: [D loss: 0.708867, acc: 0.525391]  [A loss: 0.842318, acc: 0.218750]\n",
      "1965: [D loss: 0.689734, acc: 0.554688]  [A loss: 0.757773, acc: 0.332031]\n",
      "1966: [D loss: 0.688594, acc: 0.539062]  [A loss: 0.781982, acc: 0.312500]\n",
      "1967: [D loss: 0.704616, acc: 0.546875]  [A loss: 0.891094, acc: 0.121094]\n",
      "1968: [D loss: 0.689891, acc: 0.560547]  [A loss: 0.753249, acc: 0.378906]\n",
      "1969: [D loss: 0.708805, acc: 0.519531]  [A loss: 0.897242, acc: 0.125000]\n",
      "1970: [D loss: 0.694049, acc: 0.515625]  [A loss: 0.778181, acc: 0.328125]\n",
      "1971: [D loss: 0.701948, acc: 0.521484]  [A loss: 0.909728, acc: 0.132812]\n",
      "1972: [D loss: 0.696382, acc: 0.529297]  [A loss: 0.741312, acc: 0.417969]\n",
      "1973: [D loss: 0.703211, acc: 0.523438]  [A loss: 0.887288, acc: 0.179688]\n",
      "1974: [D loss: 0.688106, acc: 0.546875]  [A loss: 0.754850, acc: 0.355469]\n",
      "1975: [D loss: 0.714596, acc: 0.492188]  [A loss: 0.947914, acc: 0.054688]\n",
      "1976: [D loss: 0.698796, acc: 0.507812]  [A loss: 0.695061, acc: 0.535156]\n",
      "1977: [D loss: 0.701249, acc: 0.505859]  [A loss: 0.912117, acc: 0.093750]\n",
      "1978: [D loss: 0.692600, acc: 0.523438]  [A loss: 0.739771, acc: 0.402344]\n",
      "1979: [D loss: 0.697667, acc: 0.531250]  [A loss: 0.859817, acc: 0.175781]\n",
      "1980: [D loss: 0.683201, acc: 0.572266]  [A loss: 0.745768, acc: 0.398438]\n",
      "1981: [D loss: 0.696440, acc: 0.519531]  [A loss: 0.843394, acc: 0.199219]\n",
      "1982: [D loss: 0.694506, acc: 0.531250]  [A loss: 0.755298, acc: 0.347656]\n",
      "1983: [D loss: 0.701513, acc: 0.546875]  [A loss: 0.905865, acc: 0.089844]\n",
      "1984: [D loss: 0.692986, acc: 0.541016]  [A loss: 0.740206, acc: 0.394531]\n",
      "1985: [D loss: 0.717330, acc: 0.513672]  [A loss: 0.999248, acc: 0.035156]\n",
      "1986: [D loss: 0.685500, acc: 0.568359]  [A loss: 0.721200, acc: 0.445312]\n",
      "1987: [D loss: 0.700885, acc: 0.535156]  [A loss: 0.914820, acc: 0.089844]\n",
      "1988: [D loss: 0.701572, acc: 0.511719]  [A loss: 0.744152, acc: 0.355469]\n",
      "1989: [D loss: 0.707771, acc: 0.519531]  [A loss: 0.899285, acc: 0.105469]\n",
      "1990: [D loss: 0.686848, acc: 0.535156]  [A loss: 0.759494, acc: 0.347656]\n",
      "1991: [D loss: 0.700360, acc: 0.527344]  [A loss: 0.875091, acc: 0.156250]\n",
      "1992: [D loss: 0.694769, acc: 0.539062]  [A loss: 0.806869, acc: 0.242188]\n",
      "1993: [D loss: 0.699026, acc: 0.525391]  [A loss: 0.816180, acc: 0.242188]\n",
      "1994: [D loss: 0.701151, acc: 0.564453]  [A loss: 0.811313, acc: 0.265625]\n",
      "1995: [D loss: 0.705329, acc: 0.515625]  [A loss: 0.877058, acc: 0.156250]\n",
      "1996: [D loss: 0.682465, acc: 0.558594]  [A loss: 0.771582, acc: 0.289062]\n",
      "1997: [D loss: 0.712243, acc: 0.501953]  [A loss: 0.870590, acc: 0.164062]\n",
      "1998: [D loss: 0.699553, acc: 0.513672]  [A loss: 0.768581, acc: 0.363281]\n",
      "1999: [D loss: 0.712555, acc: 0.519531]  [A loss: 0.806870, acc: 0.273438]\n",
      "2000: [D loss: 0.704173, acc: 0.521484]  [A loss: 1.002006, acc: 0.058594]\n",
      "2001: [D loss: 0.705306, acc: 0.537109]  [A loss: 0.675883, acc: 0.558594]\n",
      "2002: [D loss: 0.718362, acc: 0.523438]  [A loss: 0.923396, acc: 0.097656]\n",
      "2003: [D loss: 0.704300, acc: 0.494141]  [A loss: 0.682045, acc: 0.539062]\n",
      "2004: [D loss: 0.733011, acc: 0.517578]  [A loss: 0.858566, acc: 0.171875]\n",
      "2005: [D loss: 0.710430, acc: 0.492188]  [A loss: 0.904185, acc: 0.117188]\n",
      "2006: [D loss: 0.696623, acc: 0.501953]  [A loss: 0.761297, acc: 0.320312]\n",
      "2007: [D loss: 0.724699, acc: 0.472656]  [A loss: 0.901824, acc: 0.117188]\n",
      "2008: [D loss: 0.688778, acc: 0.533203]  [A loss: 0.754260, acc: 0.308594]\n",
      "2009: [D loss: 0.706376, acc: 0.523438]  [A loss: 0.920451, acc: 0.109375]\n",
      "2010: [D loss: 0.689743, acc: 0.521484]  [A loss: 0.761410, acc: 0.328125]\n",
      "2011: [D loss: 0.692356, acc: 0.533203]  [A loss: 0.846759, acc: 0.136719]\n",
      "2012: [D loss: 0.695202, acc: 0.519531]  [A loss: 0.833717, acc: 0.203125]\n",
      "2013: [D loss: 0.700866, acc: 0.517578]  [A loss: 0.808101, acc: 0.246094]\n",
      "2014: [D loss: 0.697656, acc: 0.521484]  [A loss: 0.817677, acc: 0.203125]\n",
      "2015: [D loss: 0.721921, acc: 0.482422]  [A loss: 0.899885, acc: 0.113281]\n",
      "2016: [D loss: 0.701238, acc: 0.517578]  [A loss: 0.707970, acc: 0.492188]\n",
      "2017: [D loss: 0.708616, acc: 0.523438]  [A loss: 0.951669, acc: 0.113281]\n",
      "2018: [D loss: 0.682762, acc: 0.548828]  [A loss: 0.674649, acc: 0.566406]\n",
      "2019: [D loss: 0.718754, acc: 0.496094]  [A loss: 0.922902, acc: 0.078125]\n",
      "2020: [D loss: 0.698685, acc: 0.517578]  [A loss: 0.768168, acc: 0.316406]\n",
      "2021: [D loss: 0.704282, acc: 0.505859]  [A loss: 0.834038, acc: 0.171875]\n",
      "2022: [D loss: 0.691351, acc: 0.552734]  [A loss: 0.814819, acc: 0.257812]\n",
      "2023: [D loss: 0.706895, acc: 0.509766]  [A loss: 0.969474, acc: 0.062500]\n",
      "2024: [D loss: 0.695992, acc: 0.501953]  [A loss: 0.695763, acc: 0.531250]\n",
      "2025: [D loss: 0.700889, acc: 0.521484]  [A loss: 0.981636, acc: 0.054688]\n",
      "2026: [D loss: 0.681406, acc: 0.550781]  [A loss: 0.688221, acc: 0.554688]\n",
      "2027: [D loss: 0.724646, acc: 0.507812]  [A loss: 0.929825, acc: 0.085938]\n",
      "2028: [D loss: 0.684414, acc: 0.554688]  [A loss: 0.716655, acc: 0.460938]\n",
      "2029: [D loss: 0.710725, acc: 0.523438]  [A loss: 0.997251, acc: 0.039062]\n",
      "2030: [D loss: 0.699375, acc: 0.511719]  [A loss: 0.718508, acc: 0.445312]\n",
      "2031: [D loss: 0.715473, acc: 0.498047]  [A loss: 0.816971, acc: 0.250000]\n",
      "2032: [D loss: 0.702864, acc: 0.492188]  [A loss: 0.797114, acc: 0.253906]\n",
      "2033: [D loss: 0.699926, acc: 0.531250]  [A loss: 0.885748, acc: 0.140625]\n",
      "2034: [D loss: 0.699616, acc: 0.492188]  [A loss: 0.797624, acc: 0.265625]\n",
      "2035: [D loss: 0.695515, acc: 0.560547]  [A loss: 0.861104, acc: 0.128906]\n",
      "2036: [D loss: 0.687298, acc: 0.537109]  [A loss: 0.751689, acc: 0.375000]\n",
      "2037: [D loss: 0.702931, acc: 0.533203]  [A loss: 0.892897, acc: 0.117188]\n",
      "2038: [D loss: 0.697052, acc: 0.541016]  [A loss: 0.759830, acc: 0.332031]\n",
      "2039: [D loss: 0.704125, acc: 0.513672]  [A loss: 0.811855, acc: 0.234375]\n",
      "2040: [D loss: 0.698490, acc: 0.544922]  [A loss: 0.839286, acc: 0.210938]\n",
      "2041: [D loss: 0.683366, acc: 0.562500]  [A loss: 0.795256, acc: 0.289062]\n",
      "2042: [D loss: 0.697895, acc: 0.503906]  [A loss: 0.865936, acc: 0.148438]\n",
      "2043: [D loss: 0.688621, acc: 0.529297]  [A loss: 0.783543, acc: 0.296875]\n",
      "2044: [D loss: 0.698107, acc: 0.519531]  [A loss: 0.860501, acc: 0.164062]\n",
      "2045: [D loss: 0.676880, acc: 0.562500]  [A loss: 0.775989, acc: 0.320312]\n",
      "2046: [D loss: 0.705061, acc: 0.511719]  [A loss: 0.955071, acc: 0.097656]\n",
      "2047: [D loss: 0.696266, acc: 0.501953]  [A loss: 0.709706, acc: 0.484375]\n",
      "2048: [D loss: 0.698522, acc: 0.537109]  [A loss: 0.935153, acc: 0.125000]\n",
      "2049: [D loss: 0.703795, acc: 0.505859]  [A loss: 0.825134, acc: 0.183594]\n",
      "2050: [D loss: 0.704440, acc: 0.521484]  [A loss: 0.834503, acc: 0.175781]\n",
      "2051: [D loss: 0.697231, acc: 0.525391]  [A loss: 0.803814, acc: 0.273438]\n",
      "2052: [D loss: 0.700305, acc: 0.496094]  [A loss: 0.794116, acc: 0.265625]\n",
      "2053: [D loss: 0.692094, acc: 0.533203]  [A loss: 0.879779, acc: 0.160156]\n",
      "2054: [D loss: 0.698259, acc: 0.515625]  [A loss: 0.714828, acc: 0.453125]\n",
      "2055: [D loss: 0.718699, acc: 0.500000]  [A loss: 0.981739, acc: 0.042969]\n",
      "2056: [D loss: 0.700588, acc: 0.515625]  [A loss: 0.693837, acc: 0.550781]\n",
      "2057: [D loss: 0.718229, acc: 0.533203]  [A loss: 0.928475, acc: 0.097656]\n",
      "2058: [D loss: 0.688405, acc: 0.566406]  [A loss: 0.707141, acc: 0.500000]\n",
      "2059: [D loss: 0.707940, acc: 0.505859]  [A loss: 0.916295, acc: 0.089844]\n",
      "2060: [D loss: 0.698454, acc: 0.523438]  [A loss: 0.740217, acc: 0.394531]\n",
      "2061: [D loss: 0.707286, acc: 0.519531]  [A loss: 0.869118, acc: 0.140625]\n",
      "2062: [D loss: 0.703313, acc: 0.494141]  [A loss: 0.776216, acc: 0.292969]\n",
      "2063: [D loss: 0.687968, acc: 0.552734]  [A loss: 0.895952, acc: 0.113281]\n",
      "2064: [D loss: 0.687918, acc: 0.544922]  [A loss: 0.739463, acc: 0.382812]\n",
      "2065: [D loss: 0.704137, acc: 0.523438]  [A loss: 0.911300, acc: 0.085938]\n",
      "2066: [D loss: 0.695032, acc: 0.515625]  [A loss: 0.793757, acc: 0.277344]\n",
      "2067: [D loss: 0.696503, acc: 0.517578]  [A loss: 0.835373, acc: 0.183594]\n",
      "2068: [D loss: 0.686583, acc: 0.544922]  [A loss: 0.823670, acc: 0.199219]\n",
      "2069: [D loss: 0.698825, acc: 0.529297]  [A loss: 0.824586, acc: 0.218750]\n",
      "2070: [D loss: 0.712811, acc: 0.541016]  [A loss: 1.032732, acc: 0.042969]\n",
      "2071: [D loss: 0.697187, acc: 0.500000]  [A loss: 0.663182, acc: 0.601562]\n",
      "2072: [D loss: 0.722822, acc: 0.509766]  [A loss: 1.007236, acc: 0.050781]\n",
      "2073: [D loss: 0.693895, acc: 0.544922]  [A loss: 0.715883, acc: 0.453125]\n",
      "2074: [D loss: 0.721209, acc: 0.492188]  [A loss: 0.948514, acc: 0.085938]\n",
      "2075: [D loss: 0.701728, acc: 0.486328]  [A loss: 0.747931, acc: 0.335938]\n",
      "2076: [D loss: 0.697259, acc: 0.531250]  [A loss: 0.850196, acc: 0.156250]\n",
      "2077: [D loss: 0.678659, acc: 0.568359]  [A loss: 0.711221, acc: 0.503906]\n",
      "2078: [D loss: 0.712712, acc: 0.529297]  [A loss: 0.895385, acc: 0.132812]\n",
      "2079: [D loss: 0.685315, acc: 0.535156]  [A loss: 0.772559, acc: 0.300781]\n",
      "2080: [D loss: 0.690267, acc: 0.541016]  [A loss: 0.815073, acc: 0.246094]\n",
      "2081: [D loss: 0.691136, acc: 0.519531]  [A loss: 0.881794, acc: 0.148438]\n",
      "2082: [D loss: 0.683340, acc: 0.552734]  [A loss: 0.798565, acc: 0.292969]\n",
      "2083: [D loss: 0.690311, acc: 0.548828]  [A loss: 0.808608, acc: 0.261719]\n",
      "2084: [D loss: 0.716238, acc: 0.490234]  [A loss: 0.839861, acc: 0.191406]\n",
      "2085: [D loss: 0.693879, acc: 0.539062]  [A loss: 0.818328, acc: 0.238281]\n",
      "2086: [D loss: 0.690497, acc: 0.531250]  [A loss: 0.869402, acc: 0.148438]\n",
      "2087: [D loss: 0.696595, acc: 0.523438]  [A loss: 0.740163, acc: 0.394531]\n",
      "2088: [D loss: 0.706111, acc: 0.523438]  [A loss: 1.018758, acc: 0.031250]\n",
      "2089: [D loss: 0.698174, acc: 0.488281]  [A loss: 0.698282, acc: 0.503906]\n",
      "2090: [D loss: 0.709952, acc: 0.535156]  [A loss: 0.946035, acc: 0.074219]\n",
      "2091: [D loss: 0.701214, acc: 0.529297]  [A loss: 0.730751, acc: 0.417969]\n",
      "2092: [D loss: 0.702419, acc: 0.509766]  [A loss: 0.885035, acc: 0.132812]\n",
      "2093: [D loss: 0.688196, acc: 0.542969]  [A loss: 0.758317, acc: 0.390625]\n",
      "2094: [D loss: 0.712133, acc: 0.523438]  [A loss: 0.903480, acc: 0.093750]\n",
      "2095: [D loss: 0.690055, acc: 0.546875]  [A loss: 0.715929, acc: 0.429688]\n",
      "2096: [D loss: 0.703552, acc: 0.503906]  [A loss: 0.947313, acc: 0.093750]\n",
      "2097: [D loss: 0.692804, acc: 0.535156]  [A loss: 0.714636, acc: 0.500000]\n",
      "2098: [D loss: 0.708277, acc: 0.521484]  [A loss: 0.860292, acc: 0.128906]\n",
      "2099: [D loss: 0.684307, acc: 0.550781]  [A loss: 0.823951, acc: 0.222656]\n",
      "2100: [D loss: 0.698509, acc: 0.529297]  [A loss: 0.792939, acc: 0.273438]\n",
      "2101: [D loss: 0.691319, acc: 0.521484]  [A loss: 0.836716, acc: 0.199219]\n",
      "2102: [D loss: 0.698552, acc: 0.525391]  [A loss: 0.848740, acc: 0.167969]\n",
      "2103: [D loss: 0.688657, acc: 0.527344]  [A loss: 0.772638, acc: 0.308594]\n",
      "2104: [D loss: 0.699384, acc: 0.531250]  [A loss: 0.836262, acc: 0.183594]\n",
      "2105: [D loss: 0.683217, acc: 0.572266]  [A loss: 0.779664, acc: 0.320312]\n",
      "2106: [D loss: 0.702450, acc: 0.519531]  [A loss: 0.917637, acc: 0.136719]\n",
      "2107: [D loss: 0.684440, acc: 0.550781]  [A loss: 0.703305, acc: 0.492188]\n",
      "2108: [D loss: 0.716261, acc: 0.500000]  [A loss: 1.002900, acc: 0.085938]\n",
      "2109: [D loss: 0.700304, acc: 0.515625]  [A loss: 0.766828, acc: 0.324219]\n",
      "2110: [D loss: 0.688552, acc: 0.554688]  [A loss: 0.858758, acc: 0.144531]\n",
      "2111: [D loss: 0.696604, acc: 0.511719]  [A loss: 0.757446, acc: 0.351562]\n",
      "2112: [D loss: 0.691761, acc: 0.580078]  [A loss: 0.850926, acc: 0.156250]\n",
      "2113: [D loss: 0.698631, acc: 0.513672]  [A loss: 0.797880, acc: 0.242188]\n",
      "2114: [D loss: 0.696375, acc: 0.505859]  [A loss: 0.825817, acc: 0.214844]\n",
      "2115: [D loss: 0.696242, acc: 0.544922]  [A loss: 0.854891, acc: 0.187500]\n",
      "2116: [D loss: 0.685033, acc: 0.552734]  [A loss: 0.811713, acc: 0.277344]\n",
      "2117: [D loss: 0.706166, acc: 0.494141]  [A loss: 0.845186, acc: 0.199219]\n",
      "2118: [D loss: 0.698976, acc: 0.507812]  [A loss: 0.883506, acc: 0.105469]\n",
      "2119: [D loss: 0.689114, acc: 0.554688]  [A loss: 0.852822, acc: 0.167969]\n",
      "2120: [D loss: 0.706040, acc: 0.500000]  [A loss: 0.852780, acc: 0.191406]\n",
      "2121: [D loss: 0.691602, acc: 0.513672]  [A loss: 0.772356, acc: 0.324219]\n",
      "2122: [D loss: 0.701681, acc: 0.523438]  [A loss: 0.953410, acc: 0.089844]\n",
      "2123: [D loss: 0.705552, acc: 0.490234]  [A loss: 0.751883, acc: 0.347656]\n",
      "2124: [D loss: 0.705804, acc: 0.523438]  [A loss: 1.039663, acc: 0.046875]\n",
      "2125: [D loss: 0.682527, acc: 0.546875]  [A loss: 0.635447, acc: 0.683594]\n",
      "2126: [D loss: 0.730817, acc: 0.513672]  [A loss: 0.986367, acc: 0.046875]\n",
      "2127: [D loss: 0.700478, acc: 0.501953]  [A loss: 0.723134, acc: 0.457031]\n",
      "2128: [D loss: 0.710857, acc: 0.501953]  [A loss: 0.843988, acc: 0.175781]\n",
      "2129: [D loss: 0.689986, acc: 0.535156]  [A loss: 0.741401, acc: 0.421875]\n",
      "2130: [D loss: 0.702893, acc: 0.501953]  [A loss: 0.876381, acc: 0.164062]\n",
      "2131: [D loss: 0.681419, acc: 0.552734]  [A loss: 0.756634, acc: 0.394531]\n",
      "2132: [D loss: 0.707661, acc: 0.523438]  [A loss: 0.884268, acc: 0.117188]\n",
      "2133: [D loss: 0.682435, acc: 0.556641]  [A loss: 0.737721, acc: 0.386719]\n",
      "2134: [D loss: 0.707996, acc: 0.527344]  [A loss: 0.807079, acc: 0.265625]\n",
      "2135: [D loss: 0.684285, acc: 0.548828]  [A loss: 0.809887, acc: 0.265625]\n",
      "2136: [D loss: 0.693375, acc: 0.539062]  [A loss: 0.783746, acc: 0.289062]\n",
      "2137: [D loss: 0.698503, acc: 0.537109]  [A loss: 0.858122, acc: 0.167969]\n",
      "2138: [D loss: 0.699471, acc: 0.490234]  [A loss: 0.785999, acc: 0.269531]\n",
      "2139: [D loss: 0.706141, acc: 0.525391]  [A loss: 0.907639, acc: 0.105469]\n",
      "2140: [D loss: 0.684959, acc: 0.566406]  [A loss: 0.727211, acc: 0.437500]\n",
      "2141: [D loss: 0.713810, acc: 0.500000]  [A loss: 0.989402, acc: 0.085938]\n",
      "2142: [D loss: 0.692969, acc: 0.535156]  [A loss: 0.706015, acc: 0.511719]\n",
      "2143: [D loss: 0.705845, acc: 0.507812]  [A loss: 0.947288, acc: 0.074219]\n",
      "2144: [D loss: 0.701500, acc: 0.535156]  [A loss: 0.748113, acc: 0.375000]\n",
      "2145: [D loss: 0.688968, acc: 0.527344]  [A loss: 0.844918, acc: 0.222656]\n",
      "2146: [D loss: 0.693487, acc: 0.564453]  [A loss: 0.747963, acc: 0.410156]\n",
      "2147: [D loss: 0.708172, acc: 0.494141]  [A loss: 0.849849, acc: 0.171875]\n",
      "2148: [D loss: 0.704200, acc: 0.525391]  [A loss: 0.975776, acc: 0.070312]\n",
      "2149: [D loss: 0.689017, acc: 0.539062]  [A loss: 0.711019, acc: 0.492188]\n",
      "2150: [D loss: 0.711869, acc: 0.521484]  [A loss: 0.802711, acc: 0.257812]\n",
      "2151: [D loss: 0.694610, acc: 0.527344]  [A loss: 0.802606, acc: 0.273438]\n",
      "2152: [D loss: 0.702075, acc: 0.511719]  [A loss: 0.781396, acc: 0.324219]\n",
      "2153: [D loss: 0.696506, acc: 0.529297]  [A loss: 0.863818, acc: 0.187500]\n",
      "2154: [D loss: 0.695948, acc: 0.531250]  [A loss: 0.792688, acc: 0.300781]\n",
      "2155: [D loss: 0.708415, acc: 0.517578]  [A loss: 0.839403, acc: 0.207031]\n",
      "2156: [D loss: 0.688547, acc: 0.535156]  [A loss: 0.852430, acc: 0.203125]\n",
      "2157: [D loss: 0.706452, acc: 0.484375]  [A loss: 0.809073, acc: 0.257812]\n",
      "2158: [D loss: 0.706678, acc: 0.515625]  [A loss: 0.945389, acc: 0.093750]\n",
      "2159: [D loss: 0.680086, acc: 0.542969]  [A loss: 0.709994, acc: 0.488281]\n",
      "2160: [D loss: 0.703858, acc: 0.550781]  [A loss: 0.977242, acc: 0.082031]\n",
      "2161: [D loss: 0.696202, acc: 0.531250]  [A loss: 0.685730, acc: 0.531250]\n",
      "2162: [D loss: 0.719028, acc: 0.507812]  [A loss: 0.935056, acc: 0.117188]\n",
      "2163: [D loss: 0.698931, acc: 0.517578]  [A loss: 0.752250, acc: 0.363281]\n",
      "2164: [D loss: 0.711192, acc: 0.505859]  [A loss: 0.824424, acc: 0.218750]\n",
      "2165: [D loss: 0.692418, acc: 0.535156]  [A loss: 0.779776, acc: 0.304688]\n",
      "2166: [D loss: 0.697302, acc: 0.527344]  [A loss: 0.791742, acc: 0.328125]\n",
      "2167: [D loss: 0.696793, acc: 0.529297]  [A loss: 0.826268, acc: 0.183594]\n",
      "2168: [D loss: 0.688895, acc: 0.562500]  [A loss: 0.771125, acc: 0.285156]\n",
      "2169: [D loss: 0.691200, acc: 0.521484]  [A loss: 0.875602, acc: 0.164062]\n",
      "2170: [D loss: 0.703735, acc: 0.513672]  [A loss: 0.784212, acc: 0.296875]\n",
      "2171: [D loss: 0.701068, acc: 0.509766]  [A loss: 0.925108, acc: 0.109375]\n",
      "2172: [D loss: 0.689058, acc: 0.531250]  [A loss: 0.737307, acc: 0.417969]\n",
      "2173: [D loss: 0.707697, acc: 0.525391]  [A loss: 0.945521, acc: 0.058594]\n",
      "2174: [D loss: 0.680657, acc: 0.570312]  [A loss: 0.708587, acc: 0.476562]\n",
      "2175: [D loss: 0.718093, acc: 0.498047]  [A loss: 0.946046, acc: 0.085938]\n",
      "2176: [D loss: 0.696368, acc: 0.515625]  [A loss: 0.809211, acc: 0.257812]\n",
      "2177: [D loss: 0.696974, acc: 0.533203]  [A loss: 0.840250, acc: 0.234375]\n",
      "2178: [D loss: 0.691112, acc: 0.531250]  [A loss: 0.744784, acc: 0.378906]\n",
      "2179: [D loss: 0.703812, acc: 0.541016]  [A loss: 0.877922, acc: 0.125000]\n",
      "2180: [D loss: 0.685314, acc: 0.566406]  [A loss: 0.808812, acc: 0.261719]\n",
      "2181: [D loss: 0.705056, acc: 0.507812]  [A loss: 0.826594, acc: 0.238281]\n",
      "2182: [D loss: 0.686966, acc: 0.546875]  [A loss: 0.801703, acc: 0.296875]\n",
      "2183: [D loss: 0.702518, acc: 0.505859]  [A loss: 0.903777, acc: 0.125000]\n",
      "2184: [D loss: 0.686778, acc: 0.541016]  [A loss: 0.761877, acc: 0.339844]\n",
      "2185: [D loss: 0.705477, acc: 0.541016]  [A loss: 0.927286, acc: 0.136719]\n",
      "2186: [D loss: 0.707613, acc: 0.513672]  [A loss: 0.778123, acc: 0.308594]\n",
      "2187: [D loss: 0.700641, acc: 0.511719]  [A loss: 0.872273, acc: 0.171875]\n",
      "2188: [D loss: 0.698344, acc: 0.509766]  [A loss: 0.814966, acc: 0.253906]\n",
      "2189: [D loss: 0.696288, acc: 0.535156]  [A loss: 0.943930, acc: 0.085938]\n",
      "2190: [D loss: 0.691833, acc: 0.527344]  [A loss: 0.753351, acc: 0.371094]\n",
      "2191: [D loss: 0.696930, acc: 0.529297]  [A loss: 0.863774, acc: 0.164062]\n",
      "2192: [D loss: 0.708498, acc: 0.505859]  [A loss: 0.799179, acc: 0.269531]\n",
      "2193: [D loss: 0.696176, acc: 0.550781]  [A loss: 0.876048, acc: 0.156250]\n",
      "2194: [D loss: 0.688546, acc: 0.546875]  [A loss: 0.804618, acc: 0.300781]\n",
      "2195: [D loss: 0.691155, acc: 0.537109]  [A loss: 0.902074, acc: 0.097656]\n",
      "2196: [D loss: 0.696900, acc: 0.509766]  [A loss: 0.802639, acc: 0.281250]\n",
      "2197: [D loss: 0.703709, acc: 0.496094]  [A loss: 0.838069, acc: 0.238281]\n",
      "2198: [D loss: 0.680352, acc: 0.576172]  [A loss: 0.829005, acc: 0.199219]\n",
      "2199: [D loss: 0.703878, acc: 0.519531]  [A loss: 0.886751, acc: 0.132812]\n",
      "2200: [D loss: 0.701639, acc: 0.541016]  [A loss: 0.954289, acc: 0.082031]\n",
      "2201: [D loss: 0.704328, acc: 0.490234]  [A loss: 0.694543, acc: 0.519531]\n",
      "2202: [D loss: 0.717048, acc: 0.515625]  [A loss: 0.991491, acc: 0.082031]\n",
      "2203: [D loss: 0.686629, acc: 0.562500]  [A loss: 0.625293, acc: 0.687500]\n",
      "2204: [D loss: 0.766025, acc: 0.501953]  [A loss: 1.038942, acc: 0.042969]\n",
      "2205: [D loss: 0.716586, acc: 0.498047]  [A loss: 0.773056, acc: 0.347656]\n",
      "2206: [D loss: 0.710306, acc: 0.509766]  [A loss: 0.819515, acc: 0.242188]\n",
      "2207: [D loss: 0.699204, acc: 0.542969]  [A loss: 0.847678, acc: 0.187500]\n",
      "2208: [D loss: 0.688190, acc: 0.546875]  [A loss: 0.771723, acc: 0.363281]\n",
      "2209: [D loss: 0.706067, acc: 0.523438]  [A loss: 0.862658, acc: 0.164062]\n",
      "2210: [D loss: 0.698386, acc: 0.533203]  [A loss: 0.801745, acc: 0.246094]\n",
      "2211: [D loss: 0.721509, acc: 0.464844]  [A loss: 0.903513, acc: 0.144531]\n",
      "2212: [D loss: 0.695984, acc: 0.531250]  [A loss: 0.716180, acc: 0.429688]\n",
      "2213: [D loss: 0.695069, acc: 0.544922]  [A loss: 0.935812, acc: 0.101562]\n",
      "2214: [D loss: 0.691456, acc: 0.533203]  [A loss: 0.731408, acc: 0.429688]\n",
      "2215: [D loss: 0.712553, acc: 0.503906]  [A loss: 0.878974, acc: 0.167969]\n",
      "2216: [D loss: 0.681745, acc: 0.560547]  [A loss: 0.766501, acc: 0.343750]\n",
      "2217: [D loss: 0.700933, acc: 0.531250]  [A loss: 0.859835, acc: 0.156250]\n",
      "2218: [D loss: 0.695390, acc: 0.548828]  [A loss: 0.831244, acc: 0.226562]\n",
      "2219: [D loss: 0.699798, acc: 0.525391]  [A loss: 0.812422, acc: 0.242188]\n",
      "2220: [D loss: 0.693694, acc: 0.550781]  [A loss: 0.836607, acc: 0.234375]\n",
      "2221: [D loss: 0.699253, acc: 0.503906]  [A loss: 0.817687, acc: 0.257812]\n",
      "2222: [D loss: 0.684971, acc: 0.542969]  [A loss: 0.876116, acc: 0.191406]\n",
      "2223: [D loss: 0.709058, acc: 0.505859]  [A loss: 0.765701, acc: 0.316406]\n",
      "2224: [D loss: 0.720399, acc: 0.490234]  [A loss: 1.015246, acc: 0.031250]\n",
      "2225: [D loss: 0.691910, acc: 0.509766]  [A loss: 0.669331, acc: 0.574219]\n",
      "2226: [D loss: 0.747819, acc: 0.496094]  [A loss: 1.002373, acc: 0.050781]\n",
      "2227: [D loss: 0.693422, acc: 0.544922]  [A loss: 0.735142, acc: 0.394531]\n",
      "2228: [D loss: 0.733910, acc: 0.496094]  [A loss: 1.005287, acc: 0.074219]\n",
      "2229: [D loss: 0.686578, acc: 0.554688]  [A loss: 0.698299, acc: 0.507812]\n",
      "2230: [D loss: 0.715031, acc: 0.523438]  [A loss: 0.872616, acc: 0.171875]\n",
      "2231: [D loss: 0.686579, acc: 0.566406]  [A loss: 0.790756, acc: 0.308594]\n",
      "2232: [D loss: 0.699301, acc: 0.523438]  [A loss: 0.802232, acc: 0.265625]\n",
      "2233: [D loss: 0.690237, acc: 0.541016]  [A loss: 0.808622, acc: 0.250000]\n",
      "2234: [D loss: 0.693506, acc: 0.558594]  [A loss: 0.818992, acc: 0.230469]\n",
      "2235: [D loss: 0.703558, acc: 0.529297]  [A loss: 0.822287, acc: 0.210938]\n",
      "2236: [D loss: 0.706869, acc: 0.539062]  [A loss: 0.938157, acc: 0.070312]\n",
      "2237: [D loss: 0.690346, acc: 0.544922]  [A loss: 0.721282, acc: 0.425781]\n",
      "2238: [D loss: 0.728601, acc: 0.478516]  [A loss: 0.965900, acc: 0.070312]\n",
      "2239: [D loss: 0.687291, acc: 0.521484]  [A loss: 0.702002, acc: 0.500000]\n",
      "2240: [D loss: 0.744782, acc: 0.507812]  [A loss: 0.899670, acc: 0.117188]\n",
      "2241: [D loss: 0.699479, acc: 0.505859]  [A loss: 0.857836, acc: 0.167969]\n",
      "2242: [D loss: 0.690288, acc: 0.544922]  [A loss: 0.843184, acc: 0.144531]\n",
      "2243: [D loss: 0.694366, acc: 0.517578]  [A loss: 0.847778, acc: 0.199219]\n",
      "2244: [D loss: 0.696343, acc: 0.537109]  [A loss: 0.851593, acc: 0.210938]\n",
      "2245: [D loss: 0.702379, acc: 0.511719]  [A loss: 0.852563, acc: 0.164062]\n",
      "2246: [D loss: 0.688003, acc: 0.523438]  [A loss: 0.848787, acc: 0.210938]\n",
      "2247: [D loss: 0.690815, acc: 0.554688]  [A loss: 0.786437, acc: 0.273438]\n",
      "2248: [D loss: 0.701584, acc: 0.519531]  [A loss: 0.847215, acc: 0.183594]\n",
      "2249: [D loss: 0.704369, acc: 0.478516]  [A loss: 0.813984, acc: 0.257812]\n",
      "2250: [D loss: 0.687921, acc: 0.541016]  [A loss: 0.786305, acc: 0.332031]\n",
      "2251: [D loss: 0.696560, acc: 0.537109]  [A loss: 0.857271, acc: 0.160156]\n",
      "2252: [D loss: 0.699957, acc: 0.533203]  [A loss: 0.861029, acc: 0.195312]\n",
      "2253: [D loss: 0.689660, acc: 0.529297]  [A loss: 0.758231, acc: 0.316406]\n",
      "2254: [D loss: 0.720829, acc: 0.500000]  [A loss: 1.064104, acc: 0.027344]\n",
      "2255: [D loss: 0.713744, acc: 0.505859]  [A loss: 0.629364, acc: 0.703125]\n",
      "2256: [D loss: 0.755454, acc: 0.498047]  [A loss: 0.982911, acc: 0.031250]\n",
      "2257: [D loss: 0.702595, acc: 0.500000]  [A loss: 0.720691, acc: 0.441406]\n",
      "2258: [D loss: 0.709747, acc: 0.511719]  [A loss: 0.898260, acc: 0.109375]\n",
      "2259: [D loss: 0.691057, acc: 0.552734]  [A loss: 0.757857, acc: 0.328125]\n",
      "2260: [D loss: 0.695598, acc: 0.556641]  [A loss: 0.835266, acc: 0.207031]\n",
      "2261: [D loss: 0.695679, acc: 0.527344]  [A loss: 0.798429, acc: 0.265625]\n",
      "2262: [D loss: 0.714710, acc: 0.484375]  [A loss: 0.822281, acc: 0.214844]\n",
      "2263: [D loss: 0.692834, acc: 0.521484]  [A loss: 0.821743, acc: 0.238281]\n",
      "2264: [D loss: 0.690496, acc: 0.546875]  [A loss: 0.818092, acc: 0.210938]\n",
      "2265: [D loss: 0.684727, acc: 0.582031]  [A loss: 0.844417, acc: 0.234375]\n",
      "2266: [D loss: 0.719274, acc: 0.482422]  [A loss: 0.855476, acc: 0.179688]\n",
      "2267: [D loss: 0.690730, acc: 0.542969]  [A loss: 0.947528, acc: 0.097656]\n",
      "2268: [D loss: 0.690118, acc: 0.529297]  [A loss: 0.684826, acc: 0.554688]\n",
      "2269: [D loss: 0.713944, acc: 0.517578]  [A loss: 0.957846, acc: 0.082031]\n",
      "2270: [D loss: 0.701592, acc: 0.509766]  [A loss: 0.694744, acc: 0.503906]\n",
      "2271: [D loss: 0.737891, acc: 0.498047]  [A loss: 1.038216, acc: 0.054688]\n",
      "2272: [D loss: 0.694266, acc: 0.541016]  [A loss: 0.685596, acc: 0.539062]\n",
      "2273: [D loss: 0.701143, acc: 0.535156]  [A loss: 0.851580, acc: 0.144531]\n",
      "2274: [D loss: 0.682624, acc: 0.550781]  [A loss: 0.749478, acc: 0.406250]\n",
      "2275: [D loss: 0.705595, acc: 0.500000]  [A loss: 0.880768, acc: 0.125000]\n",
      "2276: [D loss: 0.677510, acc: 0.597656]  [A loss: 0.740595, acc: 0.390625]\n",
      "2277: [D loss: 0.718020, acc: 0.470703]  [A loss: 0.920664, acc: 0.132812]\n",
      "2278: [D loss: 0.699147, acc: 0.542969]  [A loss: 0.781837, acc: 0.292969]\n",
      "2279: [D loss: 0.713033, acc: 0.527344]  [A loss: 0.884459, acc: 0.148438]\n",
      "2280: [D loss: 0.687929, acc: 0.546875]  [A loss: 0.733566, acc: 0.421875]\n",
      "2281: [D loss: 0.701297, acc: 0.541016]  [A loss: 0.858544, acc: 0.167969]\n",
      "2282: [D loss: 0.694633, acc: 0.560547]  [A loss: 0.756814, acc: 0.351562]\n",
      "2283: [D loss: 0.708958, acc: 0.517578]  [A loss: 0.954880, acc: 0.082031]\n",
      "2284: [D loss: 0.707045, acc: 0.505859]  [A loss: 0.710164, acc: 0.464844]\n",
      "2285: [D loss: 0.704356, acc: 0.529297]  [A loss: 0.897214, acc: 0.136719]\n",
      "2286: [D loss: 0.695084, acc: 0.537109]  [A loss: 0.738585, acc: 0.394531]\n",
      "2287: [D loss: 0.696724, acc: 0.552734]  [A loss: 0.890044, acc: 0.148438]\n",
      "2288: [D loss: 0.689719, acc: 0.511719]  [A loss: 0.765874, acc: 0.328125]\n",
      "2289: [D loss: 0.696848, acc: 0.541016]  [A loss: 0.923097, acc: 0.105469]\n",
      "2290: [D loss: 0.687514, acc: 0.566406]  [A loss: 0.709325, acc: 0.488281]\n",
      "2291: [D loss: 0.707016, acc: 0.521484]  [A loss: 0.894328, acc: 0.140625]\n",
      "2292: [D loss: 0.709378, acc: 0.503906]  [A loss: 0.787149, acc: 0.320312]\n",
      "2293: [D loss: 0.687578, acc: 0.535156]  [A loss: 0.888757, acc: 0.136719]\n",
      "2294: [D loss: 0.694825, acc: 0.539062]  [A loss: 0.761573, acc: 0.371094]\n",
      "2295: [D loss: 0.728503, acc: 0.482422]  [A loss: 0.982907, acc: 0.066406]\n",
      "2296: [D loss: 0.695392, acc: 0.507812]  [A loss: 0.699405, acc: 0.507812]\n",
      "2297: [D loss: 0.727156, acc: 0.492188]  [A loss: 0.980073, acc: 0.050781]\n",
      "2298: [D loss: 0.686723, acc: 0.533203]  [A loss: 0.743313, acc: 0.382812]\n",
      "2299: [D loss: 0.699502, acc: 0.527344]  [A loss: 0.849428, acc: 0.207031]\n",
      "2300: [D loss: 0.708534, acc: 0.519531]  [A loss: 0.845789, acc: 0.171875]\n",
      "2301: [D loss: 0.684948, acc: 0.552734]  [A loss: 0.845235, acc: 0.195312]\n",
      "2302: [D loss: 0.690737, acc: 0.535156]  [A loss: 0.773375, acc: 0.292969]\n",
      "2303: [D loss: 0.695044, acc: 0.525391]  [A loss: 0.929535, acc: 0.074219]\n",
      "2304: [D loss: 0.691983, acc: 0.523438]  [A loss: 0.744439, acc: 0.378906]\n",
      "2305: [D loss: 0.718714, acc: 0.505859]  [A loss: 0.917800, acc: 0.109375]\n",
      "2306: [D loss: 0.686337, acc: 0.542969]  [A loss: 0.693996, acc: 0.531250]\n",
      "2307: [D loss: 0.718101, acc: 0.511719]  [A loss: 0.960848, acc: 0.085938]\n",
      "2308: [D loss: 0.698298, acc: 0.501953]  [A loss: 0.706335, acc: 0.460938]\n",
      "2309: [D loss: 0.722086, acc: 0.505859]  [A loss: 0.960222, acc: 0.070312]\n",
      "2310: [D loss: 0.689728, acc: 0.550781]  [A loss: 0.787661, acc: 0.300781]\n",
      "2311: [D loss: 0.702072, acc: 0.513672]  [A loss: 0.893427, acc: 0.140625]\n",
      "2312: [D loss: 0.699574, acc: 0.501953]  [A loss: 0.808557, acc: 0.265625]\n",
      "2313: [D loss: 0.701163, acc: 0.539062]  [A loss: 0.846952, acc: 0.242188]\n",
      "2314: [D loss: 0.684153, acc: 0.556641]  [A loss: 0.824301, acc: 0.222656]\n",
      "2315: [D loss: 0.698375, acc: 0.488281]  [A loss: 0.822148, acc: 0.222656]\n",
      "2316: [D loss: 0.696067, acc: 0.541016]  [A loss: 0.819954, acc: 0.269531]\n",
      "2317: [D loss: 0.687179, acc: 0.539062]  [A loss: 0.857345, acc: 0.164062]\n",
      "2318: [D loss: 0.692827, acc: 0.544922]  [A loss: 0.830397, acc: 0.179688]\n",
      "2319: [D loss: 0.692159, acc: 0.531250]  [A loss: 0.847385, acc: 0.191406]\n",
      "2320: [D loss: 0.695280, acc: 0.541016]  [A loss: 0.790208, acc: 0.296875]\n",
      "2321: [D loss: 0.714225, acc: 0.492188]  [A loss: 0.976003, acc: 0.074219]\n",
      "2322: [D loss: 0.688623, acc: 0.550781]  [A loss: 0.643307, acc: 0.648438]\n",
      "2323: [D loss: 0.738085, acc: 0.501953]  [A loss: 0.996793, acc: 0.062500]\n",
      "2324: [D loss: 0.695156, acc: 0.525391]  [A loss: 0.704278, acc: 0.472656]\n",
      "2325: [D loss: 0.740643, acc: 0.488281]  [A loss: 1.005548, acc: 0.078125]\n",
      "2326: [D loss: 0.718868, acc: 0.488281]  [A loss: 0.774714, acc: 0.300781]\n",
      "2327: [D loss: 0.701495, acc: 0.542969]  [A loss: 0.867713, acc: 0.136719]\n",
      "2328: [D loss: 0.692024, acc: 0.556641]  [A loss: 0.860366, acc: 0.152344]\n",
      "2329: [D loss: 0.694492, acc: 0.513672]  [A loss: 0.780169, acc: 0.312500]\n",
      "2330: [D loss: 0.701447, acc: 0.513672]  [A loss: 0.904873, acc: 0.113281]\n",
      "2331: [D loss: 0.683453, acc: 0.568359]  [A loss: 0.712122, acc: 0.476562]\n",
      "2332: [D loss: 0.703972, acc: 0.533203]  [A loss: 0.851961, acc: 0.156250]\n",
      "2333: [D loss: 0.699745, acc: 0.515625]  [A loss: 0.746882, acc: 0.363281]\n",
      "2334: [D loss: 0.710534, acc: 0.509766]  [A loss: 0.930631, acc: 0.082031]\n",
      "2335: [D loss: 0.694258, acc: 0.509766]  [A loss: 0.720310, acc: 0.433594]\n",
      "2336: [D loss: 0.697885, acc: 0.527344]  [A loss: 0.877797, acc: 0.171875]\n",
      "2337: [D loss: 0.688819, acc: 0.539062]  [A loss: 0.764393, acc: 0.363281]\n",
      "2338: [D loss: 0.704076, acc: 0.521484]  [A loss: 0.846913, acc: 0.195312]\n",
      "2339: [D loss: 0.703955, acc: 0.492188]  [A loss: 0.787898, acc: 0.308594]\n",
      "2340: [D loss: 0.719158, acc: 0.494141]  [A loss: 1.025147, acc: 0.058594]\n",
      "2341: [D loss: 0.689421, acc: 0.531250]  [A loss: 0.660786, acc: 0.589844]\n",
      "2342: [D loss: 0.735741, acc: 0.466797]  [A loss: 0.906953, acc: 0.113281]\n",
      "2343: [D loss: 0.697448, acc: 0.539062]  [A loss: 0.750313, acc: 0.359375]\n",
      "2344: [D loss: 0.712252, acc: 0.507812]  [A loss: 0.969331, acc: 0.054688]\n",
      "2345: [D loss: 0.689338, acc: 0.542969]  [A loss: 0.715307, acc: 0.460938]\n",
      "2346: [D loss: 0.713019, acc: 0.503906]  [A loss: 0.851325, acc: 0.183594]\n",
      "2347: [D loss: 0.689474, acc: 0.539062]  [A loss: 0.746361, acc: 0.347656]\n",
      "2348: [D loss: 0.696390, acc: 0.519531]  [A loss: 0.817489, acc: 0.234375]\n",
      "2349: [D loss: 0.701705, acc: 0.515625]  [A loss: 0.815177, acc: 0.265625]\n",
      "2350: [D loss: 0.689633, acc: 0.544922]  [A loss: 0.829198, acc: 0.234375]\n",
      "2351: [D loss: 0.699337, acc: 0.513672]  [A loss: 0.769126, acc: 0.359375]\n",
      "2352: [D loss: 0.692339, acc: 0.539062]  [A loss: 0.799666, acc: 0.246094]\n",
      "2353: [D loss: 0.689089, acc: 0.552734]  [A loss: 0.842119, acc: 0.156250]\n",
      "2354: [D loss: 0.698470, acc: 0.501953]  [A loss: 0.762451, acc: 0.339844]\n",
      "2355: [D loss: 0.714215, acc: 0.486328]  [A loss: 0.940794, acc: 0.078125]\n",
      "2356: [D loss: 0.702906, acc: 0.513672]  [A loss: 0.705857, acc: 0.453125]\n",
      "2357: [D loss: 0.712491, acc: 0.521484]  [A loss: 0.976805, acc: 0.054688]\n",
      "2358: [D loss: 0.690490, acc: 0.537109]  [A loss: 0.688630, acc: 0.507812]\n",
      "2359: [D loss: 0.706696, acc: 0.533203]  [A loss: 0.911304, acc: 0.117188]\n",
      "2360: [D loss: 0.699000, acc: 0.515625]  [A loss: 0.767280, acc: 0.324219]\n",
      "2361: [D loss: 0.718934, acc: 0.503906]  [A loss: 0.860136, acc: 0.148438]\n",
      "2362: [D loss: 0.690814, acc: 0.541016]  [A loss: 0.746684, acc: 0.355469]\n",
      "2363: [D loss: 0.698244, acc: 0.544922]  [A loss: 0.880599, acc: 0.105469]\n",
      "2364: [D loss: 0.694149, acc: 0.521484]  [A loss: 0.726254, acc: 0.437500]\n",
      "2365: [D loss: 0.698126, acc: 0.521484]  [A loss: 0.929204, acc: 0.093750]\n",
      "2366: [D loss: 0.698161, acc: 0.541016]  [A loss: 0.769947, acc: 0.296875]\n",
      "2367: [D loss: 0.699047, acc: 0.523438]  [A loss: 0.868241, acc: 0.152344]\n",
      "2368: [D loss: 0.694404, acc: 0.529297]  [A loss: 0.728288, acc: 0.449219]\n",
      "2369: [D loss: 0.710849, acc: 0.500000]  [A loss: 0.960817, acc: 0.070312]\n",
      "2370: [D loss: 0.685465, acc: 0.552734]  [A loss: 0.706474, acc: 0.500000]\n",
      "2371: [D loss: 0.715764, acc: 0.492188]  [A loss: 0.914385, acc: 0.113281]\n",
      "2372: [D loss: 0.704831, acc: 0.515625]  [A loss: 0.713141, acc: 0.429688]\n",
      "2373: [D loss: 0.707994, acc: 0.523438]  [A loss: 0.943812, acc: 0.101562]\n",
      "2374: [D loss: 0.714661, acc: 0.466797]  [A loss: 0.751405, acc: 0.355469]\n",
      "2375: [D loss: 0.690038, acc: 0.533203]  [A loss: 0.840828, acc: 0.179688]\n",
      "2376: [D loss: 0.697426, acc: 0.525391]  [A loss: 0.770854, acc: 0.335938]\n",
      "2377: [D loss: 0.694163, acc: 0.525391]  [A loss: 0.865817, acc: 0.167969]\n",
      "2378: [D loss: 0.691268, acc: 0.552734]  [A loss: 0.819104, acc: 0.195312]\n",
      "2379: [D loss: 0.695461, acc: 0.542969]  [A loss: 0.769641, acc: 0.300781]\n",
      "2380: [D loss: 0.709023, acc: 0.503906]  [A loss: 0.887268, acc: 0.144531]\n",
      "2381: [D loss: 0.684093, acc: 0.578125]  [A loss: 0.744737, acc: 0.351562]\n",
      "2382: [D loss: 0.703744, acc: 0.523438]  [A loss: 0.929173, acc: 0.117188]\n",
      "2383: [D loss: 0.681081, acc: 0.562500]  [A loss: 0.732678, acc: 0.441406]\n",
      "2384: [D loss: 0.721607, acc: 0.488281]  [A loss: 0.952515, acc: 0.109375]\n",
      "2385: [D loss: 0.702157, acc: 0.523438]  [A loss: 0.722044, acc: 0.468750]\n",
      "2386: [D loss: 0.709816, acc: 0.513672]  [A loss: 0.900349, acc: 0.128906]\n",
      "2387: [D loss: 0.702434, acc: 0.550781]  [A loss: 0.729985, acc: 0.445312]\n",
      "2388: [D loss: 0.712069, acc: 0.507812]  [A loss: 0.897832, acc: 0.164062]\n",
      "2389: [D loss: 0.684578, acc: 0.580078]  [A loss: 0.764230, acc: 0.351562]\n",
      "2390: [D loss: 0.714576, acc: 0.498047]  [A loss: 0.927366, acc: 0.109375]\n",
      "2391: [D loss: 0.697008, acc: 0.517578]  [A loss: 0.813436, acc: 0.234375]\n",
      "2392: [D loss: 0.710555, acc: 0.492188]  [A loss: 0.853357, acc: 0.164062]\n",
      "2393: [D loss: 0.702075, acc: 0.527344]  [A loss: 0.828313, acc: 0.226562]\n",
      "2394: [D loss: 0.704463, acc: 0.503906]  [A loss: 0.809218, acc: 0.226562]\n",
      "2395: [D loss: 0.699358, acc: 0.537109]  [A loss: 0.834531, acc: 0.234375]\n",
      "2396: [D loss: 0.704110, acc: 0.509766]  [A loss: 0.901090, acc: 0.121094]\n",
      "2397: [D loss: 0.690467, acc: 0.531250]  [A loss: 0.806811, acc: 0.250000]\n",
      "2398: [D loss: 0.697582, acc: 0.515625]  [A loss: 0.853339, acc: 0.195312]\n",
      "2399: [D loss: 0.690630, acc: 0.515625]  [A loss: 0.784354, acc: 0.316406]\n",
      "2400: [D loss: 0.704385, acc: 0.501953]  [A loss: 0.882829, acc: 0.167969]\n",
      "2401: [D loss: 0.697371, acc: 0.515625]  [A loss: 0.802583, acc: 0.265625]\n",
      "2402: [D loss: 0.704553, acc: 0.501953]  [A loss: 0.919805, acc: 0.128906]\n",
      "2403: [D loss: 0.694138, acc: 0.539062]  [A loss: 0.746299, acc: 0.359375]\n",
      "2404: [D loss: 0.710900, acc: 0.521484]  [A loss: 1.045998, acc: 0.042969]\n",
      "2405: [D loss: 0.698471, acc: 0.509766]  [A loss: 0.620611, acc: 0.695312]\n",
      "2406: [D loss: 0.743533, acc: 0.496094]  [A loss: 0.936115, acc: 0.105469]\n",
      "2407: [D loss: 0.687410, acc: 0.542969]  [A loss: 0.733230, acc: 0.406250]\n",
      "2408: [D loss: 0.727180, acc: 0.492188]  [A loss: 0.979194, acc: 0.070312]\n",
      "2409: [D loss: 0.705747, acc: 0.492188]  [A loss: 0.803330, acc: 0.265625]\n",
      "2410: [D loss: 0.695325, acc: 0.548828]  [A loss: 0.839324, acc: 0.195312]\n",
      "2411: [D loss: 0.699864, acc: 0.492188]  [A loss: 0.807473, acc: 0.273438]\n",
      "2412: [D loss: 0.706023, acc: 0.503906]  [A loss: 0.810404, acc: 0.257812]\n",
      "2413: [D loss: 0.698230, acc: 0.500000]  [A loss: 0.804715, acc: 0.277344]\n",
      "2414: [D loss: 0.706626, acc: 0.500000]  [A loss: 0.766535, acc: 0.355469]\n",
      "2415: [D loss: 0.710922, acc: 0.529297]  [A loss: 0.896791, acc: 0.152344]\n",
      "2416: [D loss: 0.692905, acc: 0.537109]  [A loss: 0.742402, acc: 0.394531]\n",
      "2417: [D loss: 0.716085, acc: 0.535156]  [A loss: 0.935400, acc: 0.101562]\n",
      "2418: [D loss: 0.704714, acc: 0.507812]  [A loss: 0.730956, acc: 0.429688]\n",
      "2419: [D loss: 0.720923, acc: 0.513672]  [A loss: 1.002301, acc: 0.046875]\n",
      "2420: [D loss: 0.689836, acc: 0.527344]  [A loss: 0.660534, acc: 0.613281]\n",
      "2421: [D loss: 0.723804, acc: 0.507812]  [A loss: 0.921945, acc: 0.082031]\n",
      "2422: [D loss: 0.700670, acc: 0.541016]  [A loss: 0.697826, acc: 0.523438]\n",
      "2423: [D loss: 0.713480, acc: 0.529297]  [A loss: 0.935901, acc: 0.054688]\n",
      "2424: [D loss: 0.691586, acc: 0.548828]  [A loss: 0.728408, acc: 0.457031]\n",
      "2425: [D loss: 0.709587, acc: 0.511719]  [A loss: 0.902657, acc: 0.144531]\n",
      "2426: [D loss: 0.694940, acc: 0.529297]  [A loss: 0.736094, acc: 0.441406]\n",
      "2427: [D loss: 0.701730, acc: 0.523438]  [A loss: 0.901916, acc: 0.089844]\n",
      "2428: [D loss: 0.696562, acc: 0.525391]  [A loss: 0.759226, acc: 0.355469]\n",
      "2429: [D loss: 0.709761, acc: 0.515625]  [A loss: 0.894061, acc: 0.128906]\n",
      "2430: [D loss: 0.692359, acc: 0.511719]  [A loss: 0.785836, acc: 0.296875]\n",
      "2431: [D loss: 0.707661, acc: 0.486328]  [A loss: 0.852643, acc: 0.191406]\n",
      "2432: [D loss: 0.690955, acc: 0.539062]  [A loss: 0.780842, acc: 0.308594]\n",
      "2433: [D loss: 0.685740, acc: 0.560547]  [A loss: 0.891868, acc: 0.144531]\n",
      "2434: [D loss: 0.694280, acc: 0.521484]  [A loss: 0.755255, acc: 0.382812]\n",
      "2435: [D loss: 0.714277, acc: 0.472656]  [A loss: 0.891701, acc: 0.128906]\n",
      "2436: [D loss: 0.693611, acc: 0.546875]  [A loss: 0.717988, acc: 0.472656]\n",
      "2437: [D loss: 0.706273, acc: 0.525391]  [A loss: 0.915512, acc: 0.144531]\n",
      "2438: [D loss: 0.710961, acc: 0.498047]  [A loss: 0.750843, acc: 0.402344]\n",
      "2439: [D loss: 0.713448, acc: 0.484375]  [A loss: 0.893039, acc: 0.097656]\n",
      "2440: [D loss: 0.696765, acc: 0.511719]  [A loss: 0.766420, acc: 0.363281]\n",
      "2441: [D loss: 0.695754, acc: 0.544922]  [A loss: 0.899654, acc: 0.085938]\n",
      "2442: [D loss: 0.699394, acc: 0.531250]  [A loss: 0.768978, acc: 0.347656]\n",
      "2443: [D loss: 0.715151, acc: 0.501953]  [A loss: 0.850060, acc: 0.156250]\n",
      "2444: [D loss: 0.689736, acc: 0.550781]  [A loss: 0.875379, acc: 0.156250]\n",
      "2445: [D loss: 0.688875, acc: 0.568359]  [A loss: 0.813723, acc: 0.214844]\n",
      "2446: [D loss: 0.699168, acc: 0.511719]  [A loss: 0.945648, acc: 0.062500]\n",
      "2447: [D loss: 0.697293, acc: 0.539062]  [A loss: 0.767979, acc: 0.355469]\n",
      "2448: [D loss: 0.704524, acc: 0.511719]  [A loss: 0.959161, acc: 0.058594]\n",
      "2449: [D loss: 0.687725, acc: 0.527344]  [A loss: 0.720351, acc: 0.421875]\n",
      "2450: [D loss: 0.719343, acc: 0.486328]  [A loss: 0.954852, acc: 0.085938]\n",
      "2451: [D loss: 0.696378, acc: 0.505859]  [A loss: 0.725484, acc: 0.437500]\n",
      "2452: [D loss: 0.717495, acc: 0.488281]  [A loss: 0.900977, acc: 0.113281]\n",
      "2453: [D loss: 0.696831, acc: 0.529297]  [A loss: 0.685086, acc: 0.542969]\n",
      "2454: [D loss: 0.732821, acc: 0.496094]  [A loss: 1.004135, acc: 0.074219]\n",
      "2455: [D loss: 0.712092, acc: 0.496094]  [A loss: 0.853426, acc: 0.160156]\n",
      "2456: [D loss: 0.700348, acc: 0.486328]  [A loss: 0.780123, acc: 0.308594]\n",
      "2457: [D loss: 0.694795, acc: 0.552734]  [A loss: 0.851259, acc: 0.183594]\n",
      "2458: [D loss: 0.693830, acc: 0.529297]  [A loss: 0.726708, acc: 0.417969]\n",
      "2459: [D loss: 0.708143, acc: 0.498047]  [A loss: 0.937749, acc: 0.070312]\n",
      "2460: [D loss: 0.696167, acc: 0.529297]  [A loss: 0.683762, acc: 0.542969]\n",
      "2461: [D loss: 0.720923, acc: 0.490234]  [A loss: 0.960455, acc: 0.066406]\n",
      "2462: [D loss: 0.693276, acc: 0.509766]  [A loss: 0.760395, acc: 0.359375]\n",
      "2463: [D loss: 0.708370, acc: 0.496094]  [A loss: 0.887088, acc: 0.156250]\n",
      "2464: [D loss: 0.698198, acc: 0.511719]  [A loss: 0.736319, acc: 0.433594]\n",
      "2465: [D loss: 0.709431, acc: 0.496094]  [A loss: 0.863211, acc: 0.140625]\n",
      "2466: [D loss: 0.690226, acc: 0.529297]  [A loss: 0.734007, acc: 0.414062]\n",
      "2467: [D loss: 0.703232, acc: 0.527344]  [A loss: 0.919181, acc: 0.105469]\n",
      "2468: [D loss: 0.684522, acc: 0.564453]  [A loss: 0.788466, acc: 0.308594]\n",
      "2469: [D loss: 0.695072, acc: 0.525391]  [A loss: 0.787239, acc: 0.308594]\n",
      "2470: [D loss: 0.690848, acc: 0.548828]  [A loss: 0.844405, acc: 0.203125]\n",
      "2471: [D loss: 0.713224, acc: 0.490234]  [A loss: 0.818024, acc: 0.207031]\n",
      "2472: [D loss: 0.695129, acc: 0.544922]  [A loss: 0.831583, acc: 0.214844]\n",
      "2473: [D loss: 0.704548, acc: 0.490234]  [A loss: 0.839838, acc: 0.187500]\n",
      "2474: [D loss: 0.685795, acc: 0.544922]  [A loss: 0.852851, acc: 0.152344]\n",
      "2475: [D loss: 0.696030, acc: 0.529297]  [A loss: 0.895079, acc: 0.128906]\n",
      "2476: [D loss: 0.703925, acc: 0.519531]  [A loss: 0.813727, acc: 0.253906]\n",
      "2477: [D loss: 0.684256, acc: 0.552734]  [A loss: 0.813751, acc: 0.238281]\n",
      "2478: [D loss: 0.688962, acc: 0.562500]  [A loss: 0.940505, acc: 0.082031]\n",
      "2479: [D loss: 0.684660, acc: 0.539062]  [A loss: 0.800780, acc: 0.269531]\n",
      "2480: [D loss: 0.706136, acc: 0.515625]  [A loss: 0.976639, acc: 0.093750]\n",
      "2481: [D loss: 0.704208, acc: 0.517578]  [A loss: 0.744085, acc: 0.402344]\n",
      "2482: [D loss: 0.719066, acc: 0.498047]  [A loss: 0.974536, acc: 0.066406]\n",
      "2483: [D loss: 0.695512, acc: 0.509766]  [A loss: 0.682271, acc: 0.589844]\n",
      "2484: [D loss: 0.756903, acc: 0.492188]  [A loss: 1.094854, acc: 0.058594]\n",
      "2485: [D loss: 0.697042, acc: 0.552734]  [A loss: 0.643654, acc: 0.640625]\n",
      "2486: [D loss: 0.798169, acc: 0.480469]  [A loss: 0.917122, acc: 0.082031]\n",
      "2487: [D loss: 0.688216, acc: 0.542969]  [A loss: 0.780007, acc: 0.269531]\n",
      "2488: [D loss: 0.712246, acc: 0.523438]  [A loss: 0.974981, acc: 0.027344]\n",
      "2489: [D loss: 0.689670, acc: 0.521484]  [A loss: 0.735470, acc: 0.398438]\n",
      "2490: [D loss: 0.717878, acc: 0.496094]  [A loss: 0.890881, acc: 0.097656]\n",
      "2491: [D loss: 0.701681, acc: 0.500000]  [A loss: 0.767387, acc: 0.324219]\n",
      "2492: [D loss: 0.706737, acc: 0.501953]  [A loss: 0.857012, acc: 0.191406]\n",
      "2493: [D loss: 0.705604, acc: 0.486328]  [A loss: 0.789130, acc: 0.300781]\n",
      "2494: [D loss: 0.689884, acc: 0.550781]  [A loss: 0.823014, acc: 0.238281]\n",
      "2495: [D loss: 0.705605, acc: 0.523438]  [A loss: 0.765013, acc: 0.363281]\n",
      "2496: [D loss: 0.702610, acc: 0.519531]  [A loss: 0.850380, acc: 0.207031]\n",
      "2497: [D loss: 0.707235, acc: 0.496094]  [A loss: 0.795649, acc: 0.234375]\n",
      "2498: [D loss: 0.696179, acc: 0.533203]  [A loss: 0.832644, acc: 0.183594]\n",
      "2499: [D loss: 0.694969, acc: 0.544922]  [A loss: 0.859990, acc: 0.160156]\n",
      "2500: [D loss: 0.688991, acc: 0.533203]  [A loss: 0.782756, acc: 0.312500]\n",
      "2501: [D loss: 0.717166, acc: 0.480469]  [A loss: 0.970725, acc: 0.039062]\n",
      "2502: [D loss: 0.685102, acc: 0.537109]  [A loss: 0.709869, acc: 0.492188]\n",
      "2503: [D loss: 0.711859, acc: 0.513672]  [A loss: 0.897816, acc: 0.109375]\n",
      "2504: [D loss: 0.688709, acc: 0.535156]  [A loss: 0.742109, acc: 0.359375]\n",
      "2505: [D loss: 0.716155, acc: 0.494141]  [A loss: 0.926299, acc: 0.101562]\n",
      "2506: [D loss: 0.683635, acc: 0.554688]  [A loss: 0.748638, acc: 0.398438]\n",
      "2507: [D loss: 0.707808, acc: 0.511719]  [A loss: 0.886130, acc: 0.113281]\n",
      "2508: [D loss: 0.697314, acc: 0.503906]  [A loss: 0.845699, acc: 0.187500]\n",
      "2509: [D loss: 0.681687, acc: 0.542969]  [A loss: 0.813261, acc: 0.250000]\n",
      "2510: [D loss: 0.703304, acc: 0.505859]  [A loss: 0.844972, acc: 0.175781]\n",
      "2511: [D loss: 0.708351, acc: 0.486328]  [A loss: 0.907167, acc: 0.136719]\n",
      "2512: [D loss: 0.694070, acc: 0.535156]  [A loss: 0.782188, acc: 0.300781]\n",
      "2513: [D loss: 0.704048, acc: 0.519531]  [A loss: 0.876753, acc: 0.156250]\n",
      "2514: [D loss: 0.692310, acc: 0.546875]  [A loss: 0.894400, acc: 0.093750]\n",
      "2515: [D loss: 0.700569, acc: 0.501953]  [A loss: 0.899136, acc: 0.140625]\n",
      "2516: [D loss: 0.706384, acc: 0.513672]  [A loss: 0.790907, acc: 0.308594]\n",
      "2517: [D loss: 0.688811, acc: 0.548828]  [A loss: 0.882351, acc: 0.179688]\n",
      "2518: [D loss: 0.699323, acc: 0.511719]  [A loss: 0.840345, acc: 0.210938]\n",
      "2519: [D loss: 0.686067, acc: 0.570312]  [A loss: 0.869696, acc: 0.148438]\n",
      "2520: [D loss: 0.689219, acc: 0.550781]  [A loss: 0.813713, acc: 0.265625]\n",
      "2521: [D loss: 0.701184, acc: 0.529297]  [A loss: 1.008786, acc: 0.046875]\n",
      "2522: [D loss: 0.699652, acc: 0.546875]  [A loss: 0.612654, acc: 0.691406]\n",
      "2523: [D loss: 0.758874, acc: 0.498047]  [A loss: 1.111834, acc: 0.019531]\n",
      "2524: [D loss: 0.716389, acc: 0.505859]  [A loss: 0.624288, acc: 0.691406]\n",
      "2525: [D loss: 0.762093, acc: 0.492188]  [A loss: 0.896195, acc: 0.089844]\n",
      "2526: [D loss: 0.696941, acc: 0.521484]  [A loss: 0.758234, acc: 0.320312]\n",
      "2527: [D loss: 0.725643, acc: 0.498047]  [A loss: 0.919702, acc: 0.109375]\n",
      "2528: [D loss: 0.690013, acc: 0.548828]  [A loss: 0.751432, acc: 0.332031]\n",
      "2529: [D loss: 0.718628, acc: 0.511719]  [A loss: 0.847112, acc: 0.156250]\n",
      "2530: [D loss: 0.700143, acc: 0.517578]  [A loss: 0.796162, acc: 0.250000]\n",
      "2531: [D loss: 0.710616, acc: 0.513672]  [A loss: 0.861805, acc: 0.164062]\n",
      "2532: [D loss: 0.711380, acc: 0.482422]  [A loss: 0.780244, acc: 0.304688]\n",
      "2533: [D loss: 0.699297, acc: 0.531250]  [A loss: 0.812945, acc: 0.218750]\n",
      "2534: [D loss: 0.705595, acc: 0.498047]  [A loss: 0.806535, acc: 0.238281]\n",
      "2535: [D loss: 0.694830, acc: 0.554688]  [A loss: 0.833594, acc: 0.242188]\n",
      "2536: [D loss: 0.704307, acc: 0.498047]  [A loss: 0.852722, acc: 0.195312]\n",
      "2537: [D loss: 0.698137, acc: 0.535156]  [A loss: 0.769996, acc: 0.343750]\n",
      "2538: [D loss: 0.703580, acc: 0.515625]  [A loss: 0.919373, acc: 0.070312]\n",
      "2539: [D loss: 0.687077, acc: 0.513672]  [A loss: 0.721859, acc: 0.421875]\n",
      "2540: [D loss: 0.707897, acc: 0.535156]  [A loss: 0.875634, acc: 0.132812]\n",
      "2541: [D loss: 0.693009, acc: 0.527344]  [A loss: 0.764516, acc: 0.339844]\n",
      "2542: [D loss: 0.692437, acc: 0.537109]  [A loss: 0.858045, acc: 0.140625]\n",
      "2543: [D loss: 0.702837, acc: 0.488281]  [A loss: 0.802184, acc: 0.250000]\n",
      "2544: [D loss: 0.697080, acc: 0.541016]  [A loss: 0.812689, acc: 0.265625]\n",
      "2545: [D loss: 0.705772, acc: 0.501953]  [A loss: 0.858090, acc: 0.175781]\n",
      "2546: [D loss: 0.696211, acc: 0.537109]  [A loss: 0.865412, acc: 0.199219]\n",
      "2547: [D loss: 0.697262, acc: 0.554688]  [A loss: 0.792534, acc: 0.257812]\n",
      "2548: [D loss: 0.702261, acc: 0.503906]  [A loss: 0.893024, acc: 0.144531]\n",
      "2549: [D loss: 0.697596, acc: 0.535156]  [A loss: 0.816536, acc: 0.214844]\n",
      "2550: [D loss: 0.696088, acc: 0.525391]  [A loss: 0.979356, acc: 0.070312]\n",
      "2551: [D loss: 0.697783, acc: 0.537109]  [A loss: 0.706337, acc: 0.507812]\n",
      "2552: [D loss: 0.716729, acc: 0.519531]  [A loss: 1.094073, acc: 0.027344]\n",
      "2553: [D loss: 0.709424, acc: 0.496094]  [A loss: 0.658063, acc: 0.656250]\n",
      "2554: [D loss: 0.722118, acc: 0.496094]  [A loss: 0.912705, acc: 0.164062]\n",
      "2555: [D loss: 0.695170, acc: 0.490234]  [A loss: 0.797505, acc: 0.300781]\n",
      "2556: [D loss: 0.705596, acc: 0.480469]  [A loss: 0.801535, acc: 0.253906]\n",
      "2557: [D loss: 0.706391, acc: 0.488281]  [A loss: 0.758281, acc: 0.351562]\n",
      "2558: [D loss: 0.705719, acc: 0.507812]  [A loss: 0.868493, acc: 0.164062]\n",
      "2559: [D loss: 0.707002, acc: 0.531250]  [A loss: 0.803990, acc: 0.261719]\n",
      "2560: [D loss: 0.713897, acc: 0.494141]  [A loss: 0.801336, acc: 0.265625]\n",
      "2561: [D loss: 0.699056, acc: 0.511719]  [A loss: 0.857253, acc: 0.187500]\n",
      "2562: [D loss: 0.687548, acc: 0.548828]  [A loss: 0.790591, acc: 0.296875]\n",
      "2563: [D loss: 0.706759, acc: 0.525391]  [A loss: 0.900823, acc: 0.144531]\n",
      "2564: [D loss: 0.688192, acc: 0.554688]  [A loss: 0.755222, acc: 0.371094]\n",
      "2565: [D loss: 0.697379, acc: 0.521484]  [A loss: 0.939810, acc: 0.093750]\n",
      "2566: [D loss: 0.690123, acc: 0.578125]  [A loss: 0.725267, acc: 0.421875]\n",
      "2567: [D loss: 0.723769, acc: 0.498047]  [A loss: 1.016556, acc: 0.050781]\n",
      "2568: [D loss: 0.708304, acc: 0.478516]  [A loss: 0.658783, acc: 0.617188]\n",
      "2569: [D loss: 0.743224, acc: 0.498047]  [A loss: 0.977509, acc: 0.089844]\n",
      "2570: [D loss: 0.704581, acc: 0.515625]  [A loss: 0.768158, acc: 0.363281]\n",
      "2571: [D loss: 0.732613, acc: 0.457031]  [A loss: 0.971913, acc: 0.054688]\n",
      "2572: [D loss: 0.689792, acc: 0.542969]  [A loss: 0.786608, acc: 0.312500]\n",
      "2573: [D loss: 0.709884, acc: 0.511719]  [A loss: 0.863860, acc: 0.132812]\n",
      "2574: [D loss: 0.697748, acc: 0.521484]  [A loss: 0.726283, acc: 0.414062]\n",
      "2575: [D loss: 0.713553, acc: 0.490234]  [A loss: 0.860840, acc: 0.125000]\n",
      "2576: [D loss: 0.691431, acc: 0.546875]  [A loss: 0.794326, acc: 0.257812]\n",
      "2577: [D loss: 0.712942, acc: 0.480469]  [A loss: 0.866237, acc: 0.148438]\n",
      "2578: [D loss: 0.714326, acc: 0.466797]  [A loss: 0.826085, acc: 0.203125]\n",
      "2579: [D loss: 0.694207, acc: 0.507812]  [A loss: 0.808439, acc: 0.242188]\n",
      "2580: [D loss: 0.696673, acc: 0.537109]  [A loss: 0.801637, acc: 0.289062]\n",
      "2581: [D loss: 0.690873, acc: 0.539062]  [A loss: 0.869094, acc: 0.183594]\n",
      "2582: [D loss: 0.695511, acc: 0.521484]  [A loss: 0.786669, acc: 0.277344]\n",
      "2583: [D loss: 0.701110, acc: 0.552734]  [A loss: 0.867138, acc: 0.148438]\n",
      "2584: [D loss: 0.695879, acc: 0.511719]  [A loss: 0.831021, acc: 0.203125]\n",
      "2585: [D loss: 0.699453, acc: 0.541016]  [A loss: 0.854061, acc: 0.144531]\n",
      "2586: [D loss: 0.696556, acc: 0.554688]  [A loss: 0.831034, acc: 0.234375]\n",
      "2587: [D loss: 0.700501, acc: 0.533203]  [A loss: 0.925884, acc: 0.132812]\n",
      "2588: [D loss: 0.693473, acc: 0.500000]  [A loss: 0.758005, acc: 0.406250]\n",
      "2589: [D loss: 0.720859, acc: 0.488281]  [A loss: 0.985948, acc: 0.050781]\n",
      "2590: [D loss: 0.694567, acc: 0.539062]  [A loss: 0.672233, acc: 0.570312]\n",
      "2591: [D loss: 0.719555, acc: 0.521484]  [A loss: 0.997690, acc: 0.074219]\n",
      "2592: [D loss: 0.681083, acc: 0.570312]  [A loss: 0.646057, acc: 0.597656]\n",
      "2593: [D loss: 0.757813, acc: 0.511719]  [A loss: 0.994563, acc: 0.078125]\n",
      "2594: [D loss: 0.684145, acc: 0.537109]  [A loss: 0.737574, acc: 0.441406]\n",
      "2595: [D loss: 0.717404, acc: 0.505859]  [A loss: 0.927595, acc: 0.101562]\n",
      "2596: [D loss: 0.694532, acc: 0.533203]  [A loss: 0.740632, acc: 0.382812]\n",
      "2597: [D loss: 0.707145, acc: 0.505859]  [A loss: 0.843805, acc: 0.195312]\n",
      "2598: [D loss: 0.695021, acc: 0.544922]  [A loss: 0.747027, acc: 0.382812]\n",
      "2599: [D loss: 0.702119, acc: 0.529297]  [A loss: 0.892240, acc: 0.148438]\n",
      "2600: [D loss: 0.694521, acc: 0.523438]  [A loss: 0.789157, acc: 0.265625]\n",
      "2601: [D loss: 0.704510, acc: 0.515625]  [A loss: 0.878729, acc: 0.132812]\n",
      "2602: [D loss: 0.691682, acc: 0.546875]  [A loss: 0.778697, acc: 0.289062]\n",
      "2603: [D loss: 0.704232, acc: 0.511719]  [A loss: 0.846483, acc: 0.191406]\n",
      "2604: [D loss: 0.702639, acc: 0.511719]  [A loss: 0.796319, acc: 0.320312]\n",
      "2605: [D loss: 0.714728, acc: 0.490234]  [A loss: 0.925935, acc: 0.101562]\n",
      "2606: [D loss: 0.701516, acc: 0.503906]  [A loss: 0.774012, acc: 0.351562]\n",
      "2607: [D loss: 0.705225, acc: 0.525391]  [A loss: 0.819768, acc: 0.230469]\n",
      "2608: [D loss: 0.697471, acc: 0.484375]  [A loss: 0.828997, acc: 0.191406]\n",
      "2609: [D loss: 0.699369, acc: 0.513672]  [A loss: 0.903946, acc: 0.101562]\n",
      "2610: [D loss: 0.702001, acc: 0.505859]  [A loss: 0.732977, acc: 0.390625]\n",
      "2611: [D loss: 0.703938, acc: 0.529297]  [A loss: 0.962651, acc: 0.074219]\n",
      "2612: [D loss: 0.691523, acc: 0.525391]  [A loss: 0.673141, acc: 0.593750]\n",
      "2613: [D loss: 0.727859, acc: 0.501953]  [A loss: 0.985155, acc: 0.070312]\n",
      "2614: [D loss: 0.701828, acc: 0.496094]  [A loss: 0.744127, acc: 0.386719]\n",
      "2615: [D loss: 0.727751, acc: 0.503906]  [A loss: 0.888099, acc: 0.144531]\n",
      "2616: [D loss: 0.695088, acc: 0.552734]  [A loss: 0.893380, acc: 0.140625]\n",
      "2617: [D loss: 0.704163, acc: 0.533203]  [A loss: 0.717761, acc: 0.460938]\n",
      "2618: [D loss: 0.729113, acc: 0.496094]  [A loss: 0.997136, acc: 0.085938]\n",
      "2619: [D loss: 0.692547, acc: 0.552734]  [A loss: 0.680712, acc: 0.582031]\n",
      "2620: [D loss: 0.722410, acc: 0.503906]  [A loss: 0.911091, acc: 0.113281]\n",
      "2621: [D loss: 0.693027, acc: 0.548828]  [A loss: 0.721767, acc: 0.433594]\n",
      "2622: [D loss: 0.716019, acc: 0.500000]  [A loss: 0.917343, acc: 0.121094]\n",
      "2623: [D loss: 0.699655, acc: 0.511719]  [A loss: 0.777400, acc: 0.265625]\n",
      "2624: [D loss: 0.716160, acc: 0.478516]  [A loss: 0.852060, acc: 0.156250]\n",
      "2625: [D loss: 0.702768, acc: 0.496094]  [A loss: 0.798256, acc: 0.253906]\n",
      "2626: [D loss: 0.694737, acc: 0.546875]  [A loss: 0.831219, acc: 0.222656]\n",
      "2627: [D loss: 0.707183, acc: 0.505859]  [A loss: 0.839323, acc: 0.203125]\n",
      "2628: [D loss: 0.681153, acc: 0.578125]  [A loss: 0.793475, acc: 0.277344]\n",
      "2629: [D loss: 0.707265, acc: 0.509766]  [A loss: 0.817239, acc: 0.210938]\n",
      "2630: [D loss: 0.700531, acc: 0.513672]  [A loss: 0.873523, acc: 0.140625]\n",
      "2631: [D loss: 0.696927, acc: 0.527344]  [A loss: 0.780463, acc: 0.328125]\n",
      "2632: [D loss: 0.698616, acc: 0.521484]  [A loss: 0.908914, acc: 0.101562]\n",
      "2633: [D loss: 0.689278, acc: 0.539062]  [A loss: 0.724451, acc: 0.476562]\n",
      "2634: [D loss: 0.709490, acc: 0.535156]  [A loss: 0.968883, acc: 0.062500]\n",
      "2635: [D loss: 0.688891, acc: 0.550781]  [A loss: 0.687476, acc: 0.539062]\n",
      "2636: [D loss: 0.716237, acc: 0.511719]  [A loss: 0.936004, acc: 0.105469]\n",
      "2637: [D loss: 0.691798, acc: 0.527344]  [A loss: 0.751099, acc: 0.355469]\n",
      "2638: [D loss: 0.696040, acc: 0.523438]  [A loss: 0.885703, acc: 0.152344]\n",
      "2639: [D loss: 0.696638, acc: 0.521484]  [A loss: 0.745912, acc: 0.378906]\n",
      "2640: [D loss: 0.721605, acc: 0.496094]  [A loss: 0.895277, acc: 0.160156]\n",
      "2641: [D loss: 0.700586, acc: 0.521484]  [A loss: 0.780819, acc: 0.320312]\n",
      "2642: [D loss: 0.694946, acc: 0.544922]  [A loss: 0.802804, acc: 0.269531]\n",
      "2643: [D loss: 0.694273, acc: 0.533203]  [A loss: 0.837305, acc: 0.199219]\n",
      "2644: [D loss: 0.706769, acc: 0.517578]  [A loss: 0.842329, acc: 0.167969]\n",
      "2645: [D loss: 0.689264, acc: 0.529297]  [A loss: 0.823207, acc: 0.234375]\n",
      "2646: [D loss: 0.688774, acc: 0.542969]  [A loss: 0.803751, acc: 0.253906]\n",
      "2647: [D loss: 0.699162, acc: 0.511719]  [A loss: 0.832854, acc: 0.218750]\n",
      "2648: [D loss: 0.679042, acc: 0.582031]  [A loss: 0.837257, acc: 0.269531]\n",
      "2649: [D loss: 0.688854, acc: 0.560547]  [A loss: 0.822885, acc: 0.218750]\n",
      "2650: [D loss: 0.699329, acc: 0.523438]  [A loss: 0.908779, acc: 0.117188]\n",
      "2651: [D loss: 0.692669, acc: 0.525391]  [A loss: 0.763884, acc: 0.355469]\n",
      "2652: [D loss: 0.708523, acc: 0.517578]  [A loss: 0.896627, acc: 0.152344]\n",
      "2653: [D loss: 0.692495, acc: 0.574219]  [A loss: 0.754308, acc: 0.359375]\n",
      "2654: [D loss: 0.724888, acc: 0.472656]  [A loss: 1.010673, acc: 0.066406]\n",
      "2655: [D loss: 0.709396, acc: 0.511719]  [A loss: 0.757057, acc: 0.398438]\n",
      "2656: [D loss: 0.719929, acc: 0.484375]  [A loss: 0.933158, acc: 0.113281]\n",
      "2657: [D loss: 0.698767, acc: 0.521484]  [A loss: 0.805701, acc: 0.281250]\n",
      "2658: [D loss: 0.707176, acc: 0.523438]  [A loss: 0.952039, acc: 0.066406]\n",
      "2659: [D loss: 0.692313, acc: 0.542969]  [A loss: 0.682095, acc: 0.574219]\n",
      "2660: [D loss: 0.722718, acc: 0.519531]  [A loss: 0.885475, acc: 0.109375]\n",
      "2661: [D loss: 0.689888, acc: 0.527344]  [A loss: 0.742811, acc: 0.378906]\n",
      "2662: [D loss: 0.694499, acc: 0.539062]  [A loss: 0.934417, acc: 0.093750]\n",
      "2663: [D loss: 0.695402, acc: 0.523438]  [A loss: 0.829104, acc: 0.222656]\n",
      "2664: [D loss: 0.688432, acc: 0.560547]  [A loss: 0.793050, acc: 0.273438]\n",
      "2665: [D loss: 0.710461, acc: 0.521484]  [A loss: 0.854526, acc: 0.226562]\n",
      "2666: [D loss: 0.685222, acc: 0.560547]  [A loss: 0.780781, acc: 0.335938]\n",
      "2667: [D loss: 0.710100, acc: 0.507812]  [A loss: 0.858575, acc: 0.152344]\n",
      "2668: [D loss: 0.696264, acc: 0.523438]  [A loss: 0.705557, acc: 0.480469]\n",
      "2669: [D loss: 0.718710, acc: 0.503906]  [A loss: 1.005304, acc: 0.070312]\n",
      "2670: [D loss: 0.713203, acc: 0.505859]  [A loss: 0.709047, acc: 0.503906]\n",
      "2671: [D loss: 0.720275, acc: 0.503906]  [A loss: 0.890317, acc: 0.097656]\n",
      "2672: [D loss: 0.683571, acc: 0.548828]  [A loss: 0.742803, acc: 0.390625]\n",
      "2673: [D loss: 0.736517, acc: 0.492188]  [A loss: 1.047549, acc: 0.019531]\n",
      "2674: [D loss: 0.689062, acc: 0.552734]  [A loss: 0.706896, acc: 0.480469]\n",
      "2675: [D loss: 0.705311, acc: 0.533203]  [A loss: 0.840886, acc: 0.218750]\n",
      "2676: [D loss: 0.701919, acc: 0.529297]  [A loss: 0.761447, acc: 0.359375]\n",
      "2677: [D loss: 0.707167, acc: 0.494141]  [A loss: 0.809792, acc: 0.210938]\n",
      "2678: [D loss: 0.695684, acc: 0.509766]  [A loss: 0.863071, acc: 0.203125]\n",
      "2679: [D loss: 0.695748, acc: 0.542969]  [A loss: 0.848099, acc: 0.179688]\n",
      "2680: [D loss: 0.701026, acc: 0.537109]  [A loss: 0.743750, acc: 0.398438]\n",
      "2681: [D loss: 0.697402, acc: 0.515625]  [A loss: 0.907670, acc: 0.093750]\n",
      "2682: [D loss: 0.690431, acc: 0.531250]  [A loss: 0.761263, acc: 0.343750]\n",
      "2683: [D loss: 0.706984, acc: 0.505859]  [A loss: 0.868363, acc: 0.164062]\n",
      "2684: [D loss: 0.705576, acc: 0.511719]  [A loss: 0.753445, acc: 0.371094]\n",
      "2685: [D loss: 0.717146, acc: 0.496094]  [A loss: 0.887871, acc: 0.148438]\n",
      "2686: [D loss: 0.697641, acc: 0.511719]  [A loss: 0.721274, acc: 0.472656]\n",
      "2687: [D loss: 0.711843, acc: 0.507812]  [A loss: 0.920542, acc: 0.109375]\n",
      "2688: [D loss: 0.703318, acc: 0.519531]  [A loss: 0.748387, acc: 0.367188]\n",
      "2689: [D loss: 0.695794, acc: 0.533203]  [A loss: 0.915862, acc: 0.093750]\n",
      "2690: [D loss: 0.697918, acc: 0.515625]  [A loss: 0.705280, acc: 0.492188]\n",
      "2691: [D loss: 0.706630, acc: 0.509766]  [A loss: 0.875823, acc: 0.167969]\n",
      "2692: [D loss: 0.699497, acc: 0.529297]  [A loss: 0.749535, acc: 0.406250]\n",
      "2693: [D loss: 0.711099, acc: 0.501953]  [A loss: 0.900913, acc: 0.121094]\n",
      "2694: [D loss: 0.695934, acc: 0.529297]  [A loss: 0.774909, acc: 0.367188]\n",
      "2695: [D loss: 0.707630, acc: 0.498047]  [A loss: 0.857754, acc: 0.148438]\n",
      "2696: [D loss: 0.696037, acc: 0.531250]  [A loss: 0.782132, acc: 0.285156]\n",
      "2697: [D loss: 0.694808, acc: 0.548828]  [A loss: 0.811583, acc: 0.250000]\n",
      "2698: [D loss: 0.701915, acc: 0.529297]  [A loss: 0.808065, acc: 0.195312]\n",
      "2699: [D loss: 0.705476, acc: 0.529297]  [A loss: 0.905031, acc: 0.113281]\n",
      "2700: [D loss: 0.683068, acc: 0.556641]  [A loss: 0.725075, acc: 0.449219]\n",
      "2701: [D loss: 0.708885, acc: 0.517578]  [A loss: 0.950163, acc: 0.085938]\n",
      "2702: [D loss: 0.690181, acc: 0.513672]  [A loss: 0.706097, acc: 0.488281]\n",
      "2703: [D loss: 0.707653, acc: 0.519531]  [A loss: 0.835136, acc: 0.179688]\n",
      "2704: [D loss: 0.689453, acc: 0.544922]  [A loss: 0.781740, acc: 0.308594]\n",
      "2705: [D loss: 0.703188, acc: 0.531250]  [A loss: 0.893408, acc: 0.113281]\n",
      "2706: [D loss: 0.703287, acc: 0.517578]  [A loss: 0.706527, acc: 0.460938]\n",
      "2707: [D loss: 0.732422, acc: 0.498047]  [A loss: 0.966947, acc: 0.042969]\n",
      "2708: [D loss: 0.688458, acc: 0.542969]  [A loss: 0.729836, acc: 0.437500]\n",
      "2709: [D loss: 0.716178, acc: 0.511719]  [A loss: 0.846857, acc: 0.164062]\n",
      "2710: [D loss: 0.694291, acc: 0.513672]  [A loss: 0.826540, acc: 0.214844]\n",
      "2711: [D loss: 0.690957, acc: 0.546875]  [A loss: 0.848806, acc: 0.207031]\n",
      "2712: [D loss: 0.690447, acc: 0.564453]  [A loss: 0.754636, acc: 0.367188]\n",
      "2713: [D loss: 0.696407, acc: 0.521484]  [A loss: 0.873601, acc: 0.183594]\n",
      "2714: [D loss: 0.685874, acc: 0.566406]  [A loss: 0.797810, acc: 0.265625]\n",
      "2715: [D loss: 0.698046, acc: 0.535156]  [A loss: 0.876522, acc: 0.171875]\n",
      "2716: [D loss: 0.695218, acc: 0.507812]  [A loss: 0.757834, acc: 0.343750]\n",
      "2717: [D loss: 0.689920, acc: 0.542969]  [A loss: 0.870302, acc: 0.152344]\n",
      "2718: [D loss: 0.697921, acc: 0.517578]  [A loss: 0.841073, acc: 0.207031]\n",
      "2719: [D loss: 0.694839, acc: 0.503906]  [A loss: 0.846691, acc: 0.207031]\n",
      "2720: [D loss: 0.690508, acc: 0.548828]  [A loss: 0.760132, acc: 0.332031]\n",
      "2721: [D loss: 0.701148, acc: 0.519531]  [A loss: 0.900905, acc: 0.113281]\n",
      "2722: [D loss: 0.697981, acc: 0.503906]  [A loss: 0.671945, acc: 0.601562]\n",
      "2723: [D loss: 0.705833, acc: 0.523438]  [A loss: 0.926041, acc: 0.089844]\n",
      "2724: [D loss: 0.703735, acc: 0.509766]  [A loss: 0.791995, acc: 0.308594]\n",
      "2725: [D loss: 0.705615, acc: 0.527344]  [A loss: 0.810861, acc: 0.250000]\n",
      "2726: [D loss: 0.700289, acc: 0.546875]  [A loss: 0.822773, acc: 0.257812]\n",
      "2727: [D loss: 0.708410, acc: 0.500000]  [A loss: 0.797638, acc: 0.265625]\n",
      "2728: [D loss: 0.711510, acc: 0.478516]  [A loss: 0.863122, acc: 0.203125]\n",
      "2729: [D loss: 0.699005, acc: 0.525391]  [A loss: 0.789119, acc: 0.285156]\n",
      "2730: [D loss: 0.693561, acc: 0.546875]  [A loss: 0.861311, acc: 0.160156]\n",
      "2731: [D loss: 0.687095, acc: 0.564453]  [A loss: 0.743968, acc: 0.386719]\n",
      "2732: [D loss: 0.706354, acc: 0.513672]  [A loss: 0.895355, acc: 0.136719]\n",
      "2733: [D loss: 0.694615, acc: 0.544922]  [A loss: 0.776839, acc: 0.292969]\n",
      "2734: [D loss: 0.688084, acc: 0.525391]  [A loss: 0.841003, acc: 0.179688]\n",
      "2735: [D loss: 0.701499, acc: 0.517578]  [A loss: 0.861394, acc: 0.210938]\n",
      "2736: [D loss: 0.703039, acc: 0.515625]  [A loss: 0.821352, acc: 0.234375]\n",
      "2737: [D loss: 0.691062, acc: 0.544922]  [A loss: 0.811902, acc: 0.250000]\n",
      "2738: [D loss: 0.701198, acc: 0.541016]  [A loss: 0.960310, acc: 0.074219]\n",
      "2739: [D loss: 0.697795, acc: 0.507812]  [A loss: 0.687230, acc: 0.531250]\n",
      "2740: [D loss: 0.729367, acc: 0.515625]  [A loss: 1.069176, acc: 0.085938]\n",
      "2741: [D loss: 0.699560, acc: 0.527344]  [A loss: 0.693975, acc: 0.496094]\n",
      "2742: [D loss: 0.747532, acc: 0.466797]  [A loss: 0.866787, acc: 0.171875]\n",
      "2743: [D loss: 0.694643, acc: 0.544922]  [A loss: 0.893158, acc: 0.132812]\n",
      "2744: [D loss: 0.700100, acc: 0.523438]  [A loss: 0.790877, acc: 0.269531]\n",
      "2745: [D loss: 0.696929, acc: 0.537109]  [A loss: 0.843033, acc: 0.179688]\n",
      "2746: [D loss: 0.708148, acc: 0.490234]  [A loss: 0.770870, acc: 0.335938]\n",
      "2747: [D loss: 0.696427, acc: 0.537109]  [A loss: 0.856310, acc: 0.183594]\n",
      "2748: [D loss: 0.696146, acc: 0.519531]  [A loss: 0.781347, acc: 0.324219]\n",
      "2749: [D loss: 0.691001, acc: 0.533203]  [A loss: 0.866293, acc: 0.195312]\n",
      "2750: [D loss: 0.690367, acc: 0.539062]  [A loss: 0.768169, acc: 0.351562]\n",
      "2751: [D loss: 0.713538, acc: 0.505859]  [A loss: 0.867501, acc: 0.167969]\n",
      "2752: [D loss: 0.698052, acc: 0.535156]  [A loss: 0.800877, acc: 0.265625]\n",
      "2753: [D loss: 0.701951, acc: 0.519531]  [A loss: 0.858643, acc: 0.156250]\n",
      "2754: [D loss: 0.700808, acc: 0.501953]  [A loss: 0.809580, acc: 0.218750]\n",
      "2755: [D loss: 0.699434, acc: 0.490234]  [A loss: 0.821941, acc: 0.234375]\n",
      "2756: [D loss: 0.709514, acc: 0.484375]  [A loss: 0.898992, acc: 0.148438]\n",
      "2757: [D loss: 0.687640, acc: 0.535156]  [A loss: 0.764776, acc: 0.347656]\n",
      "2758: [D loss: 0.690893, acc: 0.554688]  [A loss: 0.870278, acc: 0.160156]\n",
      "2759: [D loss: 0.691764, acc: 0.531250]  [A loss: 0.815416, acc: 0.253906]\n",
      "2760: [D loss: 0.706951, acc: 0.488281]  [A loss: 0.822338, acc: 0.261719]\n",
      "2761: [D loss: 0.700198, acc: 0.503906]  [A loss: 0.788760, acc: 0.281250]\n",
      "2762: [D loss: 0.700028, acc: 0.531250]  [A loss: 0.916024, acc: 0.101562]\n",
      "2763: [D loss: 0.709508, acc: 0.498047]  [A loss: 0.726950, acc: 0.433594]\n",
      "2764: [D loss: 0.718257, acc: 0.503906]  [A loss: 0.966168, acc: 0.066406]\n",
      "2765: [D loss: 0.708880, acc: 0.496094]  [A loss: 0.714776, acc: 0.468750]\n",
      "2766: [D loss: 0.711117, acc: 0.525391]  [A loss: 0.917573, acc: 0.121094]\n",
      "2767: [D loss: 0.702972, acc: 0.509766]  [A loss: 0.699799, acc: 0.531250]\n",
      "2768: [D loss: 0.713825, acc: 0.548828]  [A loss: 0.883050, acc: 0.218750]\n",
      "2769: [D loss: 0.691640, acc: 0.529297]  [A loss: 0.805842, acc: 0.289062]\n",
      "2770: [D loss: 0.707298, acc: 0.509766]  [A loss: 0.921202, acc: 0.089844]\n",
      "2771: [D loss: 0.712095, acc: 0.474609]  [A loss: 0.755234, acc: 0.335938]\n",
      "2772: [D loss: 0.712879, acc: 0.498047]  [A loss: 0.898509, acc: 0.101562]\n",
      "2773: [D loss: 0.696608, acc: 0.507812]  [A loss: 0.675529, acc: 0.550781]\n",
      "2774: [D loss: 0.710865, acc: 0.517578]  [A loss: 0.885042, acc: 0.152344]\n",
      "2775: [D loss: 0.701229, acc: 0.513672]  [A loss: 0.799411, acc: 0.277344]\n",
      "2776: [D loss: 0.697212, acc: 0.531250]  [A loss: 0.817122, acc: 0.250000]\n",
      "2777: [D loss: 0.702666, acc: 0.515625]  [A loss: 0.885801, acc: 0.187500]\n",
      "2778: [D loss: 0.700343, acc: 0.525391]  [A loss: 0.716808, acc: 0.480469]\n",
      "2779: [D loss: 0.705811, acc: 0.533203]  [A loss: 0.934319, acc: 0.101562]\n",
      "2780: [D loss: 0.693218, acc: 0.539062]  [A loss: 0.669454, acc: 0.531250]\n",
      "2781: [D loss: 0.717945, acc: 0.539062]  [A loss: 0.981253, acc: 0.070312]\n",
      "2782: [D loss: 0.697211, acc: 0.525391]  [A loss: 0.697537, acc: 0.496094]\n",
      "2783: [D loss: 0.706944, acc: 0.531250]  [A loss: 0.874105, acc: 0.160156]\n",
      "2784: [D loss: 0.693008, acc: 0.519531]  [A loss: 0.772386, acc: 0.328125]\n",
      "2785: [D loss: 0.711879, acc: 0.517578]  [A loss: 0.864202, acc: 0.195312]\n",
      "2786: [D loss: 0.688937, acc: 0.533203]  [A loss: 0.754288, acc: 0.363281]\n",
      "2787: [D loss: 0.700198, acc: 0.505859]  [A loss: 0.884781, acc: 0.195312]\n",
      "2788: [D loss: 0.696098, acc: 0.527344]  [A loss: 0.762021, acc: 0.355469]\n",
      "2789: [D loss: 0.697223, acc: 0.546875]  [A loss: 0.830369, acc: 0.218750]\n",
      "2790: [D loss: 0.689544, acc: 0.542969]  [A loss: 0.810330, acc: 0.246094]\n",
      "2791: [D loss: 0.694024, acc: 0.539062]  [A loss: 0.855561, acc: 0.152344]\n",
      "2792: [D loss: 0.693182, acc: 0.523438]  [A loss: 0.809477, acc: 0.296875]\n",
      "2793: [D loss: 0.705500, acc: 0.503906]  [A loss: 0.818789, acc: 0.214844]\n",
      "2794: [D loss: 0.696149, acc: 0.523438]  [A loss: 0.843566, acc: 0.222656]\n",
      "2795: [D loss: 0.690018, acc: 0.548828]  [A loss: 0.798510, acc: 0.222656]\n",
      "2796: [D loss: 0.689854, acc: 0.546875]  [A loss: 0.832526, acc: 0.199219]\n",
      "2797: [D loss: 0.688557, acc: 0.546875]  [A loss: 0.767184, acc: 0.312500]\n",
      "2798: [D loss: 0.698857, acc: 0.527344]  [A loss: 0.845116, acc: 0.207031]\n",
      "2799: [D loss: 0.696929, acc: 0.521484]  [A loss: 0.793693, acc: 0.242188]\n",
      "2800: [D loss: 0.704509, acc: 0.523438]  [A loss: 0.837741, acc: 0.203125]\n",
      "2801: [D loss: 0.694531, acc: 0.542969]  [A loss: 0.897308, acc: 0.128906]\n",
      "2802: [D loss: 0.709170, acc: 0.505859]  [A loss: 0.729401, acc: 0.417969]\n",
      "2803: [D loss: 0.705589, acc: 0.539062]  [A loss: 0.935026, acc: 0.109375]\n",
      "2804: [D loss: 0.697695, acc: 0.492188]  [A loss: 0.714499, acc: 0.480469]\n",
      "2805: [D loss: 0.741191, acc: 0.486328]  [A loss: 0.995618, acc: 0.074219]\n",
      "2806: [D loss: 0.711884, acc: 0.484375]  [A loss: 0.778040, acc: 0.332031]\n",
      "2807: [D loss: 0.742026, acc: 0.494141]  [A loss: 0.984825, acc: 0.050781]\n",
      "2808: [D loss: 0.688752, acc: 0.525391]  [A loss: 0.732307, acc: 0.449219]\n",
      "2809: [D loss: 0.711071, acc: 0.500000]  [A loss: 0.933360, acc: 0.078125]\n",
      "2810: [D loss: 0.679793, acc: 0.583984]  [A loss: 0.702904, acc: 0.515625]\n",
      "2811: [D loss: 0.709321, acc: 0.507812]  [A loss: 0.872782, acc: 0.160156]\n",
      "2812: [D loss: 0.702354, acc: 0.498047]  [A loss: 0.745395, acc: 0.378906]\n",
      "2813: [D loss: 0.712406, acc: 0.505859]  [A loss: 0.855295, acc: 0.160156]\n",
      "2814: [D loss: 0.696913, acc: 0.539062]  [A loss: 0.778550, acc: 0.328125]\n",
      "2815: [D loss: 0.720070, acc: 0.482422]  [A loss: 0.850609, acc: 0.183594]\n",
      "2816: [D loss: 0.697886, acc: 0.529297]  [A loss: 0.792796, acc: 0.273438]\n",
      "2817: [D loss: 0.701410, acc: 0.498047]  [A loss: 0.819957, acc: 0.226562]\n",
      "2818: [D loss: 0.683079, acc: 0.562500]  [A loss: 0.745801, acc: 0.359375]\n",
      "2819: [D loss: 0.702865, acc: 0.507812]  [A loss: 0.858668, acc: 0.191406]\n",
      "2820: [D loss: 0.687697, acc: 0.529297]  [A loss: 0.761878, acc: 0.371094]\n",
      "2821: [D loss: 0.697343, acc: 0.513672]  [A loss: 0.914567, acc: 0.148438]\n",
      "2822: [D loss: 0.699775, acc: 0.519531]  [A loss: 0.761213, acc: 0.355469]\n",
      "2823: [D loss: 0.699887, acc: 0.531250]  [A loss: 0.831238, acc: 0.234375]\n",
      "2824: [D loss: 0.696539, acc: 0.539062]  [A loss: 0.760837, acc: 0.347656]\n",
      "2825: [D loss: 0.705485, acc: 0.507812]  [A loss: 0.828326, acc: 0.230469]\n",
      "2826: [D loss: 0.697772, acc: 0.513672]  [A loss: 0.917545, acc: 0.101562]\n",
      "2827: [D loss: 0.698422, acc: 0.505859]  [A loss: 0.821165, acc: 0.257812]\n",
      "2828: [D loss: 0.694861, acc: 0.554688]  [A loss: 0.819731, acc: 0.234375]\n",
      "2829: [D loss: 0.686544, acc: 0.556641]  [A loss: 0.828151, acc: 0.226562]\n",
      "2830: [D loss: 0.695778, acc: 0.529297]  [A loss: 0.841149, acc: 0.191406]\n",
      "2831: [D loss: 0.692893, acc: 0.519531]  [A loss: 0.849190, acc: 0.179688]\n",
      "2832: [D loss: 0.709088, acc: 0.542969]  [A loss: 0.979312, acc: 0.054688]\n",
      "2833: [D loss: 0.710402, acc: 0.474609]  [A loss: 0.683745, acc: 0.527344]\n",
      "2834: [D loss: 0.714054, acc: 0.496094]  [A loss: 0.963396, acc: 0.050781]\n",
      "2835: [D loss: 0.690017, acc: 0.533203]  [A loss: 0.694421, acc: 0.511719]\n",
      "2836: [D loss: 0.717098, acc: 0.501953]  [A loss: 0.841356, acc: 0.195312]\n",
      "2837: [D loss: 0.702071, acc: 0.507812]  [A loss: 0.899297, acc: 0.082031]\n",
      "2838: [D loss: 0.701135, acc: 0.490234]  [A loss: 0.770119, acc: 0.335938]\n",
      "2839: [D loss: 0.711845, acc: 0.513672]  [A loss: 0.887194, acc: 0.164062]\n",
      "2840: [D loss: 0.712057, acc: 0.486328]  [A loss: 0.770122, acc: 0.328125]\n",
      "2841: [D loss: 0.710921, acc: 0.494141]  [A loss: 0.839409, acc: 0.203125]\n",
      "2842: [D loss: 0.711375, acc: 0.517578]  [A loss: 0.820619, acc: 0.207031]\n",
      "2843: [D loss: 0.702393, acc: 0.525391]  [A loss: 0.881712, acc: 0.140625]\n",
      "2844: [D loss: 0.702550, acc: 0.494141]  [A loss: 0.779508, acc: 0.308594]\n",
      "2845: [D loss: 0.697318, acc: 0.517578]  [A loss: 0.867488, acc: 0.136719]\n",
      "2846: [D loss: 0.688527, acc: 0.542969]  [A loss: 0.785363, acc: 0.316406]\n",
      "2847: [D loss: 0.715482, acc: 0.501953]  [A loss: 0.860755, acc: 0.191406]\n",
      "2848: [D loss: 0.713628, acc: 0.466797]  [A loss: 0.828810, acc: 0.222656]\n",
      "2849: [D loss: 0.687403, acc: 0.558594]  [A loss: 0.807912, acc: 0.273438]\n",
      "2850: [D loss: 0.696038, acc: 0.507812]  [A loss: 0.841148, acc: 0.226562]\n",
      "2851: [D loss: 0.693807, acc: 0.525391]  [A loss: 0.856088, acc: 0.234375]\n",
      "2852: [D loss: 0.703974, acc: 0.505859]  [A loss: 0.785175, acc: 0.304688]\n",
      "2853: [D loss: 0.697112, acc: 0.527344]  [A loss: 0.865489, acc: 0.144531]\n",
      "2854: [D loss: 0.701217, acc: 0.507812]  [A loss: 0.773911, acc: 0.355469]\n",
      "2855: [D loss: 0.710170, acc: 0.492188]  [A loss: 0.900233, acc: 0.148438]\n",
      "2856: [D loss: 0.698118, acc: 0.525391]  [A loss: 0.745000, acc: 0.421875]\n",
      "2857: [D loss: 0.716749, acc: 0.488281]  [A loss: 0.926029, acc: 0.113281]\n",
      "2858: [D loss: 0.693345, acc: 0.535156]  [A loss: 0.750316, acc: 0.414062]\n",
      "2859: [D loss: 0.722222, acc: 0.498047]  [A loss: 1.001308, acc: 0.097656]\n",
      "2860: [D loss: 0.680021, acc: 0.556641]  [A loss: 0.674216, acc: 0.601562]\n",
      "2861: [D loss: 0.710623, acc: 0.525391]  [A loss: 0.930895, acc: 0.136719]\n",
      "2862: [D loss: 0.709571, acc: 0.501953]  [A loss: 0.780508, acc: 0.308594]\n",
      "2863: [D loss: 0.710558, acc: 0.509766]  [A loss: 0.825087, acc: 0.222656]\n",
      "2864: [D loss: 0.696926, acc: 0.537109]  [A loss: 0.853348, acc: 0.195312]\n",
      "2865: [D loss: 0.697879, acc: 0.517578]  [A loss: 0.747595, acc: 0.378906]\n",
      "2866: [D loss: 0.703496, acc: 0.556641]  [A loss: 0.910454, acc: 0.097656]\n",
      "2867: [D loss: 0.695623, acc: 0.527344]  [A loss: 0.786859, acc: 0.273438]\n",
      "2868: [D loss: 0.714535, acc: 0.492188]  [A loss: 0.901045, acc: 0.140625]\n",
      "2869: [D loss: 0.685264, acc: 0.552734]  [A loss: 0.898116, acc: 0.105469]\n",
      "2870: [D loss: 0.690730, acc: 0.548828]  [A loss: 0.745651, acc: 0.375000]\n",
      "2871: [D loss: 0.714380, acc: 0.501953]  [A loss: 0.942934, acc: 0.113281]\n",
      "2872: [D loss: 0.670908, acc: 0.583984]  [A loss: 0.769657, acc: 0.386719]\n",
      "2873: [D loss: 0.720273, acc: 0.484375]  [A loss: 0.927477, acc: 0.101562]\n",
      "2874: [D loss: 0.689539, acc: 0.546875]  [A loss: 0.765449, acc: 0.386719]\n",
      "2875: [D loss: 0.697328, acc: 0.529297]  [A loss: 0.907845, acc: 0.148438]\n",
      "2876: [D loss: 0.703862, acc: 0.503906]  [A loss: 0.730693, acc: 0.437500]\n",
      "2877: [D loss: 0.729616, acc: 0.490234]  [A loss: 0.980805, acc: 0.027344]\n",
      "2878: [D loss: 0.713178, acc: 0.488281]  [A loss: 0.684762, acc: 0.515625]\n",
      "2879: [D loss: 0.724019, acc: 0.507812]  [A loss: 0.959112, acc: 0.042969]\n",
      "2880: [D loss: 0.692566, acc: 0.511719]  [A loss: 0.717023, acc: 0.460938]\n",
      "2881: [D loss: 0.710823, acc: 0.515625]  [A loss: 0.944534, acc: 0.093750]\n",
      "2882: [D loss: 0.690248, acc: 0.525391]  [A loss: 0.717969, acc: 0.453125]\n",
      "2883: [D loss: 0.713385, acc: 0.515625]  [A loss: 0.859369, acc: 0.156250]\n",
      "2884: [D loss: 0.693800, acc: 0.533203]  [A loss: 0.822994, acc: 0.226562]\n",
      "2885: [D loss: 0.690467, acc: 0.533203]  [A loss: 0.837223, acc: 0.222656]\n",
      "2886: [D loss: 0.700214, acc: 0.521484]  [A loss: 0.852422, acc: 0.203125]\n",
      "2887: [D loss: 0.696890, acc: 0.521484]  [A loss: 0.851674, acc: 0.175781]\n",
      "2888: [D loss: 0.699086, acc: 0.515625]  [A loss: 0.754014, acc: 0.343750]\n",
      "2889: [D loss: 0.696541, acc: 0.546875]  [A loss: 0.862125, acc: 0.218750]\n",
      "2890: [D loss: 0.692521, acc: 0.542969]  [A loss: 0.780727, acc: 0.296875]\n",
      "2891: [D loss: 0.712828, acc: 0.496094]  [A loss: 0.932539, acc: 0.113281]\n",
      "2892: [D loss: 0.701018, acc: 0.515625]  [A loss: 0.789489, acc: 0.308594]\n",
      "2893: [D loss: 0.702967, acc: 0.539062]  [A loss: 0.856838, acc: 0.152344]\n",
      "2894: [D loss: 0.695681, acc: 0.531250]  [A loss: 0.823066, acc: 0.234375]\n",
      "2895: [D loss: 0.693828, acc: 0.554688]  [A loss: 0.783849, acc: 0.324219]\n",
      "2896: [D loss: 0.712908, acc: 0.498047]  [A loss: 0.831843, acc: 0.218750]\n",
      "2897: [D loss: 0.702469, acc: 0.533203]  [A loss: 0.905845, acc: 0.140625]\n",
      "2898: [D loss: 0.692148, acc: 0.533203]  [A loss: 0.704057, acc: 0.472656]\n",
      "2899: [D loss: 0.706219, acc: 0.513672]  [A loss: 0.956306, acc: 0.074219]\n",
      "2900: [D loss: 0.692215, acc: 0.527344]  [A loss: 0.689325, acc: 0.500000]\n",
      "2901: [D loss: 0.720819, acc: 0.523438]  [A loss: 0.903026, acc: 0.144531]\n",
      "2902: [D loss: 0.718325, acc: 0.480469]  [A loss: 0.802995, acc: 0.253906]\n",
      "2903: [D loss: 0.692369, acc: 0.552734]  [A loss: 0.886660, acc: 0.144531]\n",
      "2904: [D loss: 0.704190, acc: 0.486328]  [A loss: 0.676163, acc: 0.578125]\n",
      "2905: [D loss: 0.734117, acc: 0.507812]  [A loss: 1.084804, acc: 0.023438]\n",
      "2906: [D loss: 0.713076, acc: 0.505859]  [A loss: 0.713930, acc: 0.476562]\n",
      "2907: [D loss: 0.710420, acc: 0.523438]  [A loss: 0.835655, acc: 0.199219]\n",
      "2908: [D loss: 0.697285, acc: 0.515625]  [A loss: 0.786551, acc: 0.277344]\n",
      "2909: [D loss: 0.695276, acc: 0.517578]  [A loss: 0.853926, acc: 0.179688]\n",
      "2910: [D loss: 0.690482, acc: 0.535156]  [A loss: 0.784566, acc: 0.269531]\n",
      "2911: [D loss: 0.707006, acc: 0.478516]  [A loss: 0.848189, acc: 0.175781]\n",
      "2912: [D loss: 0.696387, acc: 0.533203]  [A loss: 0.820565, acc: 0.218750]\n",
      "2913: [D loss: 0.695456, acc: 0.544922]  [A loss: 0.847395, acc: 0.156250]\n",
      "2914: [D loss: 0.689175, acc: 0.568359]  [A loss: 0.785008, acc: 0.332031]\n",
      "2915: [D loss: 0.703655, acc: 0.513672]  [A loss: 0.865929, acc: 0.148438]\n",
      "2916: [D loss: 0.700949, acc: 0.500000]  [A loss: 0.875658, acc: 0.171875]\n",
      "2917: [D loss: 0.683799, acc: 0.558594]  [A loss: 0.751819, acc: 0.378906]\n",
      "2918: [D loss: 0.720824, acc: 0.494141]  [A loss: 1.002793, acc: 0.027344]\n",
      "2919: [D loss: 0.688540, acc: 0.546875]  [A loss: 0.671866, acc: 0.589844]\n",
      "2920: [D loss: 0.741156, acc: 0.500000]  [A loss: 0.907597, acc: 0.085938]\n",
      "2921: [D loss: 0.687868, acc: 0.556641]  [A loss: 0.823988, acc: 0.199219]\n",
      "2922: [D loss: 0.697288, acc: 0.544922]  [A loss: 0.843187, acc: 0.199219]\n",
      "2923: [D loss: 0.705425, acc: 0.484375]  [A loss: 0.848835, acc: 0.218750]\n",
      "2924: [D loss: 0.709133, acc: 0.503906]  [A loss: 0.877523, acc: 0.167969]\n",
      "2925: [D loss: 0.697455, acc: 0.527344]  [A loss: 0.811899, acc: 0.242188]\n",
      "2926: [D loss: 0.702773, acc: 0.503906]  [A loss: 0.939611, acc: 0.082031]\n",
      "2927: [D loss: 0.695989, acc: 0.507812]  [A loss: 0.739772, acc: 0.406250]\n",
      "2928: [D loss: 0.725839, acc: 0.472656]  [A loss: 0.914584, acc: 0.132812]\n",
      "2929: [D loss: 0.689350, acc: 0.552734]  [A loss: 0.733528, acc: 0.406250]\n",
      "2930: [D loss: 0.718942, acc: 0.505859]  [A loss: 1.038027, acc: 0.031250]\n",
      "2931: [D loss: 0.704737, acc: 0.509766]  [A loss: 0.704162, acc: 0.472656]\n",
      "2932: [D loss: 0.709014, acc: 0.535156]  [A loss: 0.905324, acc: 0.093750]\n",
      "2933: [D loss: 0.691251, acc: 0.552734]  [A loss: 0.763493, acc: 0.320312]\n",
      "2934: [D loss: 0.708478, acc: 0.498047]  [A loss: 0.868053, acc: 0.160156]\n",
      "2935: [D loss: 0.698152, acc: 0.513672]  [A loss: 0.768719, acc: 0.335938]\n",
      "2936: [D loss: 0.703792, acc: 0.513672]  [A loss: 0.826612, acc: 0.230469]\n",
      "2937: [D loss: 0.695284, acc: 0.541016]  [A loss: 0.804106, acc: 0.265625]\n",
      "2938: [D loss: 0.701728, acc: 0.519531]  [A loss: 0.813541, acc: 0.265625]\n",
      "2939: [D loss: 0.715556, acc: 0.501953]  [A loss: 0.949029, acc: 0.058594]\n",
      "2940: [D loss: 0.693553, acc: 0.537109]  [A loss: 0.730156, acc: 0.402344]\n",
      "2941: [D loss: 0.713898, acc: 0.505859]  [A loss: 1.001456, acc: 0.070312]\n",
      "2942: [D loss: 0.702462, acc: 0.521484]  [A loss: 0.742187, acc: 0.394531]\n",
      "2943: [D loss: 0.719315, acc: 0.533203]  [A loss: 0.846545, acc: 0.164062]\n",
      "2944: [D loss: 0.693820, acc: 0.537109]  [A loss: 0.782364, acc: 0.281250]\n",
      "2945: [D loss: 0.727591, acc: 0.462891]  [A loss: 0.909300, acc: 0.113281]\n",
      "2946: [D loss: 0.707406, acc: 0.480469]  [A loss: 0.752919, acc: 0.382812]\n",
      "2947: [D loss: 0.723878, acc: 0.492188]  [A loss: 0.939656, acc: 0.046875]\n",
      "2948: [D loss: 0.703380, acc: 0.460938]  [A loss: 0.742623, acc: 0.378906]\n",
      "2949: [D loss: 0.724087, acc: 0.476562]  [A loss: 0.861885, acc: 0.183594]\n",
      "2950: [D loss: 0.686956, acc: 0.548828]  [A loss: 0.852622, acc: 0.199219]\n",
      "2951: [D loss: 0.696500, acc: 0.525391]  [A loss: 0.822246, acc: 0.195312]\n",
      "2952: [D loss: 0.694786, acc: 0.533203]  [A loss: 0.857782, acc: 0.152344]\n",
      "2953: [D loss: 0.700597, acc: 0.498047]  [A loss: 0.764758, acc: 0.324219]\n",
      "2954: [D loss: 0.702929, acc: 0.507812]  [A loss: 0.901762, acc: 0.128906]\n",
      "2955: [D loss: 0.691449, acc: 0.492188]  [A loss: 0.690030, acc: 0.500000]\n",
      "2956: [D loss: 0.719961, acc: 0.498047]  [A loss: 0.927741, acc: 0.113281]\n",
      "2957: [D loss: 0.678155, acc: 0.560547]  [A loss: 0.714479, acc: 0.453125]\n",
      "2958: [D loss: 0.715495, acc: 0.498047]  [A loss: 0.873318, acc: 0.183594]\n",
      "2959: [D loss: 0.691672, acc: 0.529297]  [A loss: 0.773967, acc: 0.335938]\n",
      "2960: [D loss: 0.708726, acc: 0.513672]  [A loss: 0.895749, acc: 0.144531]\n",
      "2961: [D loss: 0.688542, acc: 0.578125]  [A loss: 0.734864, acc: 0.402344]\n",
      "2962: [D loss: 0.687974, acc: 0.552734]  [A loss: 0.899460, acc: 0.152344]\n",
      "2963: [D loss: 0.699899, acc: 0.525391]  [A loss: 0.801673, acc: 0.250000]\n",
      "2964: [D loss: 0.704062, acc: 0.513672]  [A loss: 0.781969, acc: 0.257812]\n",
      "2965: [D loss: 0.701475, acc: 0.523438]  [A loss: 0.948359, acc: 0.093750]\n",
      "2966: [D loss: 0.701352, acc: 0.507812]  [A loss: 0.722360, acc: 0.449219]\n",
      "2967: [D loss: 0.721154, acc: 0.513672]  [A loss: 0.906173, acc: 0.121094]\n",
      "2968: [D loss: 0.703694, acc: 0.505859]  [A loss: 0.759646, acc: 0.363281]\n",
      "2969: [D loss: 0.719154, acc: 0.494141]  [A loss: 0.913108, acc: 0.128906]\n",
      "2970: [D loss: 0.699058, acc: 0.509766]  [A loss: 0.744628, acc: 0.417969]\n",
      "2971: [D loss: 0.712607, acc: 0.517578]  [A loss: 0.900435, acc: 0.128906]\n",
      "2972: [D loss: 0.680464, acc: 0.572266]  [A loss: 0.739918, acc: 0.429688]\n",
      "2973: [D loss: 0.728127, acc: 0.476562]  [A loss: 0.975047, acc: 0.070312]\n",
      "2974: [D loss: 0.708748, acc: 0.484375]  [A loss: 0.700909, acc: 0.480469]\n",
      "2975: [D loss: 0.706936, acc: 0.517578]  [A loss: 0.831630, acc: 0.199219]\n",
      "2976: [D loss: 0.697963, acc: 0.521484]  [A loss: 0.771905, acc: 0.289062]\n",
      "2977: [D loss: 0.696130, acc: 0.517578]  [A loss: 0.804510, acc: 0.277344]\n",
      "2978: [D loss: 0.703713, acc: 0.527344]  [A loss: 0.961065, acc: 0.042969]\n",
      "2979: [D loss: 0.696762, acc: 0.509766]  [A loss: 0.730045, acc: 0.449219]\n",
      "2980: [D loss: 0.719211, acc: 0.490234]  [A loss: 0.860227, acc: 0.148438]\n",
      "2981: [D loss: 0.695512, acc: 0.519531]  [A loss: 0.792304, acc: 0.300781]\n",
      "2982: [D loss: 0.703246, acc: 0.552734]  [A loss: 0.862772, acc: 0.175781]\n",
      "2983: [D loss: 0.697372, acc: 0.521484]  [A loss: 0.780371, acc: 0.312500]\n",
      "2984: [D loss: 0.692309, acc: 0.546875]  [A loss: 0.828091, acc: 0.191406]\n",
      "2985: [D loss: 0.680660, acc: 0.574219]  [A loss: 0.782221, acc: 0.335938]\n",
      "2986: [D loss: 0.695509, acc: 0.533203]  [A loss: 0.877732, acc: 0.183594]\n",
      "2987: [D loss: 0.691102, acc: 0.533203]  [A loss: 0.793913, acc: 0.234375]\n",
      "2988: [D loss: 0.701316, acc: 0.529297]  [A loss: 0.811674, acc: 0.261719]\n",
      "2989: [D loss: 0.683787, acc: 0.580078]  [A loss: 0.805650, acc: 0.257812]\n",
      "2990: [D loss: 0.703506, acc: 0.511719]  [A loss: 0.804195, acc: 0.289062]\n",
      "2991: [D loss: 0.695891, acc: 0.525391]  [A loss: 0.853813, acc: 0.175781]\n",
      "2992: [D loss: 0.705211, acc: 0.507812]  [A loss: 0.860390, acc: 0.125000]\n",
      "2993: [D loss: 0.703886, acc: 0.505859]  [A loss: 0.826758, acc: 0.253906]\n",
      "2994: [D loss: 0.706088, acc: 0.501953]  [A loss: 0.779278, acc: 0.300781]\n",
      "2995: [D loss: 0.712245, acc: 0.498047]  [A loss: 0.939466, acc: 0.078125]\n",
      "2996: [D loss: 0.694077, acc: 0.531250]  [A loss: 0.722883, acc: 0.425781]\n",
      "2997: [D loss: 0.719392, acc: 0.525391]  [A loss: 1.013867, acc: 0.042969]\n",
      "2998: [D loss: 0.698099, acc: 0.525391]  [A loss: 0.680892, acc: 0.558594]\n",
      "2999: [D loss: 0.728926, acc: 0.496094]  [A loss: 0.864884, acc: 0.183594]\n",
      "3000: [D loss: 0.715273, acc: 0.496094]  [A loss: 0.907316, acc: 0.093750]\n",
      "3001: [D loss: 0.696894, acc: 0.531250]  [A loss: 0.717089, acc: 0.468750]\n",
      "3002: [D loss: 0.702948, acc: 0.515625]  [A loss: 0.949053, acc: 0.089844]\n",
      "3003: [D loss: 0.706767, acc: 0.501953]  [A loss: 0.740961, acc: 0.394531]\n",
      "3004: [D loss: 0.694672, acc: 0.529297]  [A loss: 0.816331, acc: 0.226562]\n",
      "3005: [D loss: 0.683611, acc: 0.564453]  [A loss: 0.821914, acc: 0.285156]\n",
      "3006: [D loss: 0.700085, acc: 0.527344]  [A loss: 0.816293, acc: 0.242188]\n",
      "3007: [D loss: 0.702154, acc: 0.521484]  [A loss: 0.782139, acc: 0.316406]\n",
      "3008: [D loss: 0.692824, acc: 0.511719]  [A loss: 0.844802, acc: 0.179688]\n",
      "3009: [D loss: 0.696337, acc: 0.527344]  [A loss: 0.770108, acc: 0.386719]\n",
      "3010: [D loss: 0.697465, acc: 0.531250]  [A loss: 0.860109, acc: 0.179688]\n",
      "3011: [D loss: 0.690942, acc: 0.537109]  [A loss: 0.792957, acc: 0.242188]\n",
      "3012: [D loss: 0.701710, acc: 0.523438]  [A loss: 0.868643, acc: 0.148438]\n",
      "3013: [D loss: 0.699543, acc: 0.527344]  [A loss: 0.767112, acc: 0.320312]\n",
      "3014: [D loss: 0.713075, acc: 0.521484]  [A loss: 0.935638, acc: 0.109375]\n",
      "3015: [D loss: 0.687174, acc: 0.562500]  [A loss: 0.756288, acc: 0.378906]\n",
      "3016: [D loss: 0.721475, acc: 0.509766]  [A loss: 0.849369, acc: 0.214844]\n",
      "3017: [D loss: 0.708742, acc: 0.521484]  [A loss: 0.888642, acc: 0.144531]\n",
      "3018: [D loss: 0.681785, acc: 0.544922]  [A loss: 0.756118, acc: 0.371094]\n",
      "3019: [D loss: 0.703817, acc: 0.533203]  [A loss: 0.978332, acc: 0.046875]\n",
      "3020: [D loss: 0.693824, acc: 0.519531]  [A loss: 0.762860, acc: 0.328125]\n",
      "3021: [D loss: 0.701672, acc: 0.550781]  [A loss: 0.866801, acc: 0.179688]\n",
      "3022: [D loss: 0.705358, acc: 0.515625]  [A loss: 0.749661, acc: 0.351562]\n",
      "3023: [D loss: 0.736943, acc: 0.460938]  [A loss: 0.889378, acc: 0.171875]\n",
      "3024: [D loss: 0.704727, acc: 0.529297]  [A loss: 0.830256, acc: 0.257812]\n",
      "3025: [D loss: 0.707515, acc: 0.509766]  [A loss: 0.835110, acc: 0.187500]\n",
      "3026: [D loss: 0.695706, acc: 0.541016]  [A loss: 0.832771, acc: 0.210938]\n",
      "3027: [D loss: 0.699144, acc: 0.521484]  [A loss: 0.820035, acc: 0.187500]\n",
      "3028: [D loss: 0.700349, acc: 0.507812]  [A loss: 0.817264, acc: 0.250000]\n",
      "3029: [D loss: 0.700130, acc: 0.523438]  [A loss: 0.843325, acc: 0.207031]\n",
      "3030: [D loss: 0.706995, acc: 0.513672]  [A loss: 0.796380, acc: 0.269531]\n",
      "3031: [D loss: 0.689056, acc: 0.552734]  [A loss: 0.828228, acc: 0.199219]\n",
      "3032: [D loss: 0.712370, acc: 0.486328]  [A loss: 0.837391, acc: 0.183594]\n",
      "3033: [D loss: 0.699946, acc: 0.529297]  [A loss: 0.855770, acc: 0.171875]\n",
      "3034: [D loss: 0.712054, acc: 0.517578]  [A loss: 0.875832, acc: 0.156250]\n",
      "3035: [D loss: 0.690294, acc: 0.535156]  [A loss: 0.739482, acc: 0.378906]\n",
      "3036: [D loss: 0.706976, acc: 0.525391]  [A loss: 0.916401, acc: 0.128906]\n",
      "3037: [D loss: 0.704377, acc: 0.523438]  [A loss: 0.712025, acc: 0.476562]\n",
      "3038: [D loss: 0.714996, acc: 0.500000]  [A loss: 0.935548, acc: 0.097656]\n",
      "3039: [D loss: 0.694978, acc: 0.533203]  [A loss: 0.704823, acc: 0.507812]\n",
      "3040: [D loss: 0.719024, acc: 0.525391]  [A loss: 1.038804, acc: 0.046875]\n",
      "3041: [D loss: 0.713043, acc: 0.482422]  [A loss: 0.725109, acc: 0.441406]\n",
      "3042: [D loss: 0.714573, acc: 0.496094]  [A loss: 0.873802, acc: 0.121094]\n",
      "3043: [D loss: 0.698752, acc: 0.511719]  [A loss: 0.784419, acc: 0.281250]\n",
      "3044: [D loss: 0.690813, acc: 0.535156]  [A loss: 0.811201, acc: 0.250000]\n",
      "3045: [D loss: 0.706078, acc: 0.517578]  [A loss: 0.846324, acc: 0.218750]\n",
      "3046: [D loss: 0.705487, acc: 0.496094]  [A loss: 0.866262, acc: 0.128906]\n",
      "3047: [D loss: 0.688309, acc: 0.556641]  [A loss: 0.793332, acc: 0.285156]\n",
      "3048: [D loss: 0.700187, acc: 0.523438]  [A loss: 0.870173, acc: 0.191406]\n",
      "3049: [D loss: 0.693121, acc: 0.527344]  [A loss: 0.784974, acc: 0.335938]\n",
      "3050: [D loss: 0.701277, acc: 0.529297]  [A loss: 0.842497, acc: 0.183594]\n",
      "3051: [D loss: 0.694620, acc: 0.527344]  [A loss: 0.864981, acc: 0.179688]\n",
      "3052: [D loss: 0.698474, acc: 0.503906]  [A loss: 0.767382, acc: 0.308594]\n",
      "3053: [D loss: 0.706475, acc: 0.541016]  [A loss: 0.849171, acc: 0.210938]\n",
      "3054: [D loss: 0.701659, acc: 0.519531]  [A loss: 0.797070, acc: 0.265625]\n",
      "3055: [D loss: 0.699587, acc: 0.550781]  [A loss: 0.911622, acc: 0.140625]\n",
      "3056: [D loss: 0.695702, acc: 0.529297]  [A loss: 0.727752, acc: 0.417969]\n",
      "3057: [D loss: 0.708896, acc: 0.519531]  [A loss: 1.046203, acc: 0.050781]\n",
      "3058: [D loss: 0.704343, acc: 0.505859]  [A loss: 0.684085, acc: 0.535156]\n",
      "3059: [D loss: 0.740151, acc: 0.498047]  [A loss: 0.912225, acc: 0.093750]\n",
      "3060: [D loss: 0.693480, acc: 0.542969]  [A loss: 0.864153, acc: 0.183594]\n",
      "3061: [D loss: 0.688209, acc: 0.548828]  [A loss: 0.814045, acc: 0.265625]\n",
      "3062: [D loss: 0.706698, acc: 0.525391]  [A loss: 0.882485, acc: 0.160156]\n",
      "3063: [D loss: 0.685542, acc: 0.541016]  [A loss: 0.714241, acc: 0.460938]\n",
      "3064: [D loss: 0.708086, acc: 0.511719]  [A loss: 0.959010, acc: 0.093750]\n",
      "3065: [D loss: 0.687263, acc: 0.566406]  [A loss: 0.682955, acc: 0.578125]\n",
      "3066: [D loss: 0.722331, acc: 0.505859]  [A loss: 0.918249, acc: 0.093750]\n",
      "3067: [D loss: 0.696572, acc: 0.507812]  [A loss: 0.717694, acc: 0.464844]\n",
      "3068: [D loss: 0.708815, acc: 0.523438]  [A loss: 0.864369, acc: 0.140625]\n",
      "3069: [D loss: 0.688731, acc: 0.541016]  [A loss: 0.803014, acc: 0.238281]\n",
      "3070: [D loss: 0.690489, acc: 0.550781]  [A loss: 0.782371, acc: 0.324219]\n",
      "3071: [D loss: 0.697513, acc: 0.542969]  [A loss: 0.887559, acc: 0.144531]\n",
      "3072: [D loss: 0.691494, acc: 0.537109]  [A loss: 0.736701, acc: 0.406250]\n",
      "3073: [D loss: 0.728566, acc: 0.476562]  [A loss: 0.917032, acc: 0.101562]\n",
      "3074: [D loss: 0.695376, acc: 0.527344]  [A loss: 0.769382, acc: 0.304688]\n",
      "3075: [D loss: 0.707891, acc: 0.500000]  [A loss: 0.911163, acc: 0.097656]\n",
      "3076: [D loss: 0.696651, acc: 0.544922]  [A loss: 0.790663, acc: 0.316406]\n",
      "3077: [D loss: 0.704277, acc: 0.507812]  [A loss: 0.807006, acc: 0.296875]\n",
      "3078: [D loss: 0.693596, acc: 0.546875]  [A loss: 0.779620, acc: 0.335938]\n",
      "3079: [D loss: 0.704651, acc: 0.525391]  [A loss: 0.815331, acc: 0.250000]\n",
      "3080: [D loss: 0.698956, acc: 0.531250]  [A loss: 0.808062, acc: 0.285156]\n",
      "3081: [D loss: 0.688428, acc: 0.541016]  [A loss: 0.834183, acc: 0.191406]\n",
      "3082: [D loss: 0.695996, acc: 0.523438]  [A loss: 0.793513, acc: 0.277344]\n",
      "3083: [D loss: 0.702173, acc: 0.529297]  [A loss: 0.900521, acc: 0.148438]\n",
      "3084: [D loss: 0.691828, acc: 0.541016]  [A loss: 0.733385, acc: 0.421875]\n",
      "3085: [D loss: 0.726621, acc: 0.480469]  [A loss: 0.857595, acc: 0.187500]\n",
      "3086: [D loss: 0.693396, acc: 0.535156]  [A loss: 0.773794, acc: 0.339844]\n",
      "3087: [D loss: 0.713253, acc: 0.492188]  [A loss: 0.926299, acc: 0.125000]\n",
      "3088: [D loss: 0.707418, acc: 0.501953]  [A loss: 0.784671, acc: 0.296875]\n",
      "3089: [D loss: 0.704057, acc: 0.498047]  [A loss: 0.886297, acc: 0.144531]\n",
      "3090: [D loss: 0.691647, acc: 0.544922]  [A loss: 0.771451, acc: 0.378906]\n",
      "3091: [D loss: 0.696222, acc: 0.552734]  [A loss: 0.863244, acc: 0.187500]\n",
      "3092: [D loss: 0.687789, acc: 0.566406]  [A loss: 0.693090, acc: 0.542969]\n",
      "3093: [D loss: 0.724144, acc: 0.498047]  [A loss: 0.986685, acc: 0.066406]\n",
      "3094: [D loss: 0.705145, acc: 0.507812]  [A loss: 0.728909, acc: 0.417969]\n",
      "3095: [D loss: 0.714824, acc: 0.503906]  [A loss: 0.905163, acc: 0.140625]\n",
      "3096: [D loss: 0.698076, acc: 0.509766]  [A loss: 0.780482, acc: 0.335938]\n",
      "3097: [D loss: 0.703200, acc: 0.552734]  [A loss: 0.881203, acc: 0.167969]\n",
      "3098: [D loss: 0.704495, acc: 0.498047]  [A loss: 0.820641, acc: 0.234375]\n",
      "3099: [D loss: 0.692897, acc: 0.554688]  [A loss: 0.823344, acc: 0.218750]\n",
      "3100: [D loss: 0.712629, acc: 0.486328]  [A loss: 0.873920, acc: 0.140625]\n",
      "3101: [D loss: 0.682882, acc: 0.568359]  [A loss: 0.796775, acc: 0.277344]\n",
      "3102: [D loss: 0.706713, acc: 0.519531]  [A loss: 0.836627, acc: 0.167969]\n",
      "3103: [D loss: 0.694922, acc: 0.552734]  [A loss: 0.852326, acc: 0.152344]\n",
      "3104: [D loss: 0.696271, acc: 0.509766]  [A loss: 0.904309, acc: 0.148438]\n",
      "3105: [D loss: 0.686042, acc: 0.568359]  [A loss: 0.782843, acc: 0.308594]\n",
      "3106: [D loss: 0.703666, acc: 0.511719]  [A loss: 0.893348, acc: 0.156250]\n",
      "3107: [D loss: 0.686371, acc: 0.533203]  [A loss: 0.698334, acc: 0.511719]\n",
      "3108: [D loss: 0.705673, acc: 0.531250]  [A loss: 1.002563, acc: 0.062500]\n",
      "3109: [D loss: 0.692782, acc: 0.517578]  [A loss: 0.658842, acc: 0.617188]\n",
      "3110: [D loss: 0.725080, acc: 0.517578]  [A loss: 0.874028, acc: 0.164062]\n",
      "3111: [D loss: 0.716249, acc: 0.496094]  [A loss: 0.852471, acc: 0.187500]\n",
      "3112: [D loss: 0.702165, acc: 0.501953]  [A loss: 0.826470, acc: 0.218750]\n",
      "3113: [D loss: 0.716946, acc: 0.464844]  [A loss: 0.814735, acc: 0.273438]\n",
      "3114: [D loss: 0.704068, acc: 0.525391]  [A loss: 0.840943, acc: 0.195312]\n",
      "3115: [D loss: 0.692276, acc: 0.523438]  [A loss: 0.834581, acc: 0.203125]\n",
      "3116: [D loss: 0.696407, acc: 0.513672]  [A loss: 0.768663, acc: 0.316406]\n",
      "3117: [D loss: 0.704510, acc: 0.515625]  [A loss: 0.842888, acc: 0.230469]\n",
      "3118: [D loss: 0.688375, acc: 0.548828]  [A loss: 0.797588, acc: 0.324219]\n",
      "3119: [D loss: 0.695123, acc: 0.544922]  [A loss: 0.795010, acc: 0.312500]\n",
      "3120: [D loss: 0.708453, acc: 0.501953]  [A loss: 0.838669, acc: 0.195312]\n",
      "3121: [D loss: 0.694222, acc: 0.541016]  [A loss: 0.813417, acc: 0.222656]\n",
      "3122: [D loss: 0.714625, acc: 0.519531]  [A loss: 0.933936, acc: 0.136719]\n",
      "3123: [D loss: 0.694972, acc: 0.537109]  [A loss: 0.798857, acc: 0.300781]\n",
      "3124: [D loss: 0.699088, acc: 0.523438]  [A loss: 0.883469, acc: 0.132812]\n",
      "3125: [D loss: 0.702587, acc: 0.515625]  [A loss: 0.704983, acc: 0.507812]\n",
      "3126: [D loss: 0.710656, acc: 0.541016]  [A loss: 1.015129, acc: 0.058594]\n",
      "3127: [D loss: 0.708899, acc: 0.519531]  [A loss: 0.717646, acc: 0.480469]\n",
      "3128: [D loss: 0.718794, acc: 0.507812]  [A loss: 0.915091, acc: 0.093750]\n",
      "3129: [D loss: 0.685849, acc: 0.570312]  [A loss: 0.698202, acc: 0.500000]\n",
      "3130: [D loss: 0.715914, acc: 0.523438]  [A loss: 1.009332, acc: 0.039062]\n",
      "3131: [D loss: 0.704375, acc: 0.525391]  [A loss: 0.686194, acc: 0.535156]\n",
      "3132: [D loss: 0.719846, acc: 0.501953]  [A loss: 0.867983, acc: 0.191406]\n",
      "3133: [D loss: 0.683661, acc: 0.564453]  [A loss: 0.801399, acc: 0.312500]\n",
      "3134: [D loss: 0.712442, acc: 0.515625]  [A loss: 0.855853, acc: 0.187500]\n",
      "3135: [D loss: 0.698496, acc: 0.527344]  [A loss: 0.803843, acc: 0.238281]\n",
      "3136: [D loss: 0.702488, acc: 0.511719]  [A loss: 0.876648, acc: 0.156250]\n",
      "3137: [D loss: 0.708815, acc: 0.503906]  [A loss: 0.829009, acc: 0.199219]\n",
      "3138: [D loss: 0.699801, acc: 0.509766]  [A loss: 0.908220, acc: 0.128906]\n",
      "3139: [D loss: 0.695349, acc: 0.517578]  [A loss: 0.822303, acc: 0.257812]\n",
      "3140: [D loss: 0.696971, acc: 0.533203]  [A loss: 0.834371, acc: 0.226562]\n",
      "3141: [D loss: 0.690500, acc: 0.542969]  [A loss: 0.841391, acc: 0.191406]\n",
      "3142: [D loss: 0.700083, acc: 0.523438]  [A loss: 0.849873, acc: 0.199219]\n",
      "3143: [D loss: 0.699468, acc: 0.507812]  [A loss: 0.808680, acc: 0.296875]\n",
      "3144: [D loss: 0.716177, acc: 0.517578]  [A loss: 0.882705, acc: 0.132812]\n",
      "3145: [D loss: 0.696452, acc: 0.525391]  [A loss: 0.792063, acc: 0.300781]\n",
      "3146: [D loss: 0.703958, acc: 0.539062]  [A loss: 0.881856, acc: 0.156250]\n",
      "3147: [D loss: 0.723076, acc: 0.470703]  [A loss: 0.871351, acc: 0.144531]\n",
      "3148: [D loss: 0.693410, acc: 0.541016]  [A loss: 0.836051, acc: 0.207031]\n",
      "3149: [D loss: 0.700554, acc: 0.515625]  [A loss: 0.835990, acc: 0.207031]\n",
      "3150: [D loss: 0.696888, acc: 0.537109]  [A loss: 0.807688, acc: 0.269531]\n",
      "3151: [D loss: 0.707182, acc: 0.503906]  [A loss: 1.001328, acc: 0.058594]\n",
      "3152: [D loss: 0.697700, acc: 0.513672]  [A loss: 0.684047, acc: 0.539062]\n",
      "3153: [D loss: 0.726760, acc: 0.517578]  [A loss: 0.966621, acc: 0.089844]\n",
      "3154: [D loss: 0.719417, acc: 0.476562]  [A loss: 0.706752, acc: 0.492188]\n",
      "3155: [D loss: 0.719983, acc: 0.500000]  [A loss: 0.932931, acc: 0.074219]\n",
      "3156: [D loss: 0.686460, acc: 0.544922]  [A loss: 0.695985, acc: 0.515625]\n",
      "3157: [D loss: 0.714589, acc: 0.515625]  [A loss: 0.969964, acc: 0.050781]\n",
      "3158: [D loss: 0.713799, acc: 0.486328]  [A loss: 0.740670, acc: 0.394531]\n",
      "3159: [D loss: 0.698912, acc: 0.531250]  [A loss: 0.831513, acc: 0.210938]\n",
      "3160: [D loss: 0.702950, acc: 0.500000]  [A loss: 0.795996, acc: 0.250000]\n",
      "3161: [D loss: 0.699534, acc: 0.537109]  [A loss: 0.848072, acc: 0.191406]\n",
      "3162: [D loss: 0.711382, acc: 0.503906]  [A loss: 0.841385, acc: 0.175781]\n",
      "3163: [D loss: 0.701632, acc: 0.531250]  [A loss: 0.819411, acc: 0.175781]\n",
      "3164: [D loss: 0.696898, acc: 0.541016]  [A loss: 0.751858, acc: 0.386719]\n",
      "3165: [D loss: 0.708895, acc: 0.507812]  [A loss: 0.955673, acc: 0.105469]\n",
      "3166: [D loss: 0.702795, acc: 0.521484]  [A loss: 0.723247, acc: 0.453125]\n",
      "3167: [D loss: 0.707551, acc: 0.501953]  [A loss: 0.900434, acc: 0.105469]\n",
      "3168: [D loss: 0.701033, acc: 0.505859]  [A loss: 0.715168, acc: 0.472656]\n",
      "3169: [D loss: 0.725309, acc: 0.501953]  [A loss: 0.926958, acc: 0.089844]\n",
      "3170: [D loss: 0.701283, acc: 0.500000]  [A loss: 0.755046, acc: 0.371094]\n",
      "3171: [D loss: 0.707710, acc: 0.521484]  [A loss: 0.890649, acc: 0.113281]\n",
      "3172: [D loss: 0.701603, acc: 0.523438]  [A loss: 0.772195, acc: 0.300781]\n",
      "3173: [D loss: 0.710340, acc: 0.494141]  [A loss: 0.877074, acc: 0.164062]\n",
      "3174: [D loss: 0.703236, acc: 0.519531]  [A loss: 0.799820, acc: 0.250000]\n",
      "3175: [D loss: 0.716984, acc: 0.509766]  [A loss: 0.808359, acc: 0.238281]\n",
      "3176: [D loss: 0.698057, acc: 0.542969]  [A loss: 0.873088, acc: 0.125000]\n",
      "3177: [D loss: 0.691003, acc: 0.509766]  [A loss: 0.739344, acc: 0.394531]\n",
      "3178: [D loss: 0.732853, acc: 0.474609]  [A loss: 0.926224, acc: 0.085938]\n",
      "3179: [D loss: 0.708478, acc: 0.476562]  [A loss: 0.800922, acc: 0.257812]\n",
      "3180: [D loss: 0.700464, acc: 0.503906]  [A loss: 0.884095, acc: 0.125000]\n",
      "3181: [D loss: 0.695100, acc: 0.484375]  [A loss: 0.756337, acc: 0.386719]\n",
      "3182: [D loss: 0.686736, acc: 0.544922]  [A loss: 0.892376, acc: 0.128906]\n",
      "3183: [D loss: 0.703565, acc: 0.482422]  [A loss: 0.702266, acc: 0.515625]\n",
      "3184: [D loss: 0.704595, acc: 0.535156]  [A loss: 0.898158, acc: 0.125000]\n",
      "3185: [D loss: 0.688681, acc: 0.542969]  [A loss: 0.725460, acc: 0.453125]\n",
      "3186: [D loss: 0.706559, acc: 0.531250]  [A loss: 0.869761, acc: 0.160156]\n",
      "3187: [D loss: 0.705270, acc: 0.492188]  [A loss: 0.780085, acc: 0.292969]\n",
      "3188: [D loss: 0.696304, acc: 0.531250]  [A loss: 0.761384, acc: 0.339844]\n",
      "3189: [D loss: 0.708949, acc: 0.533203]  [A loss: 1.000078, acc: 0.042969]\n",
      "3190: [D loss: 0.698542, acc: 0.500000]  [A loss: 0.730457, acc: 0.429688]\n",
      "3191: [D loss: 0.709018, acc: 0.513672]  [A loss: 0.847351, acc: 0.171875]\n",
      "3192: [D loss: 0.691806, acc: 0.554688]  [A loss: 0.770050, acc: 0.312500]\n",
      "3193: [D loss: 0.706803, acc: 0.492188]  [A loss: 0.793944, acc: 0.324219]\n",
      "3194: [D loss: 0.699812, acc: 0.523438]  [A loss: 0.807724, acc: 0.281250]\n",
      "3195: [D loss: 0.725492, acc: 0.488281]  [A loss: 0.987605, acc: 0.054688]\n",
      "3196: [D loss: 0.692365, acc: 0.517578]  [A loss: 0.697803, acc: 0.523438]\n",
      "3197: [D loss: 0.714397, acc: 0.525391]  [A loss: 0.894114, acc: 0.121094]\n",
      "3198: [D loss: 0.696145, acc: 0.533203]  [A loss: 0.723833, acc: 0.449219]\n",
      "3199: [D loss: 0.711190, acc: 0.492188]  [A loss: 0.851554, acc: 0.218750]\n",
      "3200: [D loss: 0.680380, acc: 0.570312]  [A loss: 0.785883, acc: 0.292969]\n",
      "3201: [D loss: 0.709668, acc: 0.500000]  [A loss: 0.858835, acc: 0.175781]\n",
      "3202: [D loss: 0.706918, acc: 0.492188]  [A loss: 0.737814, acc: 0.441406]\n",
      "3203: [D loss: 0.717015, acc: 0.482422]  [A loss: 0.877392, acc: 0.175781]\n",
      "3204: [D loss: 0.690218, acc: 0.550781]  [A loss: 0.828351, acc: 0.222656]\n",
      "3205: [D loss: 0.701875, acc: 0.492188]  [A loss: 0.829525, acc: 0.207031]\n",
      "3206: [D loss: 0.684668, acc: 0.533203]  [A loss: 0.783092, acc: 0.312500]\n",
      "3207: [D loss: 0.697954, acc: 0.529297]  [A loss: 0.836098, acc: 0.218750]\n",
      "3208: [D loss: 0.679609, acc: 0.570312]  [A loss: 0.763589, acc: 0.367188]\n",
      "3209: [D loss: 0.715835, acc: 0.503906]  [A loss: 0.924014, acc: 0.070312]\n",
      "3210: [D loss: 0.694962, acc: 0.513672]  [A loss: 0.707424, acc: 0.492188]\n",
      "3211: [D loss: 0.707786, acc: 0.527344]  [A loss: 1.015317, acc: 0.054688]\n",
      "3212: [D loss: 0.687412, acc: 0.542969]  [A loss: 0.689786, acc: 0.527344]\n",
      "3213: [D loss: 0.713075, acc: 0.513672]  [A loss: 0.905309, acc: 0.125000]\n",
      "3214: [D loss: 0.693073, acc: 0.521484]  [A loss: 0.815736, acc: 0.207031]\n",
      "3215: [D loss: 0.711743, acc: 0.501953]  [A loss: 0.813736, acc: 0.257812]\n",
      "3216: [D loss: 0.699774, acc: 0.539062]  [A loss: 0.860862, acc: 0.148438]\n",
      "3217: [D loss: 0.712708, acc: 0.501953]  [A loss: 0.839132, acc: 0.207031]\n",
      "3218: [D loss: 0.699727, acc: 0.523438]  [A loss: 0.867383, acc: 0.160156]\n",
      "3219: [D loss: 0.697912, acc: 0.533203]  [A loss: 0.850046, acc: 0.195312]\n",
      "3220: [D loss: 0.708494, acc: 0.525391]  [A loss: 0.811466, acc: 0.285156]\n",
      "3221: [D loss: 0.702235, acc: 0.517578]  [A loss: 0.893759, acc: 0.113281]\n",
      "3222: [D loss: 0.691866, acc: 0.556641]  [A loss: 0.748670, acc: 0.414062]\n",
      "3223: [D loss: 0.715256, acc: 0.507812]  [A loss: 0.977231, acc: 0.097656]\n",
      "3224: [D loss: 0.686578, acc: 0.589844]  [A loss: 0.772295, acc: 0.347656]\n",
      "3225: [D loss: 0.710469, acc: 0.484375]  [A loss: 0.924551, acc: 0.089844]\n",
      "3226: [D loss: 0.696250, acc: 0.509766]  [A loss: 0.810832, acc: 0.265625]\n",
      "3227: [D loss: 0.700170, acc: 0.550781]  [A loss: 0.878821, acc: 0.167969]\n",
      "3228: [D loss: 0.705351, acc: 0.517578]  [A loss: 0.772538, acc: 0.339844]\n",
      "3229: [D loss: 0.710344, acc: 0.509766]  [A loss: 0.895814, acc: 0.136719]\n",
      "3230: [D loss: 0.700406, acc: 0.507812]  [A loss: 0.760434, acc: 0.343750]\n",
      "3231: [D loss: 0.711827, acc: 0.507812]  [A loss: 0.943070, acc: 0.074219]\n",
      "3232: [D loss: 0.700384, acc: 0.531250]  [A loss: 0.730668, acc: 0.445312]\n",
      "3233: [D loss: 0.707995, acc: 0.523438]  [A loss: 0.912486, acc: 0.121094]\n",
      "3234: [D loss: 0.685148, acc: 0.554688]  [A loss: 0.772594, acc: 0.296875]\n",
      "3235: [D loss: 0.699155, acc: 0.527344]  [A loss: 0.928357, acc: 0.089844]\n",
      "3236: [D loss: 0.702409, acc: 0.523438]  [A loss: 0.771208, acc: 0.347656]\n",
      "3237: [D loss: 0.707592, acc: 0.533203]  [A loss: 0.904611, acc: 0.132812]\n",
      "3238: [D loss: 0.706946, acc: 0.494141]  [A loss: 0.800711, acc: 0.269531]\n",
      "3239: [D loss: 0.704738, acc: 0.507812]  [A loss: 0.800948, acc: 0.253906]\n",
      "3240: [D loss: 0.701788, acc: 0.519531]  [A loss: 0.911228, acc: 0.132812]\n",
      "3241: [D loss: 0.694735, acc: 0.531250]  [A loss: 0.744294, acc: 0.421875]\n",
      "3242: [D loss: 0.699781, acc: 0.517578]  [A loss: 0.867419, acc: 0.136719]\n",
      "3243: [D loss: 0.697757, acc: 0.525391]  [A loss: 0.827134, acc: 0.242188]\n",
      "3244: [D loss: 0.712372, acc: 0.470703]  [A loss: 0.838584, acc: 0.203125]\n",
      "3245: [D loss: 0.698262, acc: 0.503906]  [A loss: 0.792916, acc: 0.296875]\n",
      "3246: [D loss: 0.700511, acc: 0.515625]  [A loss: 0.882415, acc: 0.121094]\n",
      "3247: [D loss: 0.700829, acc: 0.517578]  [A loss: 0.798512, acc: 0.265625]\n",
      "3248: [D loss: 0.714569, acc: 0.488281]  [A loss: 0.902815, acc: 0.140625]\n",
      "3249: [D loss: 0.702347, acc: 0.531250]  [A loss: 0.738475, acc: 0.421875]\n",
      "3250: [D loss: 0.708812, acc: 0.505859]  [A loss: 0.928376, acc: 0.085938]\n",
      "3251: [D loss: 0.688527, acc: 0.541016]  [A loss: 0.745330, acc: 0.382812]\n",
      "3252: [D loss: 0.709982, acc: 0.507812]  [A loss: 0.925741, acc: 0.144531]\n",
      "3253: [D loss: 0.709985, acc: 0.486328]  [A loss: 0.746495, acc: 0.394531]\n",
      "3254: [D loss: 0.720501, acc: 0.500000]  [A loss: 0.920707, acc: 0.125000]\n",
      "3255: [D loss: 0.698929, acc: 0.523438]  [A loss: 0.716989, acc: 0.445312]\n",
      "3256: [D loss: 0.724018, acc: 0.505859]  [A loss: 0.905311, acc: 0.097656]\n",
      "3257: [D loss: 0.714598, acc: 0.480469]  [A loss: 0.770722, acc: 0.308594]\n",
      "3258: [D loss: 0.704781, acc: 0.521484]  [A loss: 0.917672, acc: 0.085938]\n",
      "3259: [D loss: 0.685604, acc: 0.533203]  [A loss: 0.746612, acc: 0.390625]\n",
      "3260: [D loss: 0.691220, acc: 0.531250]  [A loss: 0.892536, acc: 0.136719]\n",
      "3261: [D loss: 0.695547, acc: 0.537109]  [A loss: 0.747442, acc: 0.402344]\n",
      "3262: [D loss: 0.723828, acc: 0.507812]  [A loss: 0.894864, acc: 0.148438]\n",
      "3263: [D loss: 0.701065, acc: 0.511719]  [A loss: 0.837359, acc: 0.171875]\n",
      "3264: [D loss: 0.699192, acc: 0.552734]  [A loss: 0.910206, acc: 0.109375]\n",
      "3265: [D loss: 0.702338, acc: 0.509766]  [A loss: 0.837645, acc: 0.203125]\n",
      "3266: [D loss: 0.698825, acc: 0.517578]  [A loss: 0.844973, acc: 0.218750]\n",
      "3267: [D loss: 0.696708, acc: 0.539062]  [A loss: 0.794090, acc: 0.304688]\n",
      "3268: [D loss: 0.694654, acc: 0.535156]  [A loss: 0.871131, acc: 0.140625]\n",
      "3269: [D loss: 0.685990, acc: 0.533203]  [A loss: 0.788384, acc: 0.312500]\n",
      "3270: [D loss: 0.709571, acc: 0.511719]  [A loss: 0.900478, acc: 0.121094]\n",
      "3271: [D loss: 0.697227, acc: 0.525391]  [A loss: 0.681957, acc: 0.550781]\n",
      "3272: [D loss: 0.743815, acc: 0.488281]  [A loss: 1.075401, acc: 0.015625]\n",
      "3273: [D loss: 0.698322, acc: 0.544922]  [A loss: 0.666401, acc: 0.566406]\n",
      "3274: [D loss: 0.743478, acc: 0.486328]  [A loss: 1.024998, acc: 0.039062]\n",
      "3275: [D loss: 0.700123, acc: 0.537109]  [A loss: 0.689978, acc: 0.535156]\n",
      "3276: [D loss: 0.721059, acc: 0.523438]  [A loss: 0.858664, acc: 0.191406]\n",
      "3277: [D loss: 0.695112, acc: 0.521484]  [A loss: 0.763060, acc: 0.300781]\n",
      "3278: [D loss: 0.708436, acc: 0.523438]  [A loss: 0.782598, acc: 0.269531]\n",
      "3279: [D loss: 0.714611, acc: 0.484375]  [A loss: 0.812795, acc: 0.214844]\n",
      "3280: [D loss: 0.698574, acc: 0.531250]  [A loss: 0.784563, acc: 0.328125]\n",
      "3281: [D loss: 0.703190, acc: 0.521484]  [A loss: 0.867235, acc: 0.144531]\n",
      "3282: [D loss: 0.688409, acc: 0.542969]  [A loss: 0.734315, acc: 0.410156]\n",
      "3283: [D loss: 0.707189, acc: 0.527344]  [A loss: 0.833509, acc: 0.234375]\n",
      "3284: [D loss: 0.701310, acc: 0.507812]  [A loss: 0.838449, acc: 0.238281]\n",
      "3285: [D loss: 0.703299, acc: 0.527344]  [A loss: 0.908773, acc: 0.132812]\n",
      "3286: [D loss: 0.695474, acc: 0.527344]  [A loss: 0.682566, acc: 0.578125]\n",
      "3287: [D loss: 0.727196, acc: 0.517578]  [A loss: 0.932944, acc: 0.097656]\n",
      "3288: [D loss: 0.688027, acc: 0.541016]  [A loss: 0.690429, acc: 0.511719]\n",
      "3289: [D loss: 0.714232, acc: 0.529297]  [A loss: 0.978382, acc: 0.128906]\n",
      "3290: [D loss: 0.717270, acc: 0.480469]  [A loss: 0.773148, acc: 0.320312]\n",
      "3291: [D loss: 0.716519, acc: 0.492188]  [A loss: 0.828603, acc: 0.207031]\n",
      "3292: [D loss: 0.695123, acc: 0.539062]  [A loss: 0.750312, acc: 0.347656]\n",
      "3293: [D loss: 0.695289, acc: 0.541016]  [A loss: 0.857979, acc: 0.210938]\n",
      "3294: [D loss: 0.698380, acc: 0.531250]  [A loss: 0.819551, acc: 0.253906]\n",
      "3295: [D loss: 0.694164, acc: 0.550781]  [A loss: 0.801276, acc: 0.246094]\n",
      "3296: [D loss: 0.706614, acc: 0.501953]  [A loss: 0.861543, acc: 0.136719]\n",
      "3297: [D loss: 0.694426, acc: 0.550781]  [A loss: 0.775588, acc: 0.347656]\n",
      "3298: [D loss: 0.715233, acc: 0.498047]  [A loss: 0.910236, acc: 0.109375]\n",
      "3299: [D loss: 0.681029, acc: 0.585938]  [A loss: 0.736793, acc: 0.398438]\n",
      "3300: [D loss: 0.713849, acc: 0.517578]  [A loss: 0.935414, acc: 0.082031]\n",
      "3301: [D loss: 0.693438, acc: 0.517578]  [A loss: 0.725920, acc: 0.433594]\n",
      "3302: [D loss: 0.708657, acc: 0.517578]  [A loss: 0.896362, acc: 0.101562]\n",
      "3303: [D loss: 0.710405, acc: 0.501953]  [A loss: 0.778220, acc: 0.351562]\n",
      "3304: [D loss: 0.697286, acc: 0.525391]  [A loss: 0.857084, acc: 0.167969]\n",
      "3305: [D loss: 0.709971, acc: 0.488281]  [A loss: 0.768931, acc: 0.332031]\n",
      "3306: [D loss: 0.709085, acc: 0.531250]  [A loss: 0.890336, acc: 0.128906]\n",
      "3307: [D loss: 0.708217, acc: 0.478516]  [A loss: 0.743947, acc: 0.386719]\n",
      "3308: [D loss: 0.721895, acc: 0.505859]  [A loss: 0.950986, acc: 0.078125]\n",
      "3309: [D loss: 0.690225, acc: 0.537109]  [A loss: 0.717349, acc: 0.453125]\n",
      "3310: [D loss: 0.713551, acc: 0.509766]  [A loss: 0.884563, acc: 0.128906]\n",
      "3311: [D loss: 0.704661, acc: 0.503906]  [A loss: 0.831401, acc: 0.199219]\n",
      "3312: [D loss: 0.697651, acc: 0.521484]  [A loss: 0.781289, acc: 0.324219]\n",
      "3313: [D loss: 0.706440, acc: 0.494141]  [A loss: 0.846102, acc: 0.175781]\n",
      "3314: [D loss: 0.703396, acc: 0.521484]  [A loss: 0.807838, acc: 0.277344]\n",
      "3315: [D loss: 0.690491, acc: 0.560547]  [A loss: 0.860936, acc: 0.210938]\n",
      "3316: [D loss: 0.701856, acc: 0.507812]  [A loss: 0.748955, acc: 0.347656]\n",
      "3317: [D loss: 0.715181, acc: 0.509766]  [A loss: 0.855479, acc: 0.160156]\n",
      "3318: [D loss: 0.700789, acc: 0.523438]  [A loss: 0.765859, acc: 0.316406]\n",
      "3319: [D loss: 0.729412, acc: 0.478516]  [A loss: 0.863111, acc: 0.167969]\n",
      "3320: [D loss: 0.702834, acc: 0.496094]  [A loss: 0.739053, acc: 0.378906]\n",
      "3321: [D loss: 0.702403, acc: 0.519531]  [A loss: 0.929469, acc: 0.125000]\n",
      "3322: [D loss: 0.709571, acc: 0.511719]  [A loss: 0.766562, acc: 0.324219]\n",
      "3323: [D loss: 0.711209, acc: 0.484375]  [A loss: 0.828500, acc: 0.273438]\n",
      "3324: [D loss: 0.697310, acc: 0.537109]  [A loss: 0.863751, acc: 0.156250]\n",
      "3325: [D loss: 0.706562, acc: 0.513672]  [A loss: 0.818660, acc: 0.230469]\n",
      "3326: [D loss: 0.702161, acc: 0.494141]  [A loss: 0.752891, acc: 0.355469]\n",
      "3327: [D loss: 0.685861, acc: 0.564453]  [A loss: 0.845156, acc: 0.207031]\n",
      "3328: [D loss: 0.709193, acc: 0.490234]  [A loss: 0.815144, acc: 0.210938]\n",
      "3329: [D loss: 0.699955, acc: 0.494141]  [A loss: 0.782004, acc: 0.328125]\n",
      "3330: [D loss: 0.708035, acc: 0.519531]  [A loss: 0.807592, acc: 0.234375]\n",
      "3331: [D loss: 0.688463, acc: 0.535156]  [A loss: 0.817955, acc: 0.230469]\n",
      "3332: [D loss: 0.699408, acc: 0.527344]  [A loss: 0.886523, acc: 0.167969]\n",
      "3333: [D loss: 0.698704, acc: 0.517578]  [A loss: 0.788164, acc: 0.285156]\n",
      "3334: [D loss: 0.707987, acc: 0.503906]  [A loss: 0.878938, acc: 0.128906]\n",
      "3335: [D loss: 0.696441, acc: 0.525391]  [A loss: 0.790841, acc: 0.265625]\n",
      "3336: [D loss: 0.706126, acc: 0.515625]  [A loss: 0.910544, acc: 0.132812]\n",
      "3337: [D loss: 0.696023, acc: 0.509766]  [A loss: 0.720981, acc: 0.468750]\n",
      "3338: [D loss: 0.721751, acc: 0.496094]  [A loss: 0.908089, acc: 0.113281]\n",
      "3339: [D loss: 0.698741, acc: 0.533203]  [A loss: 0.796527, acc: 0.335938]\n",
      "3340: [D loss: 0.700402, acc: 0.523438]  [A loss: 0.926895, acc: 0.113281]\n",
      "3341: [D loss: 0.696797, acc: 0.537109]  [A loss: 0.711598, acc: 0.511719]\n",
      "3342: [D loss: 0.730817, acc: 0.492188]  [A loss: 0.945920, acc: 0.070312]\n",
      "3343: [D loss: 0.699808, acc: 0.529297]  [A loss: 0.718920, acc: 0.437500]\n",
      "3344: [D loss: 0.702679, acc: 0.529297]  [A loss: 0.864533, acc: 0.171875]\n",
      "3345: [D loss: 0.702345, acc: 0.486328]  [A loss: 0.711493, acc: 0.464844]\n",
      "3346: [D loss: 0.706462, acc: 0.515625]  [A loss: 0.905218, acc: 0.117188]\n",
      "3347: [D loss: 0.694481, acc: 0.539062]  [A loss: 0.735450, acc: 0.398438]\n",
      "3348: [D loss: 0.694946, acc: 0.531250]  [A loss: 0.827186, acc: 0.234375]\n",
      "3349: [D loss: 0.694956, acc: 0.527344]  [A loss: 0.808818, acc: 0.257812]\n",
      "3350: [D loss: 0.716292, acc: 0.484375]  [A loss: 0.901660, acc: 0.132812]\n",
      "3351: [D loss: 0.693504, acc: 0.535156]  [A loss: 0.722041, acc: 0.468750]\n",
      "3352: [D loss: 0.707791, acc: 0.515625]  [A loss: 0.923429, acc: 0.109375]\n",
      "3353: [D loss: 0.696627, acc: 0.525391]  [A loss: 0.716778, acc: 0.484375]\n",
      "3354: [D loss: 0.712386, acc: 0.517578]  [A loss: 0.889305, acc: 0.136719]\n",
      "3355: [D loss: 0.705076, acc: 0.511719]  [A loss: 0.754860, acc: 0.378906]\n",
      "3356: [D loss: 0.694420, acc: 0.546875]  [A loss: 0.821230, acc: 0.222656]\n",
      "3357: [D loss: 0.695252, acc: 0.513672]  [A loss: 0.808548, acc: 0.261719]\n",
      "3358: [D loss: 0.701879, acc: 0.505859]  [A loss: 0.779431, acc: 0.339844]\n",
      "3359: [D loss: 0.710068, acc: 0.513672]  [A loss: 0.868051, acc: 0.160156]\n",
      "3360: [D loss: 0.697121, acc: 0.527344]  [A loss: 0.768518, acc: 0.304688]\n",
      "3361: [D loss: 0.705152, acc: 0.519531]  [A loss: 0.898406, acc: 0.144531]\n",
      "3362: [D loss: 0.696220, acc: 0.531250]  [A loss: 0.734362, acc: 0.402344]\n",
      "3363: [D loss: 0.711744, acc: 0.496094]  [A loss: 0.885885, acc: 0.156250]\n",
      "3364: [D loss: 0.694893, acc: 0.525391]  [A loss: 0.725994, acc: 0.433594]\n",
      "3365: [D loss: 0.711097, acc: 0.500000]  [A loss: 0.937200, acc: 0.101562]\n",
      "3366: [D loss: 0.694905, acc: 0.535156]  [A loss: 0.720105, acc: 0.460938]\n",
      "3367: [D loss: 0.704081, acc: 0.521484]  [A loss: 0.866781, acc: 0.152344]\n",
      "3368: [D loss: 0.704389, acc: 0.486328]  [A loss: 0.812101, acc: 0.210938]\n",
      "3369: [D loss: 0.716647, acc: 0.478516]  [A loss: 0.830673, acc: 0.222656]\n",
      "3370: [D loss: 0.708353, acc: 0.474609]  [A loss: 0.768708, acc: 0.332031]\n",
      "3371: [D loss: 0.703118, acc: 0.515625]  [A loss: 0.865327, acc: 0.167969]\n",
      "3372: [D loss: 0.693598, acc: 0.503906]  [A loss: 0.785177, acc: 0.285156]\n",
      "3373: [D loss: 0.733625, acc: 0.458984]  [A loss: 0.858844, acc: 0.156250]\n",
      "3374: [D loss: 0.713439, acc: 0.527344]  [A loss: 0.938451, acc: 0.054688]\n",
      "3375: [D loss: 0.689433, acc: 0.542969]  [A loss: 0.764655, acc: 0.367188]\n",
      "3376: [D loss: 0.702242, acc: 0.535156]  [A loss: 0.897029, acc: 0.093750]\n",
      "3377: [D loss: 0.698730, acc: 0.501953]  [A loss: 0.768975, acc: 0.359375]\n",
      "3378: [D loss: 0.714945, acc: 0.496094]  [A loss: 0.875435, acc: 0.171875]\n",
      "3379: [D loss: 0.704887, acc: 0.501953]  [A loss: 0.684588, acc: 0.554688]\n",
      "3380: [D loss: 0.710170, acc: 0.490234]  [A loss: 0.899015, acc: 0.082031]\n",
      "3381: [D loss: 0.699266, acc: 0.509766]  [A loss: 0.700231, acc: 0.519531]\n",
      "3382: [D loss: 0.715367, acc: 0.523438]  [A loss: 0.908928, acc: 0.078125]\n",
      "3383: [D loss: 0.703740, acc: 0.533203]  [A loss: 0.765469, acc: 0.355469]\n",
      "3384: [D loss: 0.709415, acc: 0.507812]  [A loss: 0.881197, acc: 0.125000]\n",
      "3385: [D loss: 0.698622, acc: 0.542969]  [A loss: 0.738432, acc: 0.398438]\n",
      "3386: [D loss: 0.705815, acc: 0.511719]  [A loss: 0.868333, acc: 0.171875]\n",
      "3387: [D loss: 0.692643, acc: 0.535156]  [A loss: 0.768730, acc: 0.316406]\n",
      "3388: [D loss: 0.700994, acc: 0.539062]  [A loss: 0.838016, acc: 0.175781]\n",
      "3389: [D loss: 0.700139, acc: 0.521484]  [A loss: 0.744420, acc: 0.406250]\n",
      "3390: [D loss: 0.712567, acc: 0.535156]  [A loss: 0.937340, acc: 0.085938]\n",
      "3391: [D loss: 0.702569, acc: 0.488281]  [A loss: 0.787473, acc: 0.300781]\n",
      "3392: [D loss: 0.716496, acc: 0.490234]  [A loss: 0.839160, acc: 0.183594]\n",
      "3393: [D loss: 0.696180, acc: 0.496094]  [A loss: 0.706052, acc: 0.476562]\n",
      "3394: [D loss: 0.702797, acc: 0.539062]  [A loss: 0.915918, acc: 0.101562]\n",
      "3395: [D loss: 0.699066, acc: 0.500000]  [A loss: 0.711491, acc: 0.500000]\n",
      "3396: [D loss: 0.727634, acc: 0.492188]  [A loss: 0.941623, acc: 0.082031]\n",
      "3397: [D loss: 0.704267, acc: 0.492188]  [A loss: 0.690823, acc: 0.503906]\n",
      "3398: [D loss: 0.716212, acc: 0.486328]  [A loss: 0.913604, acc: 0.121094]\n",
      "3399: [D loss: 0.690062, acc: 0.519531]  [A loss: 0.732835, acc: 0.386719]\n",
      "3400: [D loss: 0.707082, acc: 0.503906]  [A loss: 0.776267, acc: 0.269531]\n",
      "3401: [D loss: 0.712633, acc: 0.484375]  [A loss: 0.797266, acc: 0.261719]\n",
      "3402: [D loss: 0.707019, acc: 0.500000]  [A loss: 0.804314, acc: 0.261719]\n",
      "3403: [D loss: 0.699300, acc: 0.527344]  [A loss: 0.816148, acc: 0.210938]\n",
      "3404: [D loss: 0.698974, acc: 0.544922]  [A loss: 0.777209, acc: 0.273438]\n",
      "3405: [D loss: 0.702251, acc: 0.531250]  [A loss: 0.833672, acc: 0.171875]\n",
      "3406: [D loss: 0.695679, acc: 0.533203]  [A loss: 0.764841, acc: 0.335938]\n",
      "3407: [D loss: 0.695856, acc: 0.507812]  [A loss: 0.877056, acc: 0.187500]\n",
      "3408: [D loss: 0.706914, acc: 0.494141]  [A loss: 0.737491, acc: 0.414062]\n",
      "3409: [D loss: 0.705248, acc: 0.527344]  [A loss: 0.910494, acc: 0.101562]\n",
      "3410: [D loss: 0.684863, acc: 0.558594]  [A loss: 0.703635, acc: 0.496094]\n",
      "3411: [D loss: 0.696164, acc: 0.539062]  [A loss: 0.865697, acc: 0.128906]\n",
      "3412: [D loss: 0.693463, acc: 0.546875]  [A loss: 0.734802, acc: 0.425781]\n",
      "3413: [D loss: 0.701045, acc: 0.535156]  [A loss: 0.879949, acc: 0.156250]\n",
      "3414: [D loss: 0.682932, acc: 0.548828]  [A loss: 0.752512, acc: 0.355469]\n",
      "3415: [D loss: 0.707698, acc: 0.521484]  [A loss: 0.885111, acc: 0.148438]\n",
      "3416: [D loss: 0.692199, acc: 0.550781]  [A loss: 0.703356, acc: 0.523438]\n",
      "3417: [D loss: 0.701110, acc: 0.509766]  [A loss: 0.872280, acc: 0.132812]\n",
      "3418: [D loss: 0.701083, acc: 0.515625]  [A loss: 0.768281, acc: 0.367188]\n",
      "3419: [D loss: 0.722799, acc: 0.462891]  [A loss: 0.874647, acc: 0.132812]\n",
      "3420: [D loss: 0.694108, acc: 0.515625]  [A loss: 0.758394, acc: 0.335938]\n",
      "3421: [D loss: 0.723796, acc: 0.482422]  [A loss: 0.868890, acc: 0.144531]\n",
      "3422: [D loss: 0.696672, acc: 0.542969]  [A loss: 0.833410, acc: 0.179688]\n",
      "3423: [D loss: 0.701985, acc: 0.503906]  [A loss: 0.850963, acc: 0.183594]\n",
      "3424: [D loss: 0.690291, acc: 0.548828]  [A loss: 0.768784, acc: 0.320312]\n",
      "3425: [D loss: 0.686064, acc: 0.529297]  [A loss: 0.824000, acc: 0.210938]\n",
      "3426: [D loss: 0.701733, acc: 0.509766]  [A loss: 0.853054, acc: 0.230469]\n",
      "3427: [D loss: 0.686343, acc: 0.527344]  [A loss: 0.768728, acc: 0.300781]\n",
      "3428: [D loss: 0.706084, acc: 0.533203]  [A loss: 0.928279, acc: 0.097656]\n",
      "3429: [D loss: 0.712280, acc: 0.464844]  [A loss: 0.703981, acc: 0.503906]\n",
      "3430: [D loss: 0.733019, acc: 0.513672]  [A loss: 0.951330, acc: 0.101562]\n",
      "3431: [D loss: 0.701082, acc: 0.517578]  [A loss: 0.714705, acc: 0.496094]\n",
      "3432: [D loss: 0.722857, acc: 0.468750]  [A loss: 0.811460, acc: 0.207031]\n",
      "3433: [D loss: 0.692564, acc: 0.513672]  [A loss: 0.722820, acc: 0.429688]\n",
      "3434: [D loss: 0.706069, acc: 0.515625]  [A loss: 0.839238, acc: 0.167969]\n",
      "3435: [D loss: 0.676866, acc: 0.550781]  [A loss: 0.754074, acc: 0.375000]\n",
      "3436: [D loss: 0.713895, acc: 0.498047]  [A loss: 0.865211, acc: 0.167969]\n",
      "3437: [D loss: 0.695702, acc: 0.519531]  [A loss: 0.768137, acc: 0.320312]\n",
      "3438: [D loss: 0.698516, acc: 0.546875]  [A loss: 0.877411, acc: 0.160156]\n",
      "3439: [D loss: 0.714743, acc: 0.470703]  [A loss: 0.858885, acc: 0.152344]\n",
      "3440: [D loss: 0.695861, acc: 0.531250]  [A loss: 0.771470, acc: 0.320312]\n",
      "3441: [D loss: 0.708843, acc: 0.492188]  [A loss: 0.800129, acc: 0.292969]\n",
      "3442: [D loss: 0.710455, acc: 0.492188]  [A loss: 0.794276, acc: 0.273438]\n",
      "3443: [D loss: 0.690433, acc: 0.525391]  [A loss: 0.819330, acc: 0.261719]\n",
      "3444: [D loss: 0.699225, acc: 0.531250]  [A loss: 0.813309, acc: 0.218750]\n",
      "3445: [D loss: 0.704176, acc: 0.521484]  [A loss: 0.766606, acc: 0.316406]\n",
      "3446: [D loss: 0.693775, acc: 0.560547]  [A loss: 0.891249, acc: 0.113281]\n",
      "3447: [D loss: 0.687206, acc: 0.539062]  [A loss: 0.681098, acc: 0.535156]\n",
      "3448: [D loss: 0.694267, acc: 0.542969]  [A loss: 0.867663, acc: 0.183594]\n",
      "3449: [D loss: 0.685716, acc: 0.541016]  [A loss: 0.747474, acc: 0.382812]\n",
      "3450: [D loss: 0.712792, acc: 0.523438]  [A loss: 0.902950, acc: 0.164062]\n",
      "3451: [D loss: 0.682814, acc: 0.560547]  [A loss: 0.731891, acc: 0.457031]\n",
      "3452: [D loss: 0.711802, acc: 0.507812]  [A loss: 0.894682, acc: 0.136719]\n",
      "3453: [D loss: 0.698563, acc: 0.527344]  [A loss: 0.746368, acc: 0.410156]\n",
      "3454: [D loss: 0.694890, acc: 0.531250]  [A loss: 0.808476, acc: 0.257812]\n",
      "3455: [D loss: 0.699229, acc: 0.527344]  [A loss: 0.853286, acc: 0.152344]\n",
      "3456: [D loss: 0.708402, acc: 0.500000]  [A loss: 0.797470, acc: 0.308594]\n",
      "3457: [D loss: 0.697835, acc: 0.513672]  [A loss: 0.803529, acc: 0.296875]\n",
      "3458: [D loss: 0.699255, acc: 0.529297]  [A loss: 0.823034, acc: 0.214844]\n",
      "3459: [D loss: 0.714260, acc: 0.496094]  [A loss: 0.829221, acc: 0.199219]\n",
      "3460: [D loss: 0.703784, acc: 0.500000]  [A loss: 0.768470, acc: 0.347656]\n",
      "3461: [D loss: 0.706104, acc: 0.509766]  [A loss: 0.915795, acc: 0.101562]\n",
      "3462: [D loss: 0.705329, acc: 0.505859]  [A loss: 0.723475, acc: 0.496094]\n",
      "3463: [D loss: 0.731251, acc: 0.527344]  [A loss: 0.900232, acc: 0.113281]\n",
      "3464: [D loss: 0.692979, acc: 0.519531]  [A loss: 0.739736, acc: 0.375000]\n",
      "3465: [D loss: 0.710752, acc: 0.537109]  [A loss: 0.953712, acc: 0.054688]\n",
      "3466: [D loss: 0.696048, acc: 0.513672]  [A loss: 0.734183, acc: 0.457031]\n",
      "3467: [D loss: 0.712090, acc: 0.496094]  [A loss: 0.859089, acc: 0.187500]\n",
      "3468: [D loss: 0.692832, acc: 0.558594]  [A loss: 0.794509, acc: 0.277344]\n",
      "3469: [D loss: 0.694070, acc: 0.560547]  [A loss: 0.850085, acc: 0.214844]\n",
      "3470: [D loss: 0.691469, acc: 0.519531]  [A loss: 0.810876, acc: 0.230469]\n",
      "3471: [D loss: 0.696185, acc: 0.527344]  [A loss: 0.804479, acc: 0.281250]\n",
      "3472: [D loss: 0.701048, acc: 0.507812]  [A loss: 0.789035, acc: 0.296875]\n",
      "3473: [D loss: 0.694813, acc: 0.531250]  [A loss: 0.780829, acc: 0.320312]\n",
      "3474: [D loss: 0.688195, acc: 0.558594]  [A loss: 0.915334, acc: 0.125000]\n",
      "3475: [D loss: 0.695994, acc: 0.517578]  [A loss: 0.775556, acc: 0.316406]\n",
      "3476: [D loss: 0.704303, acc: 0.515625]  [A loss: 0.886679, acc: 0.203125]\n",
      "3477: [D loss: 0.707572, acc: 0.486328]  [A loss: 0.784178, acc: 0.277344]\n",
      "3478: [D loss: 0.715211, acc: 0.500000]  [A loss: 0.949080, acc: 0.089844]\n",
      "3479: [D loss: 0.699573, acc: 0.525391]  [A loss: 0.736208, acc: 0.421875]\n",
      "3480: [D loss: 0.711317, acc: 0.505859]  [A loss: 0.908116, acc: 0.125000]\n",
      "3481: [D loss: 0.698078, acc: 0.523438]  [A loss: 0.679605, acc: 0.550781]\n",
      "3482: [D loss: 0.722160, acc: 0.515625]  [A loss: 1.031992, acc: 0.062500]\n",
      "3483: [D loss: 0.704109, acc: 0.501953]  [A loss: 0.708378, acc: 0.445312]\n",
      "3484: [D loss: 0.712045, acc: 0.527344]  [A loss: 0.848953, acc: 0.195312]\n",
      "3485: [D loss: 0.702651, acc: 0.492188]  [A loss: 0.748693, acc: 0.375000]\n",
      "3486: [D loss: 0.695477, acc: 0.556641]  [A loss: 0.842896, acc: 0.171875]\n",
      "3487: [D loss: 0.698604, acc: 0.498047]  [A loss: 0.773663, acc: 0.347656]\n",
      "3488: [D loss: 0.700886, acc: 0.533203]  [A loss: 0.854438, acc: 0.195312]\n",
      "3489: [D loss: 0.685883, acc: 0.552734]  [A loss: 0.785307, acc: 0.339844]\n",
      "3490: [D loss: 0.703714, acc: 0.509766]  [A loss: 0.799212, acc: 0.308594]\n",
      "3491: [D loss: 0.702633, acc: 0.513672]  [A loss: 0.782767, acc: 0.339844]\n",
      "3492: [D loss: 0.703529, acc: 0.513672]  [A loss: 0.823713, acc: 0.273438]\n",
      "3493: [D loss: 0.699035, acc: 0.501953]  [A loss: 0.809436, acc: 0.308594]\n",
      "3494: [D loss: 0.701742, acc: 0.529297]  [A loss: 0.760362, acc: 0.355469]\n",
      "3495: [D loss: 0.710086, acc: 0.517578]  [A loss: 0.910879, acc: 0.125000]\n",
      "3496: [D loss: 0.707177, acc: 0.513672]  [A loss: 0.716622, acc: 0.441406]\n",
      "3497: [D loss: 0.696848, acc: 0.523438]  [A loss: 0.906131, acc: 0.175781]\n",
      "3498: [D loss: 0.700042, acc: 0.517578]  [A loss: 0.703546, acc: 0.527344]\n",
      "3499: [D loss: 0.717788, acc: 0.488281]  [A loss: 0.932640, acc: 0.113281]\n",
      "3500: [D loss: 0.697241, acc: 0.529297]  [A loss: 0.731385, acc: 0.433594]\n",
      "3501: [D loss: 0.713490, acc: 0.515625]  [A loss: 0.847362, acc: 0.164062]\n",
      "3502: [D loss: 0.706577, acc: 0.523438]  [A loss: 0.778657, acc: 0.316406]\n",
      "3503: [D loss: 0.713893, acc: 0.535156]  [A loss: 0.896868, acc: 0.160156]\n",
      "3504: [D loss: 0.695449, acc: 0.505859]  [A loss: 0.746718, acc: 0.394531]\n",
      "3505: [D loss: 0.702368, acc: 0.531250]  [A loss: 0.836983, acc: 0.195312]\n",
      "3506: [D loss: 0.688661, acc: 0.548828]  [A loss: 0.822990, acc: 0.246094]\n",
      "3507: [D loss: 0.697925, acc: 0.546875]  [A loss: 0.806417, acc: 0.273438]\n",
      "3508: [D loss: 0.689456, acc: 0.537109]  [A loss: 0.866811, acc: 0.199219]\n",
      "3509: [D loss: 0.695255, acc: 0.521484]  [A loss: 0.783489, acc: 0.289062]\n",
      "3510: [D loss: 0.696701, acc: 0.541016]  [A loss: 0.853198, acc: 0.187500]\n",
      "3511: [D loss: 0.703584, acc: 0.492188]  [A loss: 0.775690, acc: 0.320312]\n",
      "3512: [D loss: 0.717756, acc: 0.486328]  [A loss: 0.885755, acc: 0.152344]\n",
      "3513: [D loss: 0.708766, acc: 0.492188]  [A loss: 0.738627, acc: 0.390625]\n",
      "3514: [D loss: 0.693091, acc: 0.541016]  [A loss: 0.819494, acc: 0.226562]\n",
      "3515: [D loss: 0.700633, acc: 0.535156]  [A loss: 0.823552, acc: 0.230469]\n",
      "3516: [D loss: 0.695860, acc: 0.533203]  [A loss: 0.800237, acc: 0.296875]\n",
      "3517: [D loss: 0.709118, acc: 0.505859]  [A loss: 0.783830, acc: 0.320312]\n",
      "3518: [D loss: 0.707722, acc: 0.509766]  [A loss: 0.911343, acc: 0.089844]\n",
      "3519: [D loss: 0.699463, acc: 0.494141]  [A loss: 0.735058, acc: 0.425781]\n",
      "3520: [D loss: 0.712923, acc: 0.515625]  [A loss: 0.868787, acc: 0.164062]\n",
      "3521: [D loss: 0.682430, acc: 0.558594]  [A loss: 0.803284, acc: 0.277344]\n",
      "3522: [D loss: 0.701876, acc: 0.500000]  [A loss: 0.842134, acc: 0.199219]\n",
      "3523: [D loss: 0.710940, acc: 0.490234]  [A loss: 1.019388, acc: 0.062500]\n",
      "3524: [D loss: 0.701768, acc: 0.503906]  [A loss: 0.684052, acc: 0.539062]\n",
      "3525: [D loss: 0.731556, acc: 0.503906]  [A loss: 0.883457, acc: 0.164062]\n",
      "3526: [D loss: 0.688322, acc: 0.552734]  [A loss: 0.743397, acc: 0.378906]\n",
      "3527: [D loss: 0.711390, acc: 0.513672]  [A loss: 0.792251, acc: 0.257812]\n",
      "3528: [D loss: 0.695891, acc: 0.517578]  [A loss: 0.817199, acc: 0.214844]\n",
      "3529: [D loss: 0.698002, acc: 0.521484]  [A loss: 0.768744, acc: 0.386719]\n",
      "3530: [D loss: 0.709634, acc: 0.507812]  [A loss: 0.866327, acc: 0.191406]\n",
      "3531: [D loss: 0.699974, acc: 0.492188]  [A loss: 0.832272, acc: 0.230469]\n",
      "3532: [D loss: 0.700882, acc: 0.517578]  [A loss: 0.830672, acc: 0.234375]\n",
      "3533: [D loss: 0.699897, acc: 0.531250]  [A loss: 0.771178, acc: 0.324219]\n",
      "3534: [D loss: 0.702232, acc: 0.535156]  [A loss: 0.852894, acc: 0.195312]\n",
      "3535: [D loss: 0.693100, acc: 0.525391]  [A loss: 0.737248, acc: 0.390625]\n",
      "3536: [D loss: 0.719752, acc: 0.500000]  [A loss: 0.838654, acc: 0.234375]\n",
      "3537: [D loss: 0.701013, acc: 0.529297]  [A loss: 0.881755, acc: 0.156250]\n",
      "3538: [D loss: 0.701666, acc: 0.507812]  [A loss: 0.770994, acc: 0.332031]\n",
      "3539: [D loss: 0.699027, acc: 0.507812]  [A loss: 0.882455, acc: 0.140625]\n",
      "3540: [D loss: 0.705812, acc: 0.501953]  [A loss: 0.797752, acc: 0.273438]\n",
      "3541: [D loss: 0.715288, acc: 0.503906]  [A loss: 0.887408, acc: 0.171875]\n",
      "3542: [D loss: 0.699784, acc: 0.501953]  [A loss: 0.766681, acc: 0.351562]\n",
      "3543: [D loss: 0.697359, acc: 0.519531]  [A loss: 0.918978, acc: 0.128906]\n",
      "3544: [D loss: 0.696404, acc: 0.519531]  [A loss: 0.790829, acc: 0.277344]\n",
      "3545: [D loss: 0.707850, acc: 0.486328]  [A loss: 0.904481, acc: 0.140625]\n",
      "3546: [D loss: 0.719356, acc: 0.480469]  [A loss: 0.747034, acc: 0.367188]\n",
      "3547: [D loss: 0.692791, acc: 0.556641]  [A loss: 0.826156, acc: 0.246094]\n",
      "3548: [D loss: 0.699560, acc: 0.507812]  [A loss: 0.802146, acc: 0.257812]\n",
      "3549: [D loss: 0.703430, acc: 0.515625]  [A loss: 0.805841, acc: 0.234375]\n",
      "3550: [D loss: 0.712180, acc: 0.486328]  [A loss: 0.943561, acc: 0.085938]\n",
      "3551: [D loss: 0.692366, acc: 0.525391]  [A loss: 0.700446, acc: 0.492188]\n",
      "3552: [D loss: 0.712847, acc: 0.509766]  [A loss: 0.962666, acc: 0.062500]\n",
      "3553: [D loss: 0.690466, acc: 0.542969]  [A loss: 0.709311, acc: 0.527344]\n",
      "3554: [D loss: 0.718717, acc: 0.490234]  [A loss: 0.897487, acc: 0.121094]\n",
      "3555: [D loss: 0.699040, acc: 0.515625]  [A loss: 0.818078, acc: 0.238281]\n",
      "3556: [D loss: 0.699675, acc: 0.517578]  [A loss: 0.855787, acc: 0.210938]\n",
      "3557: [D loss: 0.700128, acc: 0.507812]  [A loss: 0.773019, acc: 0.347656]\n",
      "3558: [D loss: 0.694322, acc: 0.525391]  [A loss: 0.862872, acc: 0.167969]\n",
      "3559: [D loss: 0.680344, acc: 0.587891]  [A loss: 0.765019, acc: 0.367188]\n",
      "3560: [D loss: 0.706216, acc: 0.501953]  [A loss: 0.856204, acc: 0.183594]\n",
      "3561: [D loss: 0.700801, acc: 0.484375]  [A loss: 0.803702, acc: 0.273438]\n",
      "3562: [D loss: 0.700780, acc: 0.535156]  [A loss: 0.894260, acc: 0.148438]\n",
      "3563: [D loss: 0.684975, acc: 0.552734]  [A loss: 0.739826, acc: 0.441406]\n",
      "3564: [D loss: 0.716014, acc: 0.503906]  [A loss: 0.874012, acc: 0.132812]\n",
      "3565: [D loss: 0.697386, acc: 0.511719]  [A loss: 0.782575, acc: 0.316406]\n",
      "3566: [D loss: 0.714489, acc: 0.482422]  [A loss: 0.935380, acc: 0.097656]\n",
      "3567: [D loss: 0.697340, acc: 0.521484]  [A loss: 0.726464, acc: 0.429688]\n",
      "3568: [D loss: 0.725507, acc: 0.490234]  [A loss: 0.900747, acc: 0.105469]\n",
      "3569: [D loss: 0.698976, acc: 0.509766]  [A loss: 0.785263, acc: 0.281250]\n",
      "3570: [D loss: 0.707736, acc: 0.521484]  [A loss: 0.934162, acc: 0.097656]\n",
      "3571: [D loss: 0.700382, acc: 0.505859]  [A loss: 0.692866, acc: 0.531250]\n",
      "3572: [D loss: 0.725916, acc: 0.482422]  [A loss: 0.912625, acc: 0.128906]\n",
      "3573: [D loss: 0.694279, acc: 0.529297]  [A loss: 0.702529, acc: 0.488281]\n",
      "3574: [D loss: 0.723299, acc: 0.498047]  [A loss: 0.949303, acc: 0.089844]\n",
      "3575: [D loss: 0.700028, acc: 0.492188]  [A loss: 0.734326, acc: 0.414062]\n",
      "3576: [D loss: 0.712488, acc: 0.511719]  [A loss: 0.824564, acc: 0.203125]\n",
      "3577: [D loss: 0.705770, acc: 0.486328]  [A loss: 0.775857, acc: 0.335938]\n",
      "3578: [D loss: 0.717167, acc: 0.492188]  [A loss: 0.820450, acc: 0.250000]\n",
      "3579: [D loss: 0.704190, acc: 0.496094]  [A loss: 0.762868, acc: 0.355469]\n",
      "3580: [D loss: 0.704775, acc: 0.480469]  [A loss: 0.827133, acc: 0.199219]\n",
      "3581: [D loss: 0.702817, acc: 0.507812]  [A loss: 0.804080, acc: 0.238281]\n",
      "3582: [D loss: 0.703470, acc: 0.511719]  [A loss: 0.835148, acc: 0.199219]\n",
      "3583: [D loss: 0.702608, acc: 0.513672]  [A loss: 0.830505, acc: 0.210938]\n",
      "3584: [D loss: 0.701434, acc: 0.523438]  [A loss: 0.812215, acc: 0.238281]\n",
      "3585: [D loss: 0.694570, acc: 0.535156]  [A loss: 0.787097, acc: 0.339844]\n",
      "3586: [D loss: 0.700501, acc: 0.537109]  [A loss: 0.843678, acc: 0.214844]\n",
      "3587: [D loss: 0.698416, acc: 0.519531]  [A loss: 0.785493, acc: 0.300781]\n",
      "3588: [D loss: 0.703825, acc: 0.517578]  [A loss: 0.944247, acc: 0.074219]\n",
      "3589: [D loss: 0.693001, acc: 0.541016]  [A loss: 0.717372, acc: 0.492188]\n",
      "3590: [D loss: 0.725806, acc: 0.492188]  [A loss: 0.921387, acc: 0.109375]\n",
      "3591: [D loss: 0.692264, acc: 0.552734]  [A loss: 0.772969, acc: 0.339844]\n",
      "3592: [D loss: 0.705541, acc: 0.513672]  [A loss: 0.857151, acc: 0.160156]\n",
      "3593: [D loss: 0.689782, acc: 0.517578]  [A loss: 0.724843, acc: 0.460938]\n",
      "3594: [D loss: 0.707660, acc: 0.515625]  [A loss: 0.917036, acc: 0.121094]\n",
      "3595: [D loss: 0.694732, acc: 0.511719]  [A loss: 0.743368, acc: 0.441406]\n",
      "3596: [D loss: 0.697424, acc: 0.531250]  [A loss: 0.861749, acc: 0.164062]\n",
      "3597: [D loss: 0.697261, acc: 0.511719]  [A loss: 0.853755, acc: 0.191406]\n",
      "3598: [D loss: 0.710287, acc: 0.505859]  [A loss: 0.782237, acc: 0.316406]\n",
      "3599: [D loss: 0.701062, acc: 0.517578]  [A loss: 0.818451, acc: 0.246094]\n",
      "3600: [D loss: 0.708739, acc: 0.515625]  [A loss: 0.860165, acc: 0.160156]\n",
      "3601: [D loss: 0.698453, acc: 0.507812]  [A loss: 0.715265, acc: 0.480469]\n",
      "3602: [D loss: 0.709213, acc: 0.519531]  [A loss: 0.913366, acc: 0.156250]\n",
      "3603: [D loss: 0.692224, acc: 0.513672]  [A loss: 0.775939, acc: 0.312500]\n",
      "3604: [D loss: 0.711227, acc: 0.509766]  [A loss: 0.906177, acc: 0.117188]\n",
      "3605: [D loss: 0.691620, acc: 0.525391]  [A loss: 0.753731, acc: 0.367188]\n",
      "3606: [D loss: 0.712504, acc: 0.531250]  [A loss: 0.944867, acc: 0.117188]\n",
      "3607: [D loss: 0.709928, acc: 0.490234]  [A loss: 0.751716, acc: 0.386719]\n",
      "3608: [D loss: 0.710920, acc: 0.500000]  [A loss: 0.913526, acc: 0.144531]\n",
      "3609: [D loss: 0.685915, acc: 0.562500]  [A loss: 0.774440, acc: 0.304688]\n",
      "3610: [D loss: 0.688038, acc: 0.537109]  [A loss: 0.791678, acc: 0.300781]\n",
      "3611: [D loss: 0.706477, acc: 0.498047]  [A loss: 0.850963, acc: 0.207031]\n",
      "3612: [D loss: 0.700485, acc: 0.521484]  [A loss: 0.728342, acc: 0.460938]\n",
      "3613: [D loss: 0.710248, acc: 0.494141]  [A loss: 0.901988, acc: 0.152344]\n",
      "3614: [D loss: 0.689410, acc: 0.548828]  [A loss: 0.692699, acc: 0.542969]\n",
      "3615: [D loss: 0.713173, acc: 0.507812]  [A loss: 0.890682, acc: 0.128906]\n",
      "3616: [D loss: 0.685908, acc: 0.558594]  [A loss: 0.758385, acc: 0.375000]\n",
      "3617: [D loss: 0.713924, acc: 0.503906]  [A loss: 0.886034, acc: 0.148438]\n",
      "3618: [D loss: 0.696137, acc: 0.517578]  [A loss: 0.715928, acc: 0.484375]\n",
      "3619: [D loss: 0.714230, acc: 0.503906]  [A loss: 0.928167, acc: 0.121094]\n",
      "3620: [D loss: 0.719591, acc: 0.472656]  [A loss: 0.770970, acc: 0.347656]\n",
      "3621: [D loss: 0.710649, acc: 0.501953]  [A loss: 0.844237, acc: 0.195312]\n",
      "3622: [D loss: 0.693321, acc: 0.535156]  [A loss: 0.802161, acc: 0.289062]\n",
      "3623: [D loss: 0.711739, acc: 0.500000]  [A loss: 0.830804, acc: 0.210938]\n",
      "3624: [D loss: 0.709769, acc: 0.486328]  [A loss: 0.835887, acc: 0.191406]\n",
      "3625: [D loss: 0.682221, acc: 0.589844]  [A loss: 0.740713, acc: 0.429688]\n",
      "3626: [D loss: 0.695570, acc: 0.523438]  [A loss: 0.826129, acc: 0.234375]\n",
      "3627: [D loss: 0.707853, acc: 0.503906]  [A loss: 0.797433, acc: 0.308594]\n",
      "3628: [D loss: 0.701985, acc: 0.529297]  [A loss: 0.791808, acc: 0.281250]\n",
      "3629: [D loss: 0.697622, acc: 0.523438]  [A loss: 0.867475, acc: 0.183594]\n",
      "3630: [D loss: 0.698936, acc: 0.542969]  [A loss: 0.875363, acc: 0.175781]\n",
      "3631: [D loss: 0.690261, acc: 0.535156]  [A loss: 0.750981, acc: 0.390625]\n",
      "3632: [D loss: 0.715100, acc: 0.525391]  [A loss: 0.942564, acc: 0.105469]\n",
      "3633: [D loss: 0.704512, acc: 0.496094]  [A loss: 0.763377, acc: 0.367188]\n",
      "3634: [D loss: 0.696183, acc: 0.523438]  [A loss: 0.931747, acc: 0.101562]\n",
      "3635: [D loss: 0.682488, acc: 0.564453]  [A loss: 0.689114, acc: 0.550781]\n",
      "3636: [D loss: 0.712185, acc: 0.509766]  [A loss: 0.963357, acc: 0.109375]\n",
      "3637: [D loss: 0.698708, acc: 0.531250]  [A loss: 0.739556, acc: 0.355469]\n",
      "3638: [D loss: 0.708635, acc: 0.525391]  [A loss: 0.906358, acc: 0.160156]\n",
      "3639: [D loss: 0.709119, acc: 0.486328]  [A loss: 0.755417, acc: 0.375000]\n",
      "3640: [D loss: 0.708362, acc: 0.535156]  [A loss: 0.905187, acc: 0.144531]\n",
      "3641: [D loss: 0.689943, acc: 0.531250]  [A loss: 0.770823, acc: 0.363281]\n",
      "3642: [D loss: 0.711121, acc: 0.517578]  [A loss: 0.878832, acc: 0.156250]\n",
      "3643: [D loss: 0.694619, acc: 0.533203]  [A loss: 0.721469, acc: 0.457031]\n",
      "3644: [D loss: 0.717237, acc: 0.490234]  [A loss: 0.896576, acc: 0.136719]\n",
      "3645: [D loss: 0.695556, acc: 0.537109]  [A loss: 0.725261, acc: 0.453125]\n",
      "3646: [D loss: 0.692442, acc: 0.546875]  [A loss: 0.803323, acc: 0.261719]\n",
      "3647: [D loss: 0.692241, acc: 0.544922]  [A loss: 0.847987, acc: 0.203125]\n",
      "3648: [D loss: 0.702731, acc: 0.480469]  [A loss: 0.777396, acc: 0.332031]\n",
      "3649: [D loss: 0.707633, acc: 0.501953]  [A loss: 0.916400, acc: 0.125000]\n",
      "3650: [D loss: 0.697778, acc: 0.503906]  [A loss: 0.732687, acc: 0.421875]\n",
      "3651: [D loss: 0.727382, acc: 0.509766]  [A loss: 0.912993, acc: 0.148438]\n",
      "3652: [D loss: 0.696187, acc: 0.531250]  [A loss: 0.789968, acc: 0.335938]\n",
      "3653: [D loss: 0.712127, acc: 0.509766]  [A loss: 0.859725, acc: 0.195312]\n",
      "3654: [D loss: 0.697299, acc: 0.535156]  [A loss: 0.830530, acc: 0.183594]\n",
      "3655: [D loss: 0.694880, acc: 0.525391]  [A loss: 0.795920, acc: 0.304688]\n",
      "3656: [D loss: 0.711483, acc: 0.505859]  [A loss: 0.838088, acc: 0.218750]\n",
      "3657: [D loss: 0.692530, acc: 0.527344]  [A loss: 0.782814, acc: 0.339844]\n",
      "3658: [D loss: 0.707488, acc: 0.505859]  [A loss: 0.873382, acc: 0.199219]\n",
      "3659: [D loss: 0.703428, acc: 0.523438]  [A loss: 0.772724, acc: 0.347656]\n",
      "3660: [D loss: 0.708060, acc: 0.511719]  [A loss: 0.851242, acc: 0.210938]\n",
      "3661: [D loss: 0.683626, acc: 0.539062]  [A loss: 0.747756, acc: 0.390625]\n",
      "3662: [D loss: 0.714918, acc: 0.486328]  [A loss: 0.918348, acc: 0.136719]\n",
      "3663: [D loss: 0.693411, acc: 0.546875]  [A loss: 0.690557, acc: 0.496094]\n",
      "3664: [D loss: 0.715759, acc: 0.498047]  [A loss: 0.930478, acc: 0.152344]\n",
      "3665: [D loss: 0.698936, acc: 0.501953]  [A loss: 0.737473, acc: 0.433594]\n",
      "3666: [D loss: 0.699466, acc: 0.537109]  [A loss: 0.870930, acc: 0.167969]\n",
      "3667: [D loss: 0.707399, acc: 0.501953]  [A loss: 0.794033, acc: 0.285156]\n",
      "3668: [D loss: 0.694976, acc: 0.539062]  [A loss: 0.822557, acc: 0.253906]\n",
      "3669: [D loss: 0.711040, acc: 0.501953]  [A loss: 0.828048, acc: 0.253906]\n",
      "3670: [D loss: 0.700347, acc: 0.539062]  [A loss: 0.829324, acc: 0.250000]\n",
      "3671: [D loss: 0.712828, acc: 0.482422]  [A loss: 0.909895, acc: 0.148438]\n",
      "3672: [D loss: 0.700119, acc: 0.529297]  [A loss: 0.751749, acc: 0.386719]\n",
      "3673: [D loss: 0.727248, acc: 0.488281]  [A loss: 0.961169, acc: 0.082031]\n",
      "3674: [D loss: 0.695344, acc: 0.544922]  [A loss: 0.674426, acc: 0.546875]\n",
      "3675: [D loss: 0.730104, acc: 0.517578]  [A loss: 0.870967, acc: 0.136719]\n",
      "3676: [D loss: 0.693624, acc: 0.533203]  [A loss: 0.768660, acc: 0.375000]\n",
      "3677: [D loss: 0.706192, acc: 0.505859]  [A loss: 0.805477, acc: 0.292969]\n",
      "3678: [D loss: 0.706701, acc: 0.490234]  [A loss: 0.818217, acc: 0.261719]\n",
      "3679: [D loss: 0.697678, acc: 0.533203]  [A loss: 0.792392, acc: 0.289062]\n",
      "3680: [D loss: 0.699014, acc: 0.537109]  [A loss: 0.834294, acc: 0.238281]\n",
      "3681: [D loss: 0.696167, acc: 0.541016]  [A loss: 0.764489, acc: 0.406250]\n",
      "3682: [D loss: 0.727214, acc: 0.484375]  [A loss: 0.912016, acc: 0.136719]\n",
      "3683: [D loss: 0.690946, acc: 0.544922]  [A loss: 0.747047, acc: 0.410156]\n",
      "3684: [D loss: 0.720382, acc: 0.519531]  [A loss: 1.025322, acc: 0.066406]\n",
      "3685: [D loss: 0.713244, acc: 0.498047]  [A loss: 0.781884, acc: 0.343750]\n",
      "3686: [D loss: 0.704322, acc: 0.505859]  [A loss: 0.814104, acc: 0.246094]\n",
      "3687: [D loss: 0.703360, acc: 0.501953]  [A loss: 0.761006, acc: 0.363281]\n",
      "3688: [D loss: 0.707071, acc: 0.527344]  [A loss: 0.881580, acc: 0.156250]\n",
      "3689: [D loss: 0.694356, acc: 0.556641]  [A loss: 0.782143, acc: 0.312500]\n",
      "3690: [D loss: 0.703286, acc: 0.511719]  [A loss: 0.894800, acc: 0.140625]\n",
      "3691: [D loss: 0.696208, acc: 0.537109]  [A loss: 0.776237, acc: 0.347656]\n",
      "3692: [D loss: 0.711578, acc: 0.468750]  [A loss: 0.884016, acc: 0.156250]\n",
      "3693: [D loss: 0.704287, acc: 0.509766]  [A loss: 0.718818, acc: 0.460938]\n",
      "3694: [D loss: 0.697697, acc: 0.531250]  [A loss: 0.897654, acc: 0.078125]\n",
      "3695: [D loss: 0.693053, acc: 0.550781]  [A loss: 0.745681, acc: 0.421875]\n",
      "3696: [D loss: 0.710583, acc: 0.511719]  [A loss: 0.940056, acc: 0.097656]\n",
      "3697: [D loss: 0.697559, acc: 0.527344]  [A loss: 0.793388, acc: 0.269531]\n",
      "3698: [D loss: 0.700957, acc: 0.509766]  [A loss: 0.886681, acc: 0.187500]\n",
      "3699: [D loss: 0.704900, acc: 0.496094]  [A loss: 0.740382, acc: 0.402344]\n",
      "3700: [D loss: 0.696679, acc: 0.533203]  [A loss: 0.827771, acc: 0.261719]\n",
      "3701: [D loss: 0.701741, acc: 0.511719]  [A loss: 0.822921, acc: 0.253906]\n",
      "3702: [D loss: 0.707871, acc: 0.494141]  [A loss: 0.870951, acc: 0.187500]\n",
      "3703: [D loss: 0.690489, acc: 0.542969]  [A loss: 0.754146, acc: 0.414062]\n",
      "3704: [D loss: 0.718485, acc: 0.527344]  [A loss: 0.890537, acc: 0.125000]\n",
      "3705: [D loss: 0.699811, acc: 0.525391]  [A loss: 0.756070, acc: 0.363281]\n",
      "3706: [D loss: 0.703348, acc: 0.519531]  [A loss: 0.867706, acc: 0.136719]\n",
      "3707: [D loss: 0.694578, acc: 0.535156]  [A loss: 0.797038, acc: 0.324219]\n",
      "3708: [D loss: 0.711555, acc: 0.492188]  [A loss: 0.844175, acc: 0.195312]\n",
      "3709: [D loss: 0.705818, acc: 0.490234]  [A loss: 0.835367, acc: 0.187500]\n",
      "3710: [D loss: 0.696657, acc: 0.537109]  [A loss: 0.871789, acc: 0.183594]\n",
      "3711: [D loss: 0.692703, acc: 0.548828]  [A loss: 0.790015, acc: 0.281250]\n",
      "3712: [D loss: 0.694651, acc: 0.529297]  [A loss: 0.868741, acc: 0.207031]\n",
      "3713: [D loss: 0.696496, acc: 0.527344]  [A loss: 0.758157, acc: 0.386719]\n",
      "3714: [D loss: 0.711187, acc: 0.509766]  [A loss: 0.930688, acc: 0.105469]\n",
      "3715: [D loss: 0.701565, acc: 0.507812]  [A loss: 0.722851, acc: 0.441406]\n",
      "3716: [D loss: 0.728923, acc: 0.494141]  [A loss: 0.953411, acc: 0.093750]\n",
      "3717: [D loss: 0.710428, acc: 0.517578]  [A loss: 0.741174, acc: 0.433594]\n",
      "3718: [D loss: 0.707903, acc: 0.542969]  [A loss: 0.893394, acc: 0.171875]\n",
      "3719: [D loss: 0.704628, acc: 0.525391]  [A loss: 0.816113, acc: 0.238281]\n",
      "3720: [D loss: 0.697349, acc: 0.525391]  [A loss: 0.815055, acc: 0.257812]\n",
      "3721: [D loss: 0.708780, acc: 0.505859]  [A loss: 0.814442, acc: 0.265625]\n",
      "3722: [D loss: 0.694341, acc: 0.546875]  [A loss: 0.809190, acc: 0.292969]\n",
      "3723: [D loss: 0.697705, acc: 0.533203]  [A loss: 0.858272, acc: 0.203125]\n",
      "3724: [D loss: 0.713323, acc: 0.468750]  [A loss: 0.689840, acc: 0.531250]\n",
      "3725: [D loss: 0.738200, acc: 0.500000]  [A loss: 1.069019, acc: 0.027344]\n",
      "3726: [D loss: 0.685302, acc: 0.572266]  [A loss: 0.701853, acc: 0.527344]\n",
      "3727: [D loss: 0.712857, acc: 0.541016]  [A loss: 0.825014, acc: 0.246094]\n",
      "3728: [D loss: 0.702894, acc: 0.509766]  [A loss: 0.777483, acc: 0.296875]\n",
      "3729: [D loss: 0.704320, acc: 0.521484]  [A loss: 0.850195, acc: 0.214844]\n",
      "3730: [D loss: 0.706624, acc: 0.490234]  [A loss: 0.778747, acc: 0.320312]\n",
      "3731: [D loss: 0.711881, acc: 0.511719]  [A loss: 0.838841, acc: 0.199219]\n",
      "3732: [D loss: 0.689934, acc: 0.562500]  [A loss: 0.804795, acc: 0.273438]\n",
      "3733: [D loss: 0.679617, acc: 0.550781]  [A loss: 0.791718, acc: 0.300781]\n",
      "3734: [D loss: 0.704677, acc: 0.509766]  [A loss: 0.827563, acc: 0.226562]\n",
      "3735: [D loss: 0.700673, acc: 0.511719]  [A loss: 0.775312, acc: 0.355469]\n",
      "3736: [D loss: 0.708777, acc: 0.511719]  [A loss: 0.906312, acc: 0.148438]\n",
      "3737: [D loss: 0.700805, acc: 0.509766]  [A loss: 0.752942, acc: 0.355469]\n",
      "3738: [D loss: 0.705241, acc: 0.509766]  [A loss: 0.815129, acc: 0.242188]\n",
      "3739: [D loss: 0.703222, acc: 0.527344]  [A loss: 0.875884, acc: 0.187500]\n",
      "3740: [D loss: 0.701577, acc: 0.505859]  [A loss: 0.768449, acc: 0.339844]\n",
      "3741: [D loss: 0.703656, acc: 0.496094]  [A loss: 0.889359, acc: 0.171875]\n",
      "3742: [D loss: 0.704349, acc: 0.503906]  [A loss: 0.735174, acc: 0.429688]\n",
      "3743: [D loss: 0.706551, acc: 0.496094]  [A loss: 0.886112, acc: 0.183594]\n",
      "3744: [D loss: 0.707286, acc: 0.494141]  [A loss: 0.762734, acc: 0.367188]\n",
      "3745: [D loss: 0.727043, acc: 0.490234]  [A loss: 0.905207, acc: 0.105469]\n",
      "3746: [D loss: 0.692567, acc: 0.521484]  [A loss: 0.699807, acc: 0.523438]\n",
      "3747: [D loss: 0.721805, acc: 0.513672]  [A loss: 1.013851, acc: 0.066406]\n",
      "3748: [D loss: 0.706299, acc: 0.509766]  [A loss: 0.716864, acc: 0.460938]\n",
      "3749: [D loss: 0.713871, acc: 0.509766]  [A loss: 0.844906, acc: 0.179688]\n",
      "3750: [D loss: 0.714284, acc: 0.447266]  [A loss: 0.830468, acc: 0.218750]\n",
      "3751: [D loss: 0.704302, acc: 0.507812]  [A loss: 0.839955, acc: 0.246094]\n",
      "3752: [D loss: 0.709612, acc: 0.509766]  [A loss: 0.842586, acc: 0.199219]\n",
      "3753: [D loss: 0.702967, acc: 0.525391]  [A loss: 0.805595, acc: 0.265625]\n",
      "3754: [D loss: 0.695328, acc: 0.509766]  [A loss: 0.825163, acc: 0.230469]\n",
      "3755: [D loss: 0.702318, acc: 0.494141]  [A loss: 0.849654, acc: 0.191406]\n",
      "3756: [D loss: 0.701303, acc: 0.523438]  [A loss: 0.830245, acc: 0.218750]\n",
      "3757: [D loss: 0.717588, acc: 0.503906]  [A loss: 0.855293, acc: 0.195312]\n",
      "3758: [D loss: 0.700281, acc: 0.539062]  [A loss: 0.755571, acc: 0.398438]\n",
      "3759: [D loss: 0.699067, acc: 0.544922]  [A loss: 0.897920, acc: 0.121094]\n",
      "3760: [D loss: 0.696770, acc: 0.525391]  [A loss: 0.766854, acc: 0.359375]\n",
      "3761: [D loss: 0.698761, acc: 0.525391]  [A loss: 0.844223, acc: 0.210938]\n",
      "3762: [D loss: 0.720243, acc: 0.490234]  [A loss: 0.809180, acc: 0.281250]\n",
      "3763: [D loss: 0.703303, acc: 0.513672]  [A loss: 0.837419, acc: 0.250000]\n",
      "3764: [D loss: 0.704702, acc: 0.529297]  [A loss: 0.848106, acc: 0.222656]\n",
      "3765: [D loss: 0.704257, acc: 0.486328]  [A loss: 0.798628, acc: 0.289062]\n",
      "3766: [D loss: 0.698501, acc: 0.527344]  [A loss: 0.831582, acc: 0.246094]\n",
      "3767: [D loss: 0.720610, acc: 0.490234]  [A loss: 0.807748, acc: 0.257812]\n",
      "3768: [D loss: 0.718805, acc: 0.498047]  [A loss: 1.012086, acc: 0.062500]\n",
      "3769: [D loss: 0.715946, acc: 0.490234]  [A loss: 0.638507, acc: 0.640625]\n",
      "3770: [D loss: 0.738389, acc: 0.496094]  [A loss: 0.921902, acc: 0.125000]\n",
      "3771: [D loss: 0.696418, acc: 0.517578]  [A loss: 0.729937, acc: 0.425781]\n",
      "3772: [D loss: 0.718468, acc: 0.537109]  [A loss: 0.858478, acc: 0.175781]\n",
      "3773: [D loss: 0.701095, acc: 0.511719]  [A loss: 0.816868, acc: 0.273438]\n",
      "3774: [D loss: 0.699528, acc: 0.550781]  [A loss: 0.833422, acc: 0.191406]\n",
      "3775: [D loss: 0.704895, acc: 0.509766]  [A loss: 0.838392, acc: 0.253906]\n",
      "3776: [D loss: 0.702481, acc: 0.521484]  [A loss: 0.853860, acc: 0.207031]\n",
      "3777: [D loss: 0.702718, acc: 0.519531]  [A loss: 0.777530, acc: 0.355469]\n",
      "3778: [D loss: 0.696139, acc: 0.537109]  [A loss: 0.948260, acc: 0.140625]\n",
      "3779: [D loss: 0.705233, acc: 0.498047]  [A loss: 0.757645, acc: 0.355469]\n",
      "3780: [D loss: 0.711757, acc: 0.521484]  [A loss: 0.828076, acc: 0.230469]\n",
      "3781: [D loss: 0.700183, acc: 0.492188]  [A loss: 0.834574, acc: 0.207031]\n",
      "3782: [D loss: 0.698031, acc: 0.539062]  [A loss: 0.852074, acc: 0.218750]\n",
      "3783: [D loss: 0.707135, acc: 0.486328]  [A loss: 0.862100, acc: 0.191406]\n",
      "3784: [D loss: 0.700709, acc: 0.511719]  [A loss: 0.775140, acc: 0.351562]\n",
      "3785: [D loss: 0.705453, acc: 0.537109]  [A loss: 0.835166, acc: 0.226562]\n",
      "3786: [D loss: 0.699310, acc: 0.513672]  [A loss: 0.804991, acc: 0.273438]\n",
      "3787: [D loss: 0.698882, acc: 0.527344]  [A loss: 0.788419, acc: 0.316406]\n",
      "3788: [D loss: 0.703226, acc: 0.513672]  [A loss: 0.938915, acc: 0.117188]\n",
      "3789: [D loss: 0.697379, acc: 0.519531]  [A loss: 0.698518, acc: 0.515625]\n",
      "3790: [D loss: 0.725896, acc: 0.490234]  [A loss: 0.973479, acc: 0.082031]\n",
      "3791: [D loss: 0.704792, acc: 0.509766]  [A loss: 0.742932, acc: 0.394531]\n",
      "3792: [D loss: 0.717035, acc: 0.494141]  [A loss: 0.915093, acc: 0.109375]\n",
      "3793: [D loss: 0.701775, acc: 0.531250]  [A loss: 0.754685, acc: 0.378906]\n",
      "3794: [D loss: 0.719972, acc: 0.490234]  [A loss: 0.896158, acc: 0.156250]\n",
      "3795: [D loss: 0.698514, acc: 0.515625]  [A loss: 0.733433, acc: 0.382812]\n",
      "3796: [D loss: 0.717044, acc: 0.509766]  [A loss: 0.837888, acc: 0.199219]\n",
      "3797: [D loss: 0.706722, acc: 0.507812]  [A loss: 0.844519, acc: 0.203125]\n",
      "3798: [D loss: 0.698990, acc: 0.525391]  [A loss: 0.737720, acc: 0.425781]\n",
      "3799: [D loss: 0.708032, acc: 0.521484]  [A loss: 0.902321, acc: 0.144531]\n",
      "3800: [D loss: 0.693125, acc: 0.515625]  [A loss: 0.781796, acc: 0.332031]\n",
      "3801: [D loss: 0.697263, acc: 0.529297]  [A loss: 0.873930, acc: 0.191406]\n",
      "3802: [D loss: 0.698751, acc: 0.517578]  [A loss: 0.769718, acc: 0.324219]\n",
      "3803: [D loss: 0.709882, acc: 0.507812]  [A loss: 0.945592, acc: 0.117188]\n",
      "3804: [D loss: 0.712942, acc: 0.498047]  [A loss: 0.711993, acc: 0.457031]\n",
      "3805: [D loss: 0.718946, acc: 0.503906]  [A loss: 0.938893, acc: 0.128906]\n",
      "3806: [D loss: 0.694543, acc: 0.542969]  [A loss: 0.738490, acc: 0.441406]\n",
      "3807: [D loss: 0.712293, acc: 0.505859]  [A loss: 0.871090, acc: 0.191406]\n",
      "3808: [D loss: 0.696275, acc: 0.535156]  [A loss: 0.778380, acc: 0.320312]\n",
      "3809: [D loss: 0.709523, acc: 0.503906]  [A loss: 0.841313, acc: 0.242188]\n",
      "3810: [D loss: 0.709620, acc: 0.525391]  [A loss: 0.841191, acc: 0.195312]\n",
      "3811: [D loss: 0.707837, acc: 0.519531]  [A loss: 0.810652, acc: 0.269531]\n",
      "3812: [D loss: 0.693937, acc: 0.533203]  [A loss: 0.854909, acc: 0.191406]\n",
      "3813: [D loss: 0.693611, acc: 0.517578]  [A loss: 0.811891, acc: 0.234375]\n",
      "3814: [D loss: 0.705773, acc: 0.523438]  [A loss: 0.916175, acc: 0.132812]\n",
      "3815: [D loss: 0.689024, acc: 0.544922]  [A loss: 0.811544, acc: 0.277344]\n",
      "3816: [D loss: 0.696783, acc: 0.535156]  [A loss: 0.898113, acc: 0.152344]\n",
      "3817: [D loss: 0.713440, acc: 0.525391]  [A loss: 0.813094, acc: 0.289062]\n",
      "3818: [D loss: 0.707290, acc: 0.501953]  [A loss: 0.850987, acc: 0.183594]\n",
      "3819: [D loss: 0.694380, acc: 0.513672]  [A loss: 0.757115, acc: 0.335938]\n",
      "3820: [D loss: 0.702756, acc: 0.529297]  [A loss: 0.865876, acc: 0.210938]\n",
      "3821: [D loss: 0.700989, acc: 0.517578]  [A loss: 0.802721, acc: 0.269531]\n",
      "3822: [D loss: 0.708885, acc: 0.509766]  [A loss: 0.945567, acc: 0.113281]\n",
      "3823: [D loss: 0.698961, acc: 0.533203]  [A loss: 0.657596, acc: 0.613281]\n",
      "3824: [D loss: 0.752641, acc: 0.462891]  [A loss: 0.879709, acc: 0.183594]\n",
      "3825: [D loss: 0.703364, acc: 0.527344]  [A loss: 0.909707, acc: 0.152344]\n",
      "3826: [D loss: 0.713807, acc: 0.500000]  [A loss: 0.772460, acc: 0.335938]\n",
      "3827: [D loss: 0.720334, acc: 0.484375]  [A loss: 0.945056, acc: 0.105469]\n",
      "3828: [D loss: 0.701142, acc: 0.507812]  [A loss: 0.688709, acc: 0.570312]\n",
      "3829: [D loss: 0.718236, acc: 0.521484]  [A loss: 0.938299, acc: 0.128906]\n",
      "3830: [D loss: 0.711508, acc: 0.490234]  [A loss: 0.722552, acc: 0.425781]\n",
      "3831: [D loss: 0.723642, acc: 0.519531]  [A loss: 0.904256, acc: 0.152344]\n",
      "3832: [D loss: 0.699542, acc: 0.527344]  [A loss: 0.724168, acc: 0.472656]\n",
      "3833: [D loss: 0.703605, acc: 0.533203]  [A loss: 0.873428, acc: 0.171875]\n",
      "3834: [D loss: 0.706874, acc: 0.496094]  [A loss: 0.760679, acc: 0.351562]\n",
      "3835: [D loss: 0.695640, acc: 0.544922]  [A loss: 0.776530, acc: 0.308594]\n",
      "3836: [D loss: 0.707153, acc: 0.501953]  [A loss: 0.850834, acc: 0.152344]\n",
      "3837: [D loss: 0.697852, acc: 0.513672]  [A loss: 0.823699, acc: 0.199219]\n",
      "3838: [D loss: 0.698520, acc: 0.513672]  [A loss: 0.852458, acc: 0.187500]\n",
      "3839: [D loss: 0.698341, acc: 0.531250]  [A loss: 0.804121, acc: 0.261719]\n",
      "3840: [D loss: 0.703463, acc: 0.535156]  [A loss: 0.770649, acc: 0.335938]\n",
      "3841: [D loss: 0.704545, acc: 0.537109]  [A loss: 0.902641, acc: 0.117188]\n",
      "3842: [D loss: 0.696421, acc: 0.513672]  [A loss: 0.825668, acc: 0.257812]\n",
      "3843: [D loss: 0.694803, acc: 0.539062]  [A loss: 0.813801, acc: 0.277344]\n",
      "3844: [D loss: 0.701364, acc: 0.513672]  [A loss: 0.815057, acc: 0.257812]\n",
      "3845: [D loss: 0.691976, acc: 0.556641]  [A loss: 0.857164, acc: 0.199219]\n",
      "3846: [D loss: 0.693375, acc: 0.541016]  [A loss: 0.801080, acc: 0.296875]\n",
      "3847: [D loss: 0.694429, acc: 0.521484]  [A loss: 0.882151, acc: 0.191406]\n",
      "3848: [D loss: 0.710391, acc: 0.503906]  [A loss: 0.772598, acc: 0.347656]\n",
      "3849: [D loss: 0.690321, acc: 0.523438]  [A loss: 0.896019, acc: 0.148438]\n",
      "3850: [D loss: 0.705737, acc: 0.494141]  [A loss: 0.784677, acc: 0.304688]\n",
      "3851: [D loss: 0.707847, acc: 0.519531]  [A loss: 0.843383, acc: 0.207031]\n",
      "3852: [D loss: 0.693513, acc: 0.541016]  [A loss: 0.840078, acc: 0.210938]\n",
      "3853: [D loss: 0.684459, acc: 0.554688]  [A loss: 0.763158, acc: 0.355469]\n",
      "3854: [D loss: 0.711626, acc: 0.519531]  [A loss: 0.982415, acc: 0.093750]\n",
      "3855: [D loss: 0.702322, acc: 0.517578]  [A loss: 0.686188, acc: 0.546875]\n",
      "3856: [D loss: 0.724347, acc: 0.500000]  [A loss: 1.048490, acc: 0.062500]\n",
      "3857: [D loss: 0.699788, acc: 0.527344]  [A loss: 0.670678, acc: 0.578125]\n",
      "3858: [D loss: 0.721263, acc: 0.501953]  [A loss: 0.821072, acc: 0.265625]\n",
      "3859: [D loss: 0.709700, acc: 0.507812]  [A loss: 0.872020, acc: 0.210938]\n",
      "3860: [D loss: 0.696890, acc: 0.519531]  [A loss: 0.802776, acc: 0.269531]\n",
      "3861: [D loss: 0.702705, acc: 0.519531]  [A loss: 0.883299, acc: 0.167969]\n",
      "3862: [D loss: 0.701817, acc: 0.511719]  [A loss: 0.820174, acc: 0.265625]\n",
      "3863: [D loss: 0.693408, acc: 0.537109]  [A loss: 0.845991, acc: 0.238281]\n",
      "3864: [D loss: 0.707406, acc: 0.505859]  [A loss: 0.802106, acc: 0.257812]\n",
      "3865: [D loss: 0.702305, acc: 0.527344]  [A loss: 0.825406, acc: 0.226562]\n",
      "3866: [D loss: 0.707498, acc: 0.503906]  [A loss: 0.799931, acc: 0.257812]\n",
      "3867: [D loss: 0.704793, acc: 0.490234]  [A loss: 0.845591, acc: 0.210938]\n",
      "3868: [D loss: 0.694945, acc: 0.542969]  [A loss: 0.722969, acc: 0.429688]\n",
      "3869: [D loss: 0.717601, acc: 0.521484]  [A loss: 0.935496, acc: 0.105469]\n",
      "3870: [D loss: 0.721497, acc: 0.466797]  [A loss: 0.705692, acc: 0.468750]\n",
      "3871: [D loss: 0.716012, acc: 0.501953]  [A loss: 0.949610, acc: 0.078125]\n",
      "3872: [D loss: 0.713284, acc: 0.470703]  [A loss: 0.779912, acc: 0.300781]\n",
      "3873: [D loss: 0.698424, acc: 0.525391]  [A loss: 0.820203, acc: 0.238281]\n",
      "3874: [D loss: 0.701915, acc: 0.517578]  [A loss: 0.770129, acc: 0.382812]\n",
      "3875: [D loss: 0.703954, acc: 0.515625]  [A loss: 0.879371, acc: 0.156250]\n",
      "3876: [D loss: 0.707593, acc: 0.513672]  [A loss: 0.770915, acc: 0.347656]\n",
      "3877: [D loss: 0.703790, acc: 0.505859]  [A loss: 0.873239, acc: 0.152344]\n",
      "3878: [D loss: 0.682953, acc: 0.552734]  [A loss: 0.700914, acc: 0.511719]\n",
      "3879: [D loss: 0.729399, acc: 0.501953]  [A loss: 1.041942, acc: 0.054688]\n",
      "3880: [D loss: 0.705455, acc: 0.507812]  [A loss: 0.687904, acc: 0.566406]\n",
      "3881: [D loss: 0.752534, acc: 0.500000]  [A loss: 0.870358, acc: 0.152344]\n",
      "3882: [D loss: 0.703287, acc: 0.523438]  [A loss: 0.794965, acc: 0.261719]\n",
      "3883: [D loss: 0.691044, acc: 0.537109]  [A loss: 0.839624, acc: 0.183594]\n",
      "3884: [D loss: 0.699797, acc: 0.529297]  [A loss: 0.807542, acc: 0.273438]\n",
      "3885: [D loss: 0.715735, acc: 0.464844]  [A loss: 0.848577, acc: 0.164062]\n",
      "3886: [D loss: 0.703578, acc: 0.511719]  [A loss: 0.779577, acc: 0.320312]\n",
      "3887: [D loss: 0.695836, acc: 0.539062]  [A loss: 0.828084, acc: 0.199219]\n",
      "3888: [D loss: 0.686963, acc: 0.542969]  [A loss: 0.821900, acc: 0.250000]\n",
      "3889: [D loss: 0.698434, acc: 0.535156]  [A loss: 0.833305, acc: 0.238281]\n",
      "3890: [D loss: 0.698420, acc: 0.517578]  [A loss: 0.776066, acc: 0.292969]\n",
      "3891: [D loss: 0.709071, acc: 0.496094]  [A loss: 0.761966, acc: 0.375000]\n",
      "3892: [D loss: 0.699714, acc: 0.527344]  [A loss: 0.838821, acc: 0.199219]\n",
      "3893: [D loss: 0.693393, acc: 0.533203]  [A loss: 0.821977, acc: 0.210938]\n",
      "3894: [D loss: 0.690140, acc: 0.558594]  [A loss: 0.860088, acc: 0.195312]\n",
      "3895: [D loss: 0.691988, acc: 0.544922]  [A loss: 0.786833, acc: 0.277344]\n",
      "3896: [D loss: 0.699995, acc: 0.521484]  [A loss: 0.890206, acc: 0.160156]\n",
      "3897: [D loss: 0.696174, acc: 0.519531]  [A loss: 0.719333, acc: 0.472656]\n",
      "3898: [D loss: 0.713578, acc: 0.500000]  [A loss: 0.937198, acc: 0.085938]\n",
      "3899: [D loss: 0.700061, acc: 0.501953]  [A loss: 0.640463, acc: 0.648438]\n",
      "3900: [D loss: 0.730726, acc: 0.509766]  [A loss: 0.915263, acc: 0.101562]\n",
      "3901: [D loss: 0.699509, acc: 0.507812]  [A loss: 0.750324, acc: 0.386719]\n",
      "3902: [D loss: 0.712243, acc: 0.509766]  [A loss: 0.903054, acc: 0.132812]\n",
      "3903: [D loss: 0.714413, acc: 0.490234]  [A loss: 0.702503, acc: 0.484375]\n",
      "3904: [D loss: 0.695568, acc: 0.529297]  [A loss: 0.777647, acc: 0.308594]\n",
      "3905: [D loss: 0.716050, acc: 0.480469]  [A loss: 0.844965, acc: 0.171875]\n",
      "3906: [D loss: 0.687038, acc: 0.517578]  [A loss: 0.707842, acc: 0.476562]\n",
      "3907: [D loss: 0.713147, acc: 0.492188]  [A loss: 0.872102, acc: 0.167969]\n",
      "3908: [D loss: 0.695714, acc: 0.503906]  [A loss: 0.776406, acc: 0.324219]\n",
      "3909: [D loss: 0.704351, acc: 0.527344]  [A loss: 0.906850, acc: 0.125000]\n",
      "3910: [D loss: 0.693749, acc: 0.513672]  [A loss: 0.738280, acc: 0.394531]\n",
      "3911: [D loss: 0.706818, acc: 0.486328]  [A loss: 0.899494, acc: 0.152344]\n",
      "3912: [D loss: 0.702680, acc: 0.496094]  [A loss: 0.756200, acc: 0.386719]\n",
      "3913: [D loss: 0.707736, acc: 0.527344]  [A loss: 0.781551, acc: 0.339844]\n",
      "3914: [D loss: 0.702941, acc: 0.513672]  [A loss: 0.821929, acc: 0.234375]\n",
      "3915: [D loss: 0.710723, acc: 0.529297]  [A loss: 0.893392, acc: 0.156250]\n",
      "3916: [D loss: 0.697236, acc: 0.529297]  [A loss: 0.787615, acc: 0.289062]\n",
      "3917: [D loss: 0.710201, acc: 0.498047]  [A loss: 0.885642, acc: 0.187500]\n",
      "3918: [D loss: 0.693517, acc: 0.535156]  [A loss: 0.806015, acc: 0.308594]\n",
      "3919: [D loss: 0.711928, acc: 0.513672]  [A loss: 0.902680, acc: 0.140625]\n",
      "3920: [D loss: 0.715240, acc: 0.466797]  [A loss: 0.693499, acc: 0.515625]\n",
      "3921: [D loss: 0.736671, acc: 0.490234]  [A loss: 0.940661, acc: 0.074219]\n",
      "3922: [D loss: 0.703100, acc: 0.507812]  [A loss: 0.682548, acc: 0.554688]\n",
      "3923: [D loss: 0.711751, acc: 0.523438]  [A loss: 0.871914, acc: 0.160156]\n",
      "3924: [D loss: 0.700461, acc: 0.500000]  [A loss: 0.748931, acc: 0.402344]\n",
      "3925: [D loss: 0.709230, acc: 0.515625]  [A loss: 0.793800, acc: 0.265625]\n",
      "3926: [D loss: 0.703015, acc: 0.503906]  [A loss: 0.819639, acc: 0.234375]\n",
      "3927: [D loss: 0.695688, acc: 0.501953]  [A loss: 0.745123, acc: 0.414062]\n",
      "3928: [D loss: 0.699784, acc: 0.542969]  [A loss: 0.872810, acc: 0.195312]\n",
      "3929: [D loss: 0.699489, acc: 0.500000]  [A loss: 0.724823, acc: 0.417969]\n",
      "3930: [D loss: 0.714323, acc: 0.523438]  [A loss: 0.909171, acc: 0.109375]\n",
      "3931: [D loss: 0.693453, acc: 0.511719]  [A loss: 0.750127, acc: 0.375000]\n",
      "3932: [D loss: 0.707457, acc: 0.531250]  [A loss: 0.856088, acc: 0.148438]\n",
      "3933: [D loss: 0.688382, acc: 0.576172]  [A loss: 0.804955, acc: 0.296875]\n",
      "3934: [D loss: 0.697508, acc: 0.544922]  [A loss: 0.826076, acc: 0.195312]\n",
      "3935: [D loss: 0.691338, acc: 0.523438]  [A loss: 0.791605, acc: 0.273438]\n",
      "3936: [D loss: 0.703208, acc: 0.527344]  [A loss: 0.879837, acc: 0.136719]\n",
      "3937: [D loss: 0.694189, acc: 0.529297]  [A loss: 0.722062, acc: 0.437500]\n",
      "3938: [D loss: 0.715571, acc: 0.507812]  [A loss: 0.969565, acc: 0.062500]\n",
      "3939: [D loss: 0.717624, acc: 0.490234]  [A loss: 0.859897, acc: 0.128906]\n",
      "3940: [D loss: 0.709183, acc: 0.503906]  [A loss: 0.828220, acc: 0.183594]\n",
      "3941: [D loss: 0.705024, acc: 0.496094]  [A loss: 0.836311, acc: 0.179688]\n",
      "3942: [D loss: 0.696611, acc: 0.521484]  [A loss: 0.843876, acc: 0.199219]\n",
      "3943: [D loss: 0.699509, acc: 0.511719]  [A loss: 0.795815, acc: 0.265625]\n",
      "3944: [D loss: 0.702750, acc: 0.513672]  [A loss: 0.872943, acc: 0.179688]\n",
      "3945: [D loss: 0.700544, acc: 0.533203]  [A loss: 0.834361, acc: 0.230469]\n",
      "3946: [D loss: 0.696395, acc: 0.529297]  [A loss: 0.739079, acc: 0.437500]\n",
      "3947: [D loss: 0.709822, acc: 0.507812]  [A loss: 0.861777, acc: 0.156250]\n",
      "3948: [D loss: 0.697784, acc: 0.503906]  [A loss: 0.784897, acc: 0.324219]\n",
      "3949: [D loss: 0.706831, acc: 0.509766]  [A loss: 0.913225, acc: 0.132812]\n",
      "3950: [D loss: 0.698405, acc: 0.503906]  [A loss: 0.772530, acc: 0.308594]\n",
      "3951: [D loss: 0.710646, acc: 0.505859]  [A loss: 0.946441, acc: 0.109375]\n",
      "3952: [D loss: 0.694526, acc: 0.533203]  [A loss: 0.691609, acc: 0.519531]\n",
      "3953: [D loss: 0.731235, acc: 0.476562]  [A loss: 0.964698, acc: 0.085938]\n",
      "3954: [D loss: 0.706788, acc: 0.488281]  [A loss: 0.681295, acc: 0.546875]\n",
      "3955: [D loss: 0.722374, acc: 0.511719]  [A loss: 0.965318, acc: 0.066406]\n",
      "3956: [D loss: 0.700772, acc: 0.542969]  [A loss: 0.736614, acc: 0.449219]\n",
      "3957: [D loss: 0.731180, acc: 0.476562]  [A loss: 0.884895, acc: 0.144531]\n",
      "3958: [D loss: 0.703915, acc: 0.517578]  [A loss: 0.772806, acc: 0.347656]\n",
      "3959: [D loss: 0.700970, acc: 0.517578]  [A loss: 0.791280, acc: 0.285156]\n",
      "3960: [D loss: 0.702167, acc: 0.521484]  [A loss: 0.759301, acc: 0.347656]\n",
      "3961: [D loss: 0.703177, acc: 0.505859]  [A loss: 0.833706, acc: 0.191406]\n",
      "3962: [D loss: 0.690022, acc: 0.544922]  [A loss: 0.736309, acc: 0.414062]\n",
      "3963: [D loss: 0.714930, acc: 0.507812]  [A loss: 0.913570, acc: 0.097656]\n",
      "3964: [D loss: 0.688401, acc: 0.556641]  [A loss: 0.691622, acc: 0.535156]\n",
      "3965: [D loss: 0.704881, acc: 0.525391]  [A loss: 0.888265, acc: 0.125000]\n",
      "3966: [D loss: 0.683448, acc: 0.535156]  [A loss: 0.690908, acc: 0.523438]\n",
      "3967: [D loss: 0.723486, acc: 0.509766]  [A loss: 0.991148, acc: 0.066406]\n",
      "3968: [D loss: 0.694839, acc: 0.546875]  [A loss: 0.692434, acc: 0.535156]\n",
      "3969: [D loss: 0.726150, acc: 0.496094]  [A loss: 0.860303, acc: 0.175781]\n",
      "3970: [D loss: 0.704508, acc: 0.519531]  [A loss: 0.763841, acc: 0.343750]\n",
      "3971: [D loss: 0.699418, acc: 0.552734]  [A loss: 0.769894, acc: 0.332031]\n",
      "3972: [D loss: 0.706731, acc: 0.523438]  [A loss: 0.872820, acc: 0.203125]\n",
      "3973: [D loss: 0.706222, acc: 0.507812]  [A loss: 0.729262, acc: 0.449219]\n",
      "3974: [D loss: 0.695135, acc: 0.521484]  [A loss: 0.878788, acc: 0.175781]\n",
      "3975: [D loss: 0.704473, acc: 0.505859]  [A loss: 0.796421, acc: 0.285156]\n",
      "3976: [D loss: 0.707859, acc: 0.509766]  [A loss: 0.908809, acc: 0.105469]\n",
      "3977: [D loss: 0.699733, acc: 0.480469]  [A loss: 0.785955, acc: 0.328125]\n",
      "3978: [D loss: 0.713159, acc: 0.501953]  [A loss: 0.848571, acc: 0.207031]\n",
      "3979: [D loss: 0.694912, acc: 0.527344]  [A loss: 0.770749, acc: 0.371094]\n",
      "3980: [D loss: 0.700469, acc: 0.515625]  [A loss: 0.841629, acc: 0.210938]\n",
      "3981: [D loss: 0.704101, acc: 0.513672]  [A loss: 0.773762, acc: 0.328125]\n",
      "3982: [D loss: 0.702064, acc: 0.476562]  [A loss: 0.860845, acc: 0.171875]\n",
      "3983: [D loss: 0.705610, acc: 0.507812]  [A loss: 0.780277, acc: 0.312500]\n",
      "3984: [D loss: 0.684958, acc: 0.552734]  [A loss: 0.835916, acc: 0.203125]\n",
      "3985: [D loss: 0.695655, acc: 0.527344]  [A loss: 0.743708, acc: 0.390625]\n",
      "3986: [D loss: 0.713149, acc: 0.517578]  [A loss: 0.931013, acc: 0.117188]\n",
      "3987: [D loss: 0.705400, acc: 0.498047]  [A loss: 0.814753, acc: 0.242188]\n",
      "3988: [D loss: 0.698038, acc: 0.501953]  [A loss: 0.835847, acc: 0.187500]\n",
      "3989: [D loss: 0.687367, acc: 0.548828]  [A loss: 0.808384, acc: 0.273438]\n",
      "3990: [D loss: 0.696950, acc: 0.539062]  [A loss: 0.778257, acc: 0.320312]\n",
      "3991: [D loss: 0.714591, acc: 0.480469]  [A loss: 0.893938, acc: 0.171875]\n",
      "3992: [D loss: 0.712499, acc: 0.501953]  [A loss: 0.754235, acc: 0.332031]\n",
      "3993: [D loss: 0.703696, acc: 0.500000]  [A loss: 0.947127, acc: 0.113281]\n",
      "3994: [D loss: 0.708928, acc: 0.500000]  [A loss: 0.779851, acc: 0.371094]\n",
      "3995: [D loss: 0.705751, acc: 0.521484]  [A loss: 0.941456, acc: 0.085938]\n",
      "3996: [D loss: 0.700463, acc: 0.525391]  [A loss: 0.734105, acc: 0.425781]\n",
      "3997: [D loss: 0.710044, acc: 0.515625]  [A loss: 0.965313, acc: 0.105469]\n",
      "3998: [D loss: 0.701694, acc: 0.517578]  [A loss: 0.703888, acc: 0.488281]\n",
      "3999: [D loss: 0.720780, acc: 0.482422]  [A loss: 0.907230, acc: 0.148438]\n"
     ]
    }
   ],
   "source": [
    "dcgan_mnist = DCGAN_MNIST(X_train_keras)\n",
    "dcgan_mnist.train(train_steps=4000, batch_size=256, save_interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1441
    },
    "colab_type": "code",
    "id": "hBKNvIOI24BO",
    "outputId": "cab997dc-c1e3-4fca-8956-fb70314018a0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1QM5UnYCsk4WhQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+Q94It+q9z37eQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate batch of synthetic MNIST images\n",
    "dcgan_mnist.plot_images(fake=True)\n",
    "dcgan_mnist.plot_images(fake=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rf35qibpB2ur"
   },
   "outputs": [],
   "source": [
    "class DCGAN_MNIST(object):\n",
    "    \n",
    "    def __init__(self, x_train):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "\n",
    "        self.x_train = x_train\n",
    "        \n",
    "        # discriminator\n",
    "        self.D = self.discriminator()\n",
    "        # generator\n",
    "        self.G = self.generator()\n",
    "        # discriminator model\n",
    "        self.DM = self.discriminator_model()\n",
    "        # adversarial model (discrimnator stacked on top of generator)\n",
    "        self.AM = self.adversarial_model() \n",
    "    \n",
    "    \n",
    "    def generator(self, depth=256, dim=7, dropout=0.3, momentum=0.8,\n",
    "                  window=5, input_dim=100, output_depth=1): \n",
    "        gen = Sequential()\n",
    "        gen.add(Dense(dim*dim*depth, input_dim=input_dim))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "        gen.add(Reshape((dim, dim, depth)))\n",
    "        gen.add(Dropout(dropout))\n",
    "        \n",
    "        gen.add(UpSampling2D())\n",
    "        gen.add(Conv2DTranspose(int(depth/2), window, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "        gen.add(UpSampling2D())\n",
    "        gen.add(Conv2DTranspose(int(depth/4), window, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "        gen.add(Conv2DTranspose(int(depth/8), window, padding='same'))\n",
    "        gen.add(BatchNormalization(momentum=momentum))\n",
    "        gen.add(Activation('relu'))\n",
    "\n",
    "        gen.add(Conv2DTranspose(output_depth, window, padding='same'))\n",
    "        gen.add(Activation('sigmoid'))\n",
    "        gen.summary()\n",
    "        return gen\n",
    "\n",
    "    def discriminator(self, depth=64, dropout=0.3, alpha=0.3): \n",
    "        dis = Sequential()\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        dis.add(Conv2D(depth*1, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "        dis.add(LeakyReLU(alpha=alpha))\n",
    "        dis.add(Dropout(dropout))\n",
    "\n",
    "        dis.add(Flatten())\n",
    "        dis.add(Dense(1))\n",
    "        dis.add(Activation('sigmoid'))\n",
    "        dis.summary()\n",
    "        return dis\n",
    "\n",
    "    def discriminator_model(self):\n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        DM = Sequential()\n",
    "        DM.add(self.D)\n",
    "        DM.compile(loss='binary_crossentropy',\n",
    "                   optimizer=optimizer, metrics=['accuracy'])\n",
    "        return DM\n",
    "\n",
    "    def adversarial_model(self):\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        AM = Sequential()\n",
    "        AM.add(self.G)\n",
    "        # Set discriminator weights to non-trainable\n",
    "        # (will only apply to the `gan` model)\n",
    "        # self.D.trainable = False\n",
    "        AM.add(self.D)\n",
    "        AM.compile(loss='binary_crossentropy',\n",
    "                   optimizer=optimizer, metrics=['accuracy'])\n",
    "        return AM\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        \n",
    "        # loop over epochs\n",
    "        for i in range(train_steps):\n",
    "            # get random REAL training samples \n",
    "            images_train = self.x_train[np.random.randint(0, self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            # noise vector as input for generator\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            # generate fake images\n",
    "            images_fake = self.G.predict(noise)\n",
    "            # labeled sample contains real and fake images\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            # build label vector (real=1, fake=0)\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            # add random noise to the labels - important trick!\n",
    "            y += 0.05 * np.random.random(y.shape)\n",
    "            \n",
    "            # train discriminator with real and fake images\n",
    "            # keras train_on_batch runs a single gradient update on a single batch of data\n",
    "            # returns loss and accuracy in list of scalars\n",
    "            d_loss = self.DM.train_on_batch(x, y)\n",
    "            \n",
    "            # train adversial network (generator+discriminator) with fake images\n",
    "            # NOTE: here fake images are labelled with y=1, since lose corresponds to fake image detected as fake\n",
    "            # generator does well if fake image declared as real in output of adversial network\n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.AM.train_on_batch(noise, y)\n",
    "            \n",
    "            # track training progress in log message\n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            \n",
    "            # print log messages\n",
    "            plot_interval = 100\n",
    "            if i%plot_interval==0:\n",
    "                print(log_mesg)\n",
    "            \n",
    "            # plot and save fake sample during training every interval steps\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(save2file=True, \n",
    "                        samples=noise_input.shape[0],\n",
    "                        noise=noise_input, step=(i+1))\n",
    "    \n",
    "    # PLOT FAKE IMAGES\n",
    "    def plot_images(self, save2file=False, fake=True, samples=16,\n",
    "                    noise=None, step=0):\n",
    "        \n",
    "        # location for storing fake images \n",
    "        current_path = os.getcwd()\n",
    "        file = '/drive/My Drive/Colab Notebooks/images/synthetic_mnist/'\n",
    "        filename = 'mnist.png'\n",
    "        \n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist2_%d.png\" % step\n",
    "            images = self.G.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(current_path+file+filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3077
    },
    "colab_type": "code",
    "id": "5iHtp3JbB28D",
    "outputId": "25c33105-febb-49f4-9066-f487bbdbf2ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,311,553\n",
      "Trainable params: 4,311,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_21 (Conv2DT (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_22 (Conv2DT (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_23 (Conv2DT (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DT (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.695335, acc: 0.000000]  [A loss: 1.029078, acc: 0.000000]\n",
      "100: [D loss: 0.499919, acc: 0.000000]  [A loss: 0.713011, acc: 0.480469]\n",
      "200: [D loss: 0.660553, acc: 0.000000]  [A loss: 1.952305, acc: 0.000000]\n",
      "300: [D loss: 0.707303, acc: 0.000000]  [A loss: 1.203618, acc: 0.003906]\n",
      "400: [D loss: 0.701098, acc: 0.000000]  [A loss: 0.965545, acc: 0.023438]\n",
      "500: [D loss: 0.672768, acc: 0.000000]  [A loss: 0.686357, acc: 0.558594]\n",
      "600: [D loss: 0.671862, acc: 0.000000]  [A loss: 0.706705, acc: 0.511719]\n",
      "700: [D loss: 0.699274, acc: 0.000000]  [A loss: 0.936000, acc: 0.042969]\n",
      "800: [D loss: 0.685494, acc: 0.000000]  [A loss: 0.831938, acc: 0.199219]\n",
      "900: [D loss: 0.685738, acc: 0.000000]  [A loss: 0.786895, acc: 0.250000]\n",
      "1000: [D loss: 0.860013, acc: 0.000000]  [A loss: 0.919418, acc: 0.089844]\n",
      "1100: [D loss: 0.710187, acc: 0.000000]  [A loss: 0.908402, acc: 0.089844]\n",
      "1200: [D loss: 0.670729, acc: 0.000000]  [A loss: 0.724532, acc: 0.437500]\n",
      "1300: [D loss: 0.683561, acc: 0.000000]  [A loss: 0.771417, acc: 0.312500]\n",
      "1400: [D loss: 0.698769, acc: 0.000000]  [A loss: 0.802149, acc: 0.269531]\n",
      "1500: [D loss: 0.680175, acc: 0.000000]  [A loss: 0.890744, acc: 0.195312]\n",
      "1600: [D loss: 0.698067, acc: 0.000000]  [A loss: 0.830310, acc: 0.199219]\n",
      "1700: [D loss: 0.698863, acc: 0.000000]  [A loss: 0.811665, acc: 0.285156]\n",
      "1800: [D loss: 0.686729, acc: 0.000000]  [A loss: 0.806610, acc: 0.269531]\n",
      "1900: [D loss: 0.693246, acc: 0.000000]  [A loss: 0.724785, acc: 0.457031]\n",
      "2000: [D loss: 0.697385, acc: 0.000000]  [A loss: 0.848045, acc: 0.199219]\n",
      "2100: [D loss: 0.699886, acc: 0.000000]  [A loss: 0.884773, acc: 0.144531]\n",
      "2200: [D loss: 0.689524, acc: 0.000000]  [A loss: 0.813427, acc: 0.265625]\n",
      "2300: [D loss: 0.697517, acc: 0.000000]  [A loss: 0.639089, acc: 0.683594]\n",
      "2400: [D loss: 0.694627, acc: 0.000000]  [A loss: 0.570063, acc: 0.804688]\n",
      "2500: [D loss: 0.699974, acc: 0.000000]  [A loss: 0.734335, acc: 0.406250]\n",
      "2600: [D loss: 0.706530, acc: 0.000000]  [A loss: 0.826587, acc: 0.230469]\n",
      "2700: [D loss: 0.683670, acc: 0.000000]  [A loss: 0.619114, acc: 0.722656]\n",
      "2800: [D loss: 0.706918, acc: 0.000000]  [A loss: 0.712469, acc: 0.453125]\n",
      "2900: [D loss: 0.691710, acc: 0.000000]  [A loss: 0.737111, acc: 0.437500]\n",
      "3000: [D loss: 0.710339, acc: 0.000000]  [A loss: 0.897174, acc: 0.199219]\n",
      "3100: [D loss: 0.706277, acc: 0.000000]  [A loss: 0.691084, acc: 0.550781]\n",
      "3200: [D loss: 0.689851, acc: 0.000000]  [A loss: 0.749735, acc: 0.378906]\n",
      "3300: [D loss: 0.706878, acc: 0.000000]  [A loss: 0.768384, acc: 0.320312]\n",
      "3400: [D loss: 0.716335, acc: 0.000000]  [A loss: 0.913406, acc: 0.132812]\n",
      "3500: [D loss: 0.742067, acc: 0.000000]  [A loss: 0.861267, acc: 0.195312]\n",
      "3600: [D loss: 0.703644, acc: 0.000000]  [A loss: 0.724125, acc: 0.445312]\n",
      "3700: [D loss: 0.694278, acc: 0.000000]  [A loss: 0.781560, acc: 0.335938]\n",
      "3800: [D loss: 0.699780, acc: 0.000000]  [A loss: 0.876332, acc: 0.199219]\n",
      "3900: [D loss: 0.722833, acc: 0.000000]  [A loss: 0.821245, acc: 0.242188]\n",
      "4000: [D loss: 0.709375, acc: 0.000000]  [A loss: 0.781273, acc: 0.316406]\n",
      "4100: [D loss: 0.694498, acc: 0.000000]  [A loss: 0.752891, acc: 0.371094]\n",
      "4200: [D loss: 0.695126, acc: 0.000000]  [A loss: 0.693457, acc: 0.566406]\n",
      "4300: [D loss: 0.689476, acc: 0.000000]  [A loss: 0.566795, acc: 0.812500]\n",
      "4400: [D loss: 0.691530, acc: 0.000000]  [A loss: 0.622078, acc: 0.710938]\n",
      "4500: [D loss: 0.694080, acc: 0.000000]  [A loss: 0.769753, acc: 0.320312]\n",
      "4600: [D loss: 0.696177, acc: 0.000000]  [A loss: 0.656617, acc: 0.644531]\n",
      "4700: [D loss: 0.697755, acc: 0.000000]  [A loss: 0.746021, acc: 0.406250]\n",
      "4800: [D loss: 0.694957, acc: 0.000000]  [A loss: 0.721595, acc: 0.425781]\n",
      "4900: [D loss: 0.704266, acc: 0.000000]  [A loss: 0.838228, acc: 0.203125]\n",
      "5000: [D loss: 0.692294, acc: 0.000000]  [A loss: 0.691559, acc: 0.496094]\n",
      "5100: [D loss: 0.698952, acc: 0.000000]  [A loss: 0.703337, acc: 0.464844]\n",
      "5200: [D loss: 0.695537, acc: 0.000000]  [A loss: 0.635887, acc: 0.710938]\n",
      "5300: [D loss: 0.689852, acc: 0.000000]  [A loss: 0.733711, acc: 0.402344]\n",
      "5400: [D loss: 0.701856, acc: 0.000000]  [A loss: 0.742715, acc: 0.425781]\n",
      "5500: [D loss: 0.695168, acc: 0.000000]  [A loss: 0.701921, acc: 0.496094]\n",
      "5600: [D loss: 0.707333, acc: 0.000000]  [A loss: 0.760890, acc: 0.363281]\n",
      "5700: [D loss: 0.709000, acc: 0.000000]  [A loss: 0.691665, acc: 0.496094]\n",
      "5800: [D loss: 0.713733, acc: 0.000000]  [A loss: 0.905284, acc: 0.078125]\n",
      "5900: [D loss: 0.707937, acc: 0.000000]  [A loss: 0.735251, acc: 0.414062]\n",
      "6000: [D loss: 0.708234, acc: 0.000000]  [A loss: 0.836461, acc: 0.253906]\n",
      "6100: [D loss: 0.713141, acc: 0.000000]  [A loss: 0.848189, acc: 0.214844]\n",
      "6200: [D loss: 0.709096, acc: 0.000000]  [A loss: 0.663391, acc: 0.617188]\n",
      "6300: [D loss: 0.696611, acc: 0.000000]  [A loss: 0.695505, acc: 0.503906]\n",
      "6400: [D loss: 0.693336, acc: 0.000000]  [A loss: 0.703583, acc: 0.468750]\n",
      "6500: [D loss: 0.692857, acc: 0.000000]  [A loss: 0.712429, acc: 0.460938]\n",
      "6600: [D loss: 0.709607, acc: 0.000000]  [A loss: 0.790034, acc: 0.265625]\n",
      "6700: [D loss: 0.713656, acc: 0.000000]  [A loss: 0.809803, acc: 0.246094]\n",
      "6800: [D loss: 0.722036, acc: 0.000000]  [A loss: 0.870031, acc: 0.171875]\n",
      "6900: [D loss: 0.687085, acc: 0.000000]  [A loss: 0.729483, acc: 0.414062]\n",
      "7000: [D loss: 0.705304, acc: 0.000000]  [A loss: 0.722495, acc: 0.460938]\n",
      "7100: [D loss: 0.708556, acc: 0.000000]  [A loss: 0.677147, acc: 0.566406]\n",
      "7200: [D loss: 0.698758, acc: 0.000000]  [A loss: 0.693641, acc: 0.531250]\n",
      "7300: [D loss: 0.707150, acc: 0.000000]  [A loss: 0.672620, acc: 0.562500]\n",
      "7400: [D loss: 0.699340, acc: 0.000000]  [A loss: 0.664592, acc: 0.621094]\n",
      "7500: [D loss: 0.703040, acc: 0.000000]  [A loss: 0.714544, acc: 0.480469]\n",
      "7600: [D loss: 0.698178, acc: 0.000000]  [A loss: 0.637857, acc: 0.656250]\n",
      "7700: [D loss: 0.704433, acc: 0.000000]  [A loss: 0.686946, acc: 0.539062]\n",
      "7800: [D loss: 0.691935, acc: 0.000000]  [A loss: 0.735393, acc: 0.468750]\n",
      "7900: [D loss: 0.705809, acc: 0.000000]  [A loss: 0.777844, acc: 0.296875]\n",
      "8000: [D loss: 0.708467, acc: 0.000000]  [A loss: 0.785129, acc: 0.289062]\n",
      "8100: [D loss: 0.710455, acc: 0.000000]  [A loss: 0.705472, acc: 0.488281]\n",
      "8200: [D loss: 0.723277, acc: 0.000000]  [A loss: 0.787216, acc: 0.281250]\n",
      "8300: [D loss: 0.693229, acc: 0.000000]  [A loss: 0.720454, acc: 0.433594]\n",
      "8400: [D loss: 0.685647, acc: 0.000000]  [A loss: 0.671819, acc: 0.585938]\n",
      "8500: [D loss: 0.700868, acc: 0.000000]  [A loss: 0.745793, acc: 0.359375]\n",
      "8600: [D loss: 0.715194, acc: 0.000000]  [A loss: 0.788682, acc: 0.308594]\n",
      "8700: [D loss: 0.707736, acc: 0.000000]  [A loss: 0.763864, acc: 0.390625]\n",
      "8800: [D loss: 0.693919, acc: 0.000000]  [A loss: 0.762683, acc: 0.382812]\n",
      "8900: [D loss: 0.698628, acc: 0.000000]  [A loss: 0.661548, acc: 0.628906]\n",
      "9000: [D loss: 0.718792, acc: 0.000000]  [A loss: 0.916266, acc: 0.128906]\n",
      "9100: [D loss: 0.708886, acc: 0.000000]  [A loss: 0.689721, acc: 0.570312]\n",
      "9200: [D loss: 0.695560, acc: 0.000000]  [A loss: 0.765720, acc: 0.355469]\n",
      "9300: [D loss: 0.713211, acc: 0.000000]  [A loss: 0.727282, acc: 0.449219]\n",
      "9400: [D loss: 0.698796, acc: 0.000000]  [A loss: 0.684872, acc: 0.550781]\n",
      "9500: [D loss: 0.694634, acc: 0.000000]  [A loss: 0.669041, acc: 0.609375]\n",
      "9600: [D loss: 0.697255, acc: 0.000000]  [A loss: 0.716415, acc: 0.476562]\n",
      "9700: [D loss: 0.694420, acc: 0.000000]  [A loss: 0.658001, acc: 0.621094]\n",
      "9800: [D loss: 0.697817, acc: 0.000000]  [A loss: 0.662904, acc: 0.625000]\n",
      "9900: [D loss: 0.700604, acc: 0.000000]  [A loss: 0.727525, acc: 0.441406]\n"
     ]
    }
   ],
   "source": [
    "# added some noise to labels\n",
    "dcgan_mnist2 = DCGAN_MNIST(X_train_keras)\n",
    "dcgan_mnist2.train(train_steps=10000, batch_size=256, save_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1441
    },
    "colab_type": "code",
    "id": "5dW0bROCCiAO",
    "outputId": "c2f156ca-6fae-4e52-98a1-1d51e0dc7d8d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAALICAYAAAB8YjbFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate batch of synthetic MNIST images\n",
    "dcgan_mnist2.plot_images(fake=True)\n",
    "dcgan_mnist2.plot_images(fake=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "12_GAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
